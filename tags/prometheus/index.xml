<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>prometheus on SoByte</title>
    <link>https://www.sobyte.net/tags/prometheus/</link>
    <description>Recent content in prometheus on SoByte</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 22 Mar 2022 14:51:27 +0800</lastBuildDate><atom:link href="https://www.sobyte.net/tags/prometheus/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Centralized data management of multiple Prometheus instances with Thanos</title>
      <link>https://www.sobyte.net/post/2022-03/manage-multiple-prometheus-using-thanos/</link>
      <pubDate>Tue, 22 Mar 2022 14:51:27 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-03/manage-multiple-prometheus-using-thanos/</guid>
      <description>1. layering of monitoring As shown above, two strategies are used when building a monitoring system:
 The advantage of separating IaaS, MySQL middleware, and App tier monitoring is that the systems are highly available and fault-tolerant to each other. When the App tier monitoring does not work, the IaaS tier monitoring will immediately show up. Separation of long and short term metrics. Short-term metrics are used to provide alerting systems with high frequency queries for recent data, and long-term metrics are used to provide people with queries for data sets that span a larger period of time.</description>
    </item>
    
    <item>
      <title>Monitor of Kubernetes Using Prometheus Grafana</title>
      <link>https://www.sobyte.net/post/2022-03/monitor-of-kubernetes-using-prometheus-grafana/</link>
      <pubDate>Sun, 20 Mar 2022 11:28:34 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-03/monitor-of-kubernetes-using-prometheus-grafana/</guid>
      <description>The Prometheus community is updating so fast that some of the documentation written before is a bit out of date. Recently, I started to focus on observability again, and make up for some knowledge points in operation and maintenance.
1. Explanation of terms  Grafana  A visualization tool that provides various visualization panels and supports various data sources, including Prometheus, OpenTSDB, MySQL, etc.
 Prometheus  A time series database, mainly used to collect, store, and provide query data to the public.</description>
    </item>
    
    <item>
      <title>Prometheus Routing and Routing Configuration with Nginx Reverse Proxy</title>
      <link>https://www.sobyte.net/post/2022-03/prometheus-nginx-proxy/</link>
      <pubDate>Sat, 19 Mar 2022 18:03:07 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-03/prometheus-nginx-proxy/</guid>
      <description>Background The company group an end-to-end containerized test environment, located under different sub-paths, proxied by Nginx: * For example, user A is under /a/prometheus/: /a/prometheus/. For example, user A is under /a/: /a/prometheus/ For example, user B under /b/: /b/prometheus/ The effect you want to achieve, in addition to the above distinction between different subpaths, requires that there is no such distinction inside the container: i.e., the container can be</description>
    </item>
    
    <item>
      <title>Prometheus Monitoring Kubernetes Job Resource False Alarm Issue</title>
      <link>https://www.sobyte.net/post/2022-03/prometheus-monitor-k8s-job-trap/</link>
      <pubDate>Sun, 06 Mar 2022 17:12:01 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-03/prometheus-monitor-k8s-job-trap/</guid>
      <description>Someone mentioned a problem before, it is about Prometheus monitoring Job task false alarm problem, the general meaning of the CronJob control Job, the front of the execution failed, monitoring will trigger the alarm, after the solution to generate a new Job can be executed normally, but will still receive the alarm in front.
This is because we generally keep some history when executing Job tasks to facilitate troubleshooting, so if there is a failed Job before, even if it will become successful later, the previous Job will continue to exist, and the default alarm rule used by most direct kube-prometheus installation and deployment is kube_job_ status_failed &amp;gt; 0, which is obviously inaccurate.</description>
    </item>
    
    <item>
      <title>Kubernetes HPA Controlled Elastic Scaling based on Prometheus Custom Metrics</title>
      <link>https://www.sobyte.net/post/2022-01/k8s-hpa-prometheus/</link>
      <pubDate>Wed, 19 Jan 2022 15:37:00 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2022-01/k8s-hpa-prometheus/</guid>
      <description>There are three main types of elastic scaling in Kubernetes: HPA, VPA, and CA. Here we will focus on Pod Horizontal Scaling HPA.
With the release of Kubernetes v1.23, the HPA API came to a stable version autoscaling/v2:
 Scaling based on custom metrics Scaling based on multiple metrics Configurable scaling behaviour  From the initial v1 version of HPA, which only supported CPU and memory utilisation scaling, to the later support for custom metrics and aggregation layer APIs, to v1.</description>
    </item>
    
    <item>
      <title>AlertManager When to Alert</title>
      <link>https://www.sobyte.net/post/2021-11/alertmanager-when-alert/</link>
      <pubDate>Mon, 22 Nov 2021 19:42:12 +0800</pubDate>
      
      <guid>https://www.sobyte.net/post/2021-11/alertmanager-when-alert/</guid>
      <description>When using Prometheus for monitoring, alerting is done through AlertManager, but there are many people who are confused about the configuration related to alerting and are not quite sure when exactly it will be done. Here we will briefly introduce a few confusing parameters in AlertManager. First of all, there are two global parameters scrape_interval and evaluation_interval in Prometheus. The scrape_interval parameter represents the time interval at which Prometheus grabs</description>
    </item>
    
  </channel>
</rss>
