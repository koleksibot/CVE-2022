<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Running Kubernetes in Kubernetes - SoByte</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6356451834813761" crossorigin="anonymous"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E8GRRGBTEZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E8GRRGBTEZ');
</script>


<meta name="author" content="" /><meta name="description" content="KinD is a very lightweight Kubernetes installation tool that treats Docker containers as Kubernetes nodes, making it very easy to use. Since you can run a Kubernetes cluster in a Docker container, it&amp;rsquo;s natural to wonder if you can run it in a Pod. What are the problems with running in a Pod?
Installing Docker Daemon in a Pod KinD is now dependent on Docker, so first we need to create an image that allows us to run Docker Deamon in the Pod, so that we can execute commands like docker run inside the Pod, but this is not the same as the DIND mode of mounting docker." /><meta name="keywords" content="k8s" />






<meta name="generator" content="Hugo 0.92.2 with theme even" />


<link rel="canonical" href="https://www.sobyte.net/post/2021-12/k8s-in-k8s/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Running Kubernetes in Kubernetes" />
<meta property="og:description" content="KinD is a very lightweight Kubernetes installation tool that treats Docker containers as Kubernetes nodes, making it very easy to use. Since you can run a Kubernetes cluster in a Docker container, it&rsquo;s natural to wonder if you can run it in a Pod. What are the problems with running in a Pod?
Installing Docker Daemon in a Pod KinD is now dependent on Docker, so first we need to create an image that allows us to run Docker Deamon in the Pod, so that we can execute commands like docker run inside the Pod, but this is not the same as the DIND mode of mounting docker." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.sobyte.net/post/2021-12/k8s-in-k8s/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-12-12T17:46:19+08:00" />
<meta property="article:modified_time" content="2021-12-12T17:46:19+08:00" />

<meta itemprop="name" content="Running Kubernetes in Kubernetes">
<meta itemprop="description" content="KinD is a very lightweight Kubernetes installation tool that treats Docker containers as Kubernetes nodes, making it very easy to use. Since you can run a Kubernetes cluster in a Docker container, it&rsquo;s natural to wonder if you can run it in a Pod. What are the problems with running in a Pod?
Installing Docker Daemon in a Pod KinD is now dependent on Docker, so first we need to create an image that allows us to run Docker Deamon in the Pod, so that we can execute commands like docker run inside the Pod, but this is not the same as the DIND mode of mounting docker."><meta itemprop="datePublished" content="2021-12-12T17:46:19+08:00" />
<meta itemprop="dateModified" content="2021-12-12T17:46:19+08:00" />
<meta itemprop="wordCount" content="2312">
<meta itemprop="keywords" content="k8s," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Running Kubernetes in Kubernetes"/>
<meta name="twitter:description" content="KinD is a very lightweight Kubernetes installation tool that treats Docker containers as Kubernetes nodes, making it very easy to use. Since you can run a Kubernetes cluster in a Docker container, it&rsquo;s natural to wonder if you can run it in a Pod. What are the problems with running in a Pod?
Installing Docker Daemon in a Pod KinD is now dependent on Docker, so first we need to create an image that allows us to run Docker Deamon in the Pod, so that we can execute commands like docker run inside the Pod, but this is not the same as the DIND mode of mounting docker."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">SoByte</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="/ukraine/">
        <li class="mobile-menu-item">UKRAINE</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">SoByte</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/ukraine/">UKRAINE</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Running Kubernetes in Kubernetes</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-12-12 17:46:19 </span>
        <div class="post-category">
            <a href="/categories/skills/"> skills </a>
            </div>
          <span class="more-meta"> 2312 words </span>
          <span class="more-meta"> 11 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#installing-docker-daemon-in-a-pod">Installing Docker Daemon in a Pod</a>
          <ul>
            <li><a href="#the-pid-1-problem">The PID 1 problem</a></li>
            <li><a href="#mount-cgroups">Mount cgroups</a></li>
            <li><a href="#iptables">IPtables</a></li>
          </ul>
        </li>
        <li><a href="#running-kind-in-a-pod">Running KinD in a Pod</a></li>
        <li><a href="#summary">Summary</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>KinD is a very lightweight Kubernetes installation tool that treats Docker containers as Kubernetes nodes, making it very easy to use. Since you can run a Kubernetes cluster in a Docker container, it&rsquo;s natural to wonder if you can run it in a Pod. What are the problems with running in a Pod?</p>
<h2 id="installing-docker-daemon-in-a-pod">Installing Docker Daemon in a Pod</h2>
<p>KinD is now dependent on Docker, so first we need to create an image that allows us to run Docker Deamon in the Pod, so that we can execute commands like <code>docker run</code> inside the Pod, but this is not the same as the DIND mode of mounting docker.sock on the host as we mentioned before This is different from the DIND mode of mounting the host docker.sock. There are still a lot of problems with running Docker Deamon in a Pod.</p>
<h3 id="the-pid-1-problem">The PID 1 problem</h3>
<p>For example, we need to run Docker Daemon and some Kubernetes cluster tests in a container, and these tests depend on KinD and Docker Damon, and we may use systemd to run multiple services in a container, but there are some problems with using systemd.</p>
<p>For example, we need to keep the exit status of the test, and the container used in Kubernetes can watch the exit status of the first process (PID 1) in the container when it is running. If we use systemd, then the exit status of our test process will not be forwarded to Kubernetes. 2.
It is also important to get the logs of the tests. In Kubernetes, the container logs written to stdout and stderr are automatically fetched, but if we use systemd, it is more difficult to get the application logs.</p>
<p>To solve the above problem, we can use the following startup script in the container image.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">dockerd <span class="p">&amp;</span>
<span class="c1"># Wait until dockerd is ready.</span>
<span class="k">until</span> docker ps &gt;/dev/null 2&gt;<span class="p">&amp;</span><span class="m">1</span>
<span class="k">do</span>
  <span class="nb">echo</span> <span class="s2">&#34;Waiting for dockerd...&#34;</span>
  sleep <span class="m">1</span>
<span class="k">done</span>
<span class="nb">exec</span> <span class="s2">&#34;</span><span class="nv">$@</span><span class="s2">&#34;</span>
</code></pre></td></tr></table>
</div>
</div><p>However, it is important to note that we cannot use the above script as an entrypoint for the container; the entrypoint defined in the image will run in the container as PID 1 in a separate pid namespace; PID 1 is a special process in the kernel that behaves differently from other processes.</p>
<p>Essentially, the process that receives the signal is PID 1: it will be handled specially by the kernel; if it does not register a processor for the signal, the kernel will not fall back to the default behavior (i.e., kill the process). Since the kernel kills the process by default when the <code>SIGTERM</code> signal is received, some processes may not register a signal handler for the <code>SIGTERM</code> signal. If this happens, when Kubernetes tries to terminate the Pod, SIGTERM will be swallowed and you will notice that the Pod will be stuck in the <code>Terminating</code> state.</p>
<p>This is not really a new problem, but not many people are aware of it and keep building containers with this problem. We can solve this problem using the application <a href="https://github.com/krallin/tini">tini</a> as the entry point for the image, as follows.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">ENTRYPOINT <span class="o">[</span><span class="s2">&#34;/usr/bin/tini&#34;</span>, <span class="s2">&#34;--&#34;</span>, <span class="s2">&#34;/entrypoint.sh&#34;</span><span class="o">]</span>
</code></pre></td></tr></table>
</div>
</div><p>This program will properly register the signal handler and forward the signal. It will also perform some other PID 1 things, such as reclaiming zombie processes in the container.</p>
<h3 id="mount-cgroups">Mount cgroups</h3>
<p>Since Docker Daemon needs to control cgroups, it needs to mount the cgroup filesystem to the container. But since cgroups are shared with the host, we need to make sure that the cgroups controlled by Docker Daemon do not affect other cgroups used by other containers or host processes, and that the cgroups created by Docker Daemon in the container are not leaked after the container exits.</p>
<p>There is a <code>-cgroup-parent</code> parameter in Docker Daemon to tell Daemon to nest all container cgroups under the specified cgroup. When a container is running under a Kubernetes cluster, we set the <code>-cgroup-parent</code> parameter in Docker Daemon in the container so that all of its cgroups are nested under the cgroup created by Kubernetes for the container.</p>
<p>In the past, to make the cgroup file system available in the container, some users would mount <code>/sys/fs/cgroup</code> from the host to this location in the container. If we use it this way, we need to set <code>-cgroup-parent</code> to the following in the container startup script so that the cgroups created by Docker Daemon will be nested correctly.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="nv">CGROUP_PARENT</span><span class="o">=</span><span class="s2">&#34;</span><span class="k">$(</span>grep systemd /proc/self/cgroup <span class="p">|</span> cut -d: -f3<span class="k">)</span><span class="s2">/docker&#34;</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Note: <code>/proc/self/cgroup</code> shows the cgorup path of the calling process.</p>
</blockquote>
<p>But we should know that mounting the host&rsquo;s <code>/sys/fs/cgroup</code> file is a very dangerous thing to do, because it exposes the entire host&rsquo;s cgroup hierarchy to the container. To solve this problem, Docker used a trick to hide unrelated cgroups from the containers. docker mounts the root of each cgroup system&rsquo;s cgroup hierarchy from the container&rsquo;s cgroups.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ docker run --rm debian findmnt -lo source,target -t cgroup       
SOURCE                                                                               TARGET
cpuset<span class="o">[</span>/docker/451b803b3cd7cd2b69dde64cd833fdd799ae16f9d2d942386ec382f6d55bffac<span class="o">]</span>     /sys/fs/cgroup/cpuset
cpu<span class="o">[</span>/docker/451b803b3cd7cd2b69dde64cd833fdd799ae16f9d2d942386ec382f6d55bffac<span class="o">]</span>        /sys/fs/cgroup/cpu
cpuacct<span class="o">[</span>/docker/451b803b3cd7cd2b69dde64cd833fdd799ae16f9d2d942386ec382f6d55bffac<span class="o">]</span>    /sys/fs/cgroup/cpuacct
blkio<span class="o">[</span>/docker/451b803b3cd7cd2b69dde64cd833fdd799ae16f9d2d942386ec382f6d55bffac<span class="o">]</span>     /sys/fs/cgroup/blkio
memory<span class="o">[</span>/docker/451b803b3cd7cd2b69dde64cd833fdd799ae16f9d2d942386ec382f6d55bffac<span class="o">]</span>     /sys/fs/cgroup/memory
 
cgroup<span class="o">[</span>/docker/451b803b3cd7cd2b69dde64cd833fdd799ae16f9d2d942386ec382f6d55bffac<span class="o">]</span>     /sys/fs/cgroup/systemd
</code></pre></td></tr></table>
</div>
</div><p>From the above, we can see that cgroups control the files at the root of the cgroup hierarchy in the container by mapping the <code>/sys/fs/cgroup/memory/memory.limit_in_bytes</code> file on the host cgroup file system to the <code>/sys/fs/cgroup/memory/docker/&lt;CONTAINER_ID&gt;/ memory.limit_in_bytes</code> to control the file at the root of the cgroup hierarchy within the container, which prevents container processes from accidentally modifying the host&rsquo;s cgroup.</p>
<p>However, this approach can sometimes be confusing for applications like cadvisor and kubelet, because binding mounts do not change the contents of <code>/proc/&lt;PID&gt;/cgroup</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ docker run --rm debian cat /proc/1/cgroup                                
14:name<span class="o">=</span>systemd:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141
 
5:memory:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141
4:blkio:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141
3:cpuacct:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141
2:cpu:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141
1:cpuset:/docker/512f6b62e3963f85f5abc09b69c370d27ab1dc56549fa8afcbb86eec8663a141
0::/
</code></pre></td></tr></table>
</div>
</div><p>cadvisor will look at <code>/proc/&lt;PID&gt;/cgroup</code> to get the cgroup of a given process and try to get CPU or memory statistics from the corresponding cgroup. To solve this problem, we did another mount inside the container, from <code>/sys/fs/cgroup/memory</code> to <code>/sys/fs/cgroup/memory/ docker/&lt;CONTAINER_ID&gt;/</code> (for all cgroup subsystems), which works well for this problem.</p>
<p>The new workaround is now to use <a href="http://man7.org/linux/man-pages/man7/cgroup_namespaces.7.html">cgroup namespace</a> if you are running under a Linux system with kernel version 4.6+, and both runc and docker both add support for cgroup namespaces. However, Kubernetes does not currently support the cgroup namespace, but will soon do so as <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/20191118-">cgroups v2 support</a> cgroups-v2.md).</p>
<h3 id="iptables">IPtables</h3>
<p>While using it we found that when running a Kubernetes cluster online, sometimes the nested containers started by the Docker Daemon inside the container could not access the extranet, but worked fine on the local development computer, which most developers should encounter often.</p>
<p>Finally, it was found that when this problem occurred, the packets from the nested Docker container did not hit the POSTROUTING chain of iptables, so it was not masqueraded.</p>
<p>This problem is because the image containing the Docker Daemon is based on <a href="https://www.debian.org/releases/buster/">Debian buster</a>, which by default uses <a href="https://wiki">nftables</a> as the default backend for iptables. .debian.org/nftables) as the default backend for iptables, but Docker itself does not support nftables yet. To solve this problem, just switch to the iptables command in the container image.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">RUN update-alternatives --set iptables  /usr/sbin/iptables-legacy <span class="o">||</span> <span class="nb">true</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span><span class="se"></span>    update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy <span class="o">||</span> <span class="nb">true</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span><span class="se"></span>    update-alternatives --set arptables /usr/sbin/arptables-legacy <span class="o">||</span> <span class="nb">true</span>
</code></pre></td></tr></table>
</div>
</div><p>The full Dockerfile file and startup script are available on GitHub (<a href="https://github.com/jieyu/docker-images/tree/master/dind)">https://github.com/jieyu/docker-images/tree/master/dind)</a>, or you can use the <code>jieyu/dind-buster:v0.1.8</code> image directly to test.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ docker run --rm --privileged jieyu/dind-buster:v0.1.8 docker run alpine wget baidu.com
</code></pre></td></tr></table>
</div>
</div><p>Simply deploy under a Kubernetes cluster using the Pod resource list shown below.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">dind</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">jieyu/dind-buster:v0.1.8</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">dind</span><span class="w">
</span><span class="w">    </span><span class="nt">stdin</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">tty</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">args</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">/bin/bash</span><span class="w">
</span><span class="w">    </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/var/lib/docker</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">varlibdocker</span><span class="w">
</span><span class="w">    </span><span class="nt">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">privileged</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">varlibdocker</span><span class="w">
</span><span class="w">    </span><span class="nt">emptyDir</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h2 id="running-kind-in-a-pod">Running KinD in a Pod</h2>
<p>We have successfully configured Docker-in-Docker (DinD) above, so let&rsquo;s start a Kubernetes cluster in that container using KinD.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ docker run -ti --rm --privileged jieyu/dind-buster:v0.1.8 /bin/bash
Waiting <span class="k">for</span> dockerd...
<span class="o">[</span>root@257b543a91a5 /<span class="o">]</span><span class="c1"># curl -Lso ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64</span>
<span class="o">[</span>root@257b543a91a5 /<span class="o">]</span><span class="c1"># chmod +x ./kind</span>
<span class="o">[</span>root@257b543a91a5 /<span class="o">]</span><span class="c1"># mv ./kind /usr/bin/ </span>
<span class="o">[</span>root@257b543a91a5 /<span class="o">]</span><span class="c1"># kind create cluster</span>
Creating cluster <span class="s2">&#34;kind&#34;</span> ...
 ‚úì Ensuring node image <span class="o">(</span>kindest/node:v1.18.2<span class="o">)</span> üñº 
 ‚úì Preparing nodes üì¶  
 ‚úì Writing configuration üìú 
 ‚úì Starting control-plane üïπÔ∏è 
 ‚úì Installing CNI üîå 
 ‚úì Installing StorageClass üíæ 
Set kubectl context to <span class="s2">&#34;kind-kind&#34;</span>
You can now use your cluster with:
kubectl cluster-info --context kind-kind
Have a nice day! üëã
<span class="o">[</span>root@257b543a91a5 /<span class="o">]</span><span class="c1"># kubectl get nodes</span>
NAME                 STATUS   ROLES    AGE   VERSION
kind-control-plane   Ready    master   11m   v1.18.2
</code></pre></td></tr></table>
</div>
</div><p>For some reason you may not be able to download kind with the above command, we can find a way to download it to the host in advance, and then directly mount it to the container can also be, I will kind and kubectl commands here are mounted to the container, use the following command to start the container can be.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ docker run -it --rm --privileged -v /usr/local/bin/kind:/usr/bin/kind -v /usr/local/bin/kubectl:/usr/bin/kubectl jieyu/dind-buster:v0.1.8 /bin/bash
</code></pre></td></tr></table>
</div>
</div><p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/12/12/98e583f3b90345208ca7eba3c8a18245.png" alt=""></p>
<p>You can see that KinD works well in containers to create Kubernetes clusters. Next, let&rsquo;s test this directly in Kubernetes.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl apply -f dind.yaml
$ kubectl <span class="nb">exec</span> -ti dind /bin/bash
root@dind:/# curl -Lso ./kind https://kind.sigs.k8s.io/dl/v0.7.0/kind-<span class="k">$(</span>uname<span class="k">)</span>-amd64
root@dind:/# chmod +x ./kind
root@dind:/# mv ./kind /usr/bin/
root@dind:/# kind create cluster
Creating cluster <span class="s2">&#34;kind&#34;</span> ...
 ‚úì Ensuring node image <span class="o">(</span>kindest/node:v1.17.0<span class="o">)</span> üñº 
 ‚úì Preparing nodes üì¶  
 ‚úì Writing configuration üìú 
 ‚úó Starting control-plane üïπÔ∏è 
ERROR: failed to create cluster: failed to init node with kubeadm: <span class="nb">command</span> <span class="s2">&#34;docker exec --privileged kind-control-plane kubeadm init --ignore-preflight-errors=all --config=/kind/kubeadm.conf --skip-token-print --v=6&#34;</span> failed with error: <span class="nb">exit</span> status <span class="m">137</span>
</code></pre></td></tr></table>
</div>
</div><p>We can see that using KinD in Pod to create a cluster fails because the kubelet running in the KinD nested container randomly kills the processes in the top-level container, which is actually still related to the cgroups mount discussed above.</p>
<p>However, when I was using KinD in v0.8.1, I was able to create clusters in the above Pods normally, so I don&rsquo;t know if there is any special treatment for KinD-built clusters.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/12/12/15274319b1814ac9993f2ecd864d9b40.png" alt=""></p>
<p>If you are experiencing the same problem as above, then you can continue to see the solution below.</p>
<p>When the top-level container (DIND) is running in a Kubernetes Pod, for each cgroup subsystem (e.g. memory), its cgroup path from the host&rsquo;s perspective is <code>/kubepods/burstable/&lt;POD_ID&gt;/&lt;DIND_CID&gt;</code>.</p>
<p>When KinD starts a kubelet within a nested node container within a DIND container, the kubelet will operate the cgroup under <code>/kubepods/burstable/</code> relative to the root cgroup of the nested KIND node container for its Pods. from the host&rsquo;s perspective, the cgroup path is <code>/ kubepods/burstable/&lt;POD_ID&gt;/&lt;DIND_CID&gt;/docker/&lt;KIND_CID&gt;/kubepods/burstable/</code>.</p>
<p>These are correct, but in the nested KinD node container, there is another cgroup that exists under <code>/kubepods/burstable/&lt;POD_ID&gt;/&lt;DIND_CID&gt;/docker/&lt;DIND_CID&gt;</code>, as opposed to the root cgroup of the nested KinD node container, which existed before the kubelet existed before it was started, as a result of the cgroups mount we discussed above, set via the KinD entrypoint script. And if you do a <code>cat /kubepods/burstable/&lt;POD_ID&gt;/docker/&lt;DIND_CID&gt;/tasks</code> inside the KinD node container, you will see the processes of the DinD container.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/12/12/835567bb05ac462693aafaba178efc03.png" alt=""></p>
<p>This is the root cause: the kubelet inside the KinD node container sees the cgroup and thinks it should manage it, but cannot find a Pod associated with the cgroup, so it tries to delete the cgroup by killing the processes belonging to the cgroup, which results in random processes being killed. The solution to this problem is to set the <code>-cgroup-root</code> parameter of the kubelet to instruct the kubelet in the KinD node container to use a root path (e.g. /kubelet) for its Pods that does not have a cgroup. This allows you to start a KinD cluster in a Kubernetes cluster, which we can fix with the YAML resource manifest file below.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">kind-cluster</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">jieyu/kind-cluster-buster:v0.1.0</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">kind-cluster</span><span class="w">
</span><span class="w">    </span><span class="nt">stdin</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">tty</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">args</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">/bin/bash</span><span class="w">
</span><span class="w">    </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">API_SERVER_ADDRESS</span><span class="w">
</span><span class="w">      </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">fieldRef</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">fieldPath</span><span class="p">:</span><span class="w"> </span><span class="l">status.podIP</span><span class="w">
</span><span class="w">    </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/var/lib/docker</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">varlibdocker</span><span class="w">
</span><span class="w">    </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/lib/modules</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">libmodules</span><span class="w">
</span><span class="w">      </span><span class="nt">readOnly</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">privileged</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">30001</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">api-server-port</span><span class="w">
</span><span class="w">      </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">TCP</span><span class="w">
</span><span class="w">    </span><span class="nt">readinessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">15</span><span class="w">
</span><span class="w">      </span><span class="nt">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/healthz</span><span class="w">
</span><span class="w">        </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l">api-server-port</span><span class="w">
</span><span class="w">        </span><span class="nt">scheme</span><span class="p">:</span><span class="w"> </span><span class="l">HTTPS</span><span class="w">
</span><span class="w">      </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">120</span><span class="w">
</span><span class="w">      </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">20</span><span class="w">
</span><span class="w">      </span><span class="nt">successThreshold</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">      </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">varlibdocker</span><span class="w">
</span><span class="w">    </span><span class="nt">emptyDir</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">libmodules</span><span class="w">
</span><span class="w">    </span><span class="nt">hostPath</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/lib/modules</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Once created using the resource list file above, we can go into the Pod and verify it in a moment.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl <span class="nb">exec</span> -ti kind-cluster /bin/bash
root@kind-cluster:/# kubectl get nodes
NAME                 STATUS   ROLES    AGE   VERSION                                                                                                                   
kind-control-plane   Ready    master   72s   v1.17.0
</code></pre></td></tr></table>
</div>
</div><p>It is also possible to use the Docker CLI directly to test.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ docker run -ti --rm --privileged jieyu/kind-cluster-buster:v0.1.0 /bin/bash
Waiting <span class="k">for</span> dockerd...
Setting up KIND cluster
Creating cluster <span class="s2">&#34;kind&#34;</span> ...
 ‚úì Ensuring node image <span class="o">(</span>jieyu/kind-node:v1.17.0<span class="o">)</span> üñº 
 ‚úì Preparing nodes üì¶  
 ‚úì Writing configuration üìú 
 ‚úì Starting control-plane üïπÔ∏è 
 ‚úì Installing CNI üîå 
 ‚úì Installing StorageClass üíæ 
 ‚úì Waiting ‚â§ 15m0s <span class="k">for</span> control-plane <span class="o">=</span> Ready ‚è≥ 
 ‚Ä¢ Ready after 31s üíö
Set kubectl context to <span class="s2">&#34;kind-kind&#34;</span>
You can now use your cluster with:
kubectl cluster-info --context kind-kind
Have a nice day! üëã
root@d95fa1302557:/# kubectl get nodes
NAME                 STATUS   ROLES    AGE   VERSION
kind-control-plane   Ready    master   71s   v1.17.0
root@d95fa1302557:/#
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>The above image corresponds to the Dockerfile and startup script address: <a href="https://github.com/jieyu/docker-images/tree/master/kind-cluster">https://github.com/jieyu/docker-images/tree/master/kind-cluster</a></p>
</blockquote>
<p>The following image shows the final result of a Pod I created in a Kubernetes cluster built by KinD, and then a standalone Kubernetes cluster created in the Pod.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/12/12/7668c38f57d64889a059657808591f1e.png" alt=""></p>
<h2 id="summary">Summary</h2>
<p>When implementing the above features, we encountered a lot of obstacles in the process, most of which are caused by the fact that Docker containers don&rsquo;t provide complete isolation from the host, and some kernel resources like cgroups are shared in the kernel, which may also cause potential conflicts if many containers operate them at the same time. But once these problems are solved, we can easily run a standalone Kubernetes cluster in a Kubernetes cluster Pod, which should be considered a real Kubernetes IN Kubernetes, right?</p>

    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/k8s/">k8s</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2021-12/deploy-k8s-on-win-use-wsl2/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Building a Kubernetes cluster with WSL2 on Windows</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/2021-12/status-in-pv-pvc/">
            <span class="next-text nav-default">PV and PVC state changes in Kubernetes</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="https://www.sobyte.net/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2021 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
