<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Zero Downtime Rolling Update K8s - SoByte</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6356451834813761" crossorigin="anonymous"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E8GRRGBTEZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E8GRRGBTEZ');
</script>


<meta name="author" content="" /><meta name="description" content="The software world is evolving faster than ever, and the need to stay competitive requires new software releases as quickly as possible, without impacting users online. Many enterprises have migrated their workloads to Kubernetes clusters, which inherently take into account some production environment practices, but there are some additional things we need to do to make Kubernetes truly zero downtime without disruption or lost requests.
Rolling Updates By default, Kubernetes Deployment has a rolling update policy for Pod updates, which ensures that some instances are still running when the application is updated at any point in time to prevent the application from going down, and only kills the old Pod when the newly deployed Pod is up and ready to handle traffic." /><meta name="keywords" content="k8s, Rolling Update, Zero Downtime" />






<meta name="generator" content="Hugo 0.92.2 with theme even" />


<link rel="canonical" href="https://www.sobyte.net/post/2021-12/zero-downtime-rolling-update-k8s/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Zero Downtime Rolling Update K8s" />
<meta property="og:description" content="The software world is evolving faster than ever, and the need to stay competitive requires new software releases as quickly as possible, without impacting users online. Many enterprises have migrated their workloads to Kubernetes clusters, which inherently take into account some production environment practices, but there are some additional things we need to do to make Kubernetes truly zero downtime without disruption or lost requests.
Rolling Updates By default, Kubernetes Deployment has a rolling update policy for Pod updates, which ensures that some instances are still running when the application is updated at any point in time to prevent the application from going down, and only kills the old Pod when the newly deployed Pod is up and ready to handle traffic." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.sobyte.net/post/2021-12/zero-downtime-rolling-update-k8s/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-12-20T21:42:55+08:00" />
<meta property="article:modified_time" content="2021-12-20T21:42:55+08:00" />

<meta itemprop="name" content="Zero Downtime Rolling Update K8s">
<meta itemprop="description" content="The software world is evolving faster than ever, and the need to stay competitive requires new software releases as quickly as possible, without impacting users online. Many enterprises have migrated their workloads to Kubernetes clusters, which inherently take into account some production environment practices, but there are some additional things we need to do to make Kubernetes truly zero downtime without disruption or lost requests.
Rolling Updates By default, Kubernetes Deployment has a rolling update policy for Pod updates, which ensures that some instances are still running when the application is updated at any point in time to prevent the application from going down, and only kills the old Pod when the newly deployed Pod is up and ready to handle traffic."><meta itemprop="datePublished" content="2021-12-20T21:42:55+08:00" />
<meta itemprop="dateModified" content="2021-12-20T21:42:55+08:00" />
<meta itemprop="wordCount" content="1829">
<meta itemprop="keywords" content="k8s," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Zero Downtime Rolling Update K8s"/>
<meta name="twitter:description" content="The software world is evolving faster than ever, and the need to stay competitive requires new software releases as quickly as possible, without impacting users online. Many enterprises have migrated their workloads to Kubernetes clusters, which inherently take into account some production environment practices, but there are some additional things we need to do to make Kubernetes truly zero downtime without disruption or lost requests.
Rolling Updates By default, Kubernetes Deployment has a rolling update policy for Pod updates, which ensures that some instances are still running when the application is updated at any point in time to prevent the application from going down, and only kills the old Pod when the newly deployed Pod is up and ready to handle traffic."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">SoByte</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="/ukraine/">
        <li class="mobile-menu-item">UKRAINE</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">SoByte</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/ukraine/">UKRAINE</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Zero Downtime Rolling Update K8s</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-12-20 21:42:55 </span>
        <div class="post-category">
            <a href="/categories/skills/"> skills </a>
            </div>
          <span class="more-meta"> 1829 words </span>
          <span class="more-meta"> 9 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#rolling-updates">Rolling Updates</a></li>
        <li><a href="#availability-detection">Availability Detection</a></li>
        <li><a href="#cause-analysis">Cause Analysis</a></li>
        <li><a href="#zero-downtime">Zero Downtime</a></li>
        <li><a href="#summary">Summary</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>The software world is evolving faster than ever, and the need to stay competitive requires new software releases as quickly as possible, without impacting users online. Many enterprises have migrated their workloads to Kubernetes clusters, which inherently take into account some production environment practices, but there are some additional things we need to do to make Kubernetes truly zero downtime without disruption or lost requests.</p>
<h2 id="rolling-updates">Rolling Updates</h2>
<p>By default, Kubernetes Deployment has a rolling update policy for Pod updates, which ensures that some instances are still running when the application is updated at any point in time to prevent the application from going down, and only kills the old Pod when the newly deployed Pod is up and ready to handle traffic.</p>
<p>We can also specify how Kubernetes handles multiple replicas during an update. For example, if we have a 3-replica application, should we immediately create all 3 new Pods and wait for them to start, or kill all but one of the old Pods during the update, or should we replace them one by one? The following example is a Deployment definition using the default rolling update upgrade policy, where there can be at most one container with more than the number of replicas (maxSurge) and no unavailable containers during the update process.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">zero-downtime</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">zero-downtime</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">zero-downtime</span><span class="w">
</span><span class="w">  </span><span class="nt">strategy</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">RollingUpdate</span><span class="w">
</span><span class="w">    </span><span class="nt">rollingUpdate</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">maxSurge</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">      </span><span class="nt">maxUnavailable</span><span class="p">:</span><span class="w"> </span><span class="m">0</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># with image nginx</span><span class="w">
</span><span class="w">    </span><span class="c"># ...</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>The <code>zero-downtime</code> application above uses the nginx image to create three copies of the Deployment to perform rolling updates: first create a new version of the Pod, wait for the Pod to start and be ready, then delete an old Pod, then move on to the next new Pod until all copies have been replaced. To let Kubernetes know when our Pod is ready to handle traffic requests, we also need to configure the liveness and readiness probes. The following is the output of the <code>old and new</code> Pod replacements.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl get pods
NAME                            READY   STATUS              RESTARTS   AGE
zero-downtime-d449b5cc4-k8b27   1/1     Running             <span class="m">0</span>          3m9s
zero-downtime-d449b5cc4-n2lc4   1/1     Running             <span class="m">0</span>          3m9s
zero-downtime-d449b5cc4-sdw8b   1/1     Running             <span class="m">0</span>          3m9s
...

zero-downtime-d449b5cc4-k8b27   1/1     Running             <span class="m">0</span>          3m9s
zero-downtime-d449b5cc4-n2lc4   1/1     Running             <span class="m">0</span>          3m9s
zero-downtime-d449b5cc4-sdw8b   1/1     Running             <span class="m">0</span>          3m9s
zero-downtime-d569474d4-q9khv   0/1     ContainerCreating   <span class="m">0</span>          12s
...

zero-downtime-d449b5cc4-n2lc4   1/1     Running             <span class="m">0</span>          3m9s
zero-downtime-d449b5cc4-sdw8b   1/1     Running             <span class="m">0</span>          3m9s
zero-downtime-d449b5cc4-k8b27   1/1     Terminating         <span class="m">0</span>          3m29s
zero-downtime-d569474d4-q9khv   1/1     Running             <span class="m">0</span>          1m
...

zero-downtime-d449b5cc4-n2lc4   1/1     Running             <span class="m">0</span>          5m
zero-downtime-d449b5cc4-sdw8b   1/1     Running             <span class="m">0</span>          5m
zero-downtime-d569474d4-q9khv   1/1     Running             <span class="m">0</span>          1m
zero-downtime-d569474d4-2c7qz   0/1     ContainerCreating   <span class="m">0</span>          10s
...

...

zero-downtime-d569474d4-2c7qz   1/1     Running             <span class="m">0</span>          40s
zero-downtime-d569474d4-mxbs4   1/1     Running             <span class="m">0</span>          13s
zero-downtime-d569474d4-q9khv   1/1     Running             <span class="m">0</span>          67s
</code></pre></td></tr></table>
</div>
</div><h2 id="availability-detection">Availability Detection</h2>
<p>If we do a rolling update from an old version to a new version and simply use the output to determine which Pods are alive and ready, then the rolling update behavior must seem valid, but often the reality is that the switch from an old version to a new version is not always smooth and the application is likely to drop some client requests.</p>
<p>In order to test whether requests are being dropped, especially for instances that are about to exit the service, we can use some load testing tools to connect our application for testing. We need to focus on whether all HTTP requests, including keep-alive HTTP connections, are being handled correctly, so we can use [Apache Bench (AB Test)](<a href="http://httpd.apache.org/docs/current/programs/ab">http://httpd.apache.org/docs/current/programs/ab</a>. html) or <a href="https://fortio.org/">Fortio (Istio Test Tool)</a> to test.</p>
<p>We use multiple threads to connect to a running application in a concurrent manner, and we care about the status of responses and failed connections, not information like latency or throughput. We use Fortio as a test tool here, for example, 500 requests per second and 8 concurrent keep-alive connections with the following test command (using the domain <code>zero.qikqiak.com</code> to proxy to the 3 Pods above).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ fortio load -a -c <span class="m">8</span> -qps <span class="m">500</span> -t 60s <span class="s2">&#34;http://zero.qikqiak.com/&#34;</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>The official documentation on the use of fortio can be found at: <a href="https://github.com/fortio/fortio">https://github.com/fortio/fortio</a></p>
</blockquote>
<p>Using the <code>-a</code> parameter we can save the test report as a web page so that we can view the test report directly in the browser. If we start the test while we are doing a rolling update of the application, we may see some requests that cannot be connected to.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/12/20/45e417bd6a0d4d198c1621344d9ae55a.png" alt="image"></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">Starting at <span class="m">1000</span> qps with <span class="m">8</span> thread<span class="o">(</span>s<span class="o">)</span> <span class="o">[</span>gomax 2<span class="o">]</span> <span class="k">for</span> 1m0s : <span class="m">7500</span> calls each <span class="o">(</span>total 60000<span class="o">)</span>
Ended after 1m0.006243654s : <span class="m">5485</span> calls. <span class="nv">qps</span><span class="o">=</span>91.407
Aggregated Sleep Time : count <span class="m">5485</span> avg -17.626081 +/- <span class="m">15</span> min -54.753398956 max 0.000709054 sum -96679.0518
<span class="o">[</span>...<span class="o">]</span>
Code <span class="m">200</span> : <span class="m">5463</span> <span class="o">(</span>99.6 %<span class="o">)</span>
Code <span class="m">502</span> : <span class="m">20</span> <span class="o">(</span>0.4 %<span class="o">)</span>
Response Header Sizes : count <span class="m">5485</span> avg 213.14166 +/- 13.53 min <span class="m">0</span> max <span class="m">214</span> sum <span class="m">1169082</span>
Response Body/Total Sizes : count <span class="m">5485</span> avg 823.18651 +/- 44.41 min <span class="m">0</span> max <span class="m">826</span> sum <span class="m">4515178</span>
<span class="o">[</span>...<span class="o">]</span>
</code></pre></td></tr></table>
</div>
</div><p>As you can see from the output above that some requests failed to be processed (502), we can run several test scenarios where we connect to the application in different ways, such as through Kubernetes Ingress or directly from inside the cluster through the Service. We will see that the behavior during rolling updates may vary, depending on the configuration parameters of the test, and that clients connecting to the service from inside the cluster may not encounter as many failed connections compared to connections via Ingress.</p>
<h2 id="cause-analysis">Cause Analysis</h2>
<p>The problem now is figuring out exactly what happens when an application reroutes traffic from an old Pod instance to a new one during a rolling update, so let&rsquo;s start by looking at how Kubernetes manages workload connections.</p>
<p>If the client performing the test connects to the zero-downtime service directly from inside the cluster, it will first resolve to the Service&rsquo;s ClusterIP through the cluster&rsquo;s DNS service and then forward to the Pod instance behind the Service, which is achieved by <code>kube-proxy</code> on top of each node by updating the iptables rules.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/12/20/1744a1e3fe2b45c1929e088e7a73aef5.png" alt="image"></p>
<p>Kubernetes updates the Endpoints object based on the status of the Pods, which ensures that the Endpoints contain Pods that are ready to handle requests.</p>
<p>But Kubernetes Ingress connects to instances in a slightly different way, which is why we see different downtime behavior during rolling updates when clients connect to applications via Ingresss.</p>
<p>Most Ingress Controllers, such as nginx-ingress and traefik, get the address of the Pod directly by watching the Endpoints object directly, without having to do a layer of forwarding through iptables.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/12/20/ca4a6ac96a28478e82ec23929cb35c1c.png" alt="image"></p>
<p>Regardless of how we connect to the application, the goal of Kubernetes is to minimize service interruptions during rolling updates. Once a new Pod is active and ready to go, Kubernetes will stop the incoming Pod, update the Pod&rsquo;s state to <code>&quot;Terminating&quot;</code>, then remove it from the Endpoints object and send a <code>SIGTERM</code> signal to the to the Pod&rsquo;s master process. The <code>SIGTERM</code> signal then causes the container to shut down in the normal way and does not accept any new connections.After the Pod is removed from the Endpoints object, the front load balancer routes the traffic to other (new) Pods. This is also the main reason for the availability gap of our application, because the termination signal goes to deactivate the Pod before the responsible balancer notices the change and updates its configuration, and this reconfiguration process happens asynchronously, so it is not guaranteed to be in the right order, so it can lead to few requests being routed to the terminated Pod.</p>
<h2 id="zero-downtime">Zero Downtime</h2>
<p>So how can we enhance our application to achieve a true zero-downtime migration?</p>
<p>First, a prerequisite to achieve this goal is that our containers handle termination signals correctly and achieve graceful shutdown on the <code>SIGTERM</code> signal. The next step is to add a readiness probe to check if our application is ready to handle the traffic.</p>
<p>The readiness probe is just the starting point for our smooth rolling updates. To solve the problem of not blocking when the Pod stops and waiting for the load balancer to reconfigure, we need to use the <code>preStop</code> lifecycle hook, which is called before the container terminates.</p>
<p>The lifecycle hook function is synchronous, so it must be done before sending the final termination signal to the container. In our example, we use this hook to simply wait and then the <code>SIGTERM</code> signal will stop the application process. At the same time, Kubernetes will remove the Pod from the Endpoints object, so the Pod will be excluded from our load balancer, and basically our lifecycle hook function waits long enough to ensure that the load balancer is reconfigured before the application stops.</p>
<p>Here we add a <code>preStop</code> hook to the <code>zero-downtime</code> Deployment.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">zero-downtime</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">zero-downtime</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">zero-downtime</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">zero-downtime</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="c"># ...</span><span class="w">
</span><span class="w">        </span><span class="nt">readinessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="c"># ...</span><span class="w">
</span><span class="w">        </span><span class="nt">lifecycle</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">preStop</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">exec</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;/bin/bash&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;-c&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;sleep 20&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="nt">strategy</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># ...</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>We use <code>preStop</code> to set a grace period of 20s, the Pod will sleep for 20s before it is really destroyed, which is equivalent to leaving time for the Endpoints controller and kube-proxy to update the Endpoints object and forwarding rules. Although the Pod is in Terminating state during this time, even if a request is forwarded to this Terminating Pod before the forwarding rules are fully updated, it can still be processed normally because it is still in sleep and not really destroyed.</p>
<p>Now, when we look at the Pod behavior during the rolling update, we will see that the terminating Pod is in the <code>Terminating</code> state, but it will not shut down until the waiting time is over, and if we retest with <code>Fortio</code>, we will see the ideal behavior with zero failed requests.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/12/20/9d9a9a270cab44009136616da321ff38.png" alt="image"></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">Starting at <span class="m">1000</span> qps with <span class="m">8</span> thread<span class="o">(</span>s<span class="o">)</span> <span class="o">[</span>gomax 2<span class="o">]</span> <span class="k">for</span> 1m0s : <span class="m">7500</span> calls each <span class="o">(</span>total 60000<span class="o">)</span>
Ended after 1m0.091439891s : <span class="m">10015</span> calls. <span class="nv">qps</span><span class="o">=</span>166.66
Aggregated Sleep Time : count <span class="m">10015</span> avg -23.316213 +/- 14.52 min -50.161414028 max 0.001811225 sum -233511.876
<span class="o">[</span>...<span class="o">]</span>
Code <span class="m">200</span> : <span class="m">10015</span> <span class="o">(</span>100.0 %<span class="o">)</span>
Response Header Sizes : count <span class="m">10015</span> avg <span class="m">214</span> +/- <span class="m">0</span> min <span class="m">214</span> max <span class="m">214</span> sum <span class="m">2143210</span>
Response Body/Total Sizes : count <span class="m">10015</span> avg <span class="m">826</span> +/- <span class="m">0</span> min <span class="m">826</span> max <span class="m">826</span> sum <span class="m">8272390</span>
Saved result to data/2020-02-12-162008_Fortio.json 
All <span class="k">done</span> <span class="m">10015</span> calls 47.405 ms avg, 166.7 qps
</code></pre></td></tr></table>
</div>
</div><h2 id="summary">Summary</h2>
<p>Kubernetes has done a good job of taking production readiness into account, but in order to run our enterprise applications in a production environment, we need to understand how Kubernetes runs in the background and how our applications behave during startup and shutdown. Some teams are converting long connections to short connections for processing, but I&rsquo;m still doing support at the application level, such as adding a retry mechanism on the client side to automatically reconnect when the connection is disconnected. If you have a better solution can also leave a message to discuss the program with each other.</p>

    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/k8s/">k8s</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2021-12/deploy-vscode-on-k8s/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Deploying VSCode on a Kubernetes Cluster</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/2021-12/github-cli-tool-usage/">
            <span class="next-text nav-default">Using the GitHub CLI Command Line Tools</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="https://www.sobyte.net/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2021 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
