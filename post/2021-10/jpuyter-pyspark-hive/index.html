<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Connecting to Hive using PySpark in Jupyter - SoByte</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6356451834813761" crossorigin="anonymous"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E8GRRGBTEZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E8GRRGBTEZ');
</script>


<meta name="author" content="" /><meta name="description" content="The company&amp;rsquo;s Jupyter environment supports PySpark. this makes it very easy to use PySpark to connect to Hive queries and use. Since I had no prior exposure to Spark at all, I put together some reference material. Spark Context The core module in PySpark is SparkContext (sc for short), and the most important data carrier is RDD, which is like a NumPy array or a Pandas Series, and can be" /><meta name="keywords" content="jpuyter, pyspark, hive" />






<meta name="generator" content="Hugo 0.92.2 with theme even" />


<link rel="canonical" href="https://www.sobyte.net/post/2021-10/jpuyter-pyspark-hive/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Connecting to Hive using PySpark in Jupyter" />
<meta property="og:description" content="The company&rsquo;s Jupyter environment supports PySpark. this makes it very easy to use PySpark to connect to Hive queries and use. Since I had no prior exposure to Spark at all, I put together some reference material. Spark Context The core module in PySpark is SparkContext (sc for short), and the most important data carrier is RDD, which is like a NumPy array or a Pandas Series, and can be" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.sobyte.net/post/2021-10/jpuyter-pyspark-hive/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-10-24T20:44:05+08:00" />
<meta property="article:modified_time" content="2021-10-24T20:44:05+08:00" />

<meta itemprop="name" content="Connecting to Hive using PySpark in Jupyter">
<meta itemprop="description" content="The company&rsquo;s Jupyter environment supports PySpark. this makes it very easy to use PySpark to connect to Hive queries and use. Since I had no prior exposure to Spark at all, I put together some reference material. Spark Context The core module in PySpark is SparkContext (sc for short), and the most important data carrier is RDD, which is like a NumPy array or a Pandas Series, and can be"><meta itemprop="datePublished" content="2021-10-24T20:44:05+08:00" />
<meta itemprop="dateModified" content="2021-10-24T20:44:05+08:00" />
<meta itemprop="wordCount" content="3221">
<meta itemprop="keywords" content="jupyter,pyspark,hive," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Connecting to Hive using PySpark in Jupyter"/>
<meta name="twitter:description" content="The company&rsquo;s Jupyter environment supports PySpark. this makes it very easy to use PySpark to connect to Hive queries and use. Since I had no prior exposure to Spark at all, I put together some reference material. Spark Context The core module in PySpark is SparkContext (sc for short), and the most important data carrier is RDD, which is like a NumPy array or a Pandas Series, and can be"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">SoByte</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="/ukraine/">
        <li class="mobile-menu-item">UKRAINE</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">SoByte</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/ukraine/">UKRAINE</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Connecting to Hive using PySpark in Jupyter</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-10-24 20:44:05 </span>
        <div class="post-category">
            <a href="/categories/skills/"> skills </a>
            </div>
          <span class="more-meta"> 3221 words </span>
          <span class="more-meta"> 7 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#spark-context">Spark Context</a></li>
        <li><a href="#spark-session">Spark Session</a></li>
        <li><a href="#rdd-dataset-and-dataframe">RDD, Dataset and DataFrame</a>
          <ul>
            <li><a href="#rdd">RDD</a></li>
            <li><a href="#dataframe">DataFrame</a></li>
            <li><a href="#dataset">DataSet</a></li>
          </ul>
        </li>
        <li><a href="#spark-dataframe-and-pandas-dataframe">Spark DataFrame and Pandas DataFrame</a>
          <ul>
            <li><a href="#origin-of-dataframe">Origin of DataFrame</a></li>
            <li><a href="#dataframe-data-model">DataFrame Data Model</a></li>
            <li><a href="#sparks-dataframe">Spark&rsquo;s DataFrame</a></li>
            <li><a href="#difference-between-pandas-dataframe-and-spark-dataframe">Difference between Pandas DataFrame and Spark DataFrame</a></li>
          </ul>
        </li>
        <li><a href="#dataframereader-class-and-dataframewriter-class">DataFrameReader class and DataFrameWriter class</a>
          <ul>
            <li><a href="#dataframereader-class">DataFrameReader class</a></li>
            <li><a href="#dataframewriter-class">DataFrameWriter class</a></li>
          </ul>
        </li>
        <li><a href="#pyspark-interaction-with-hive-database">PySpark Interaction with Hive Database</a></li>
        <li><a href="#spark-dataframe-operations">Spark DataFrame operations</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>The company&rsquo;s Jupyter environment supports PySpark. this makes it very easy to use PySpark to connect to Hive queries and use. Since I had no prior exposure to Spark at all, I put together some reference material.</p>
<h2 id="spark-context">Spark Context</h2>
<p>The core module in PySpark is SparkContext (sc for short), and the most important data carrier is RDD, which is like a NumPy array or a Pandas Series, and can be regarded as an ordered set of items. The RDD is like a NumPy array or a Pandas Series, which can be regarded as an ordered collection of items, but these items do not exist in the memory of the driver, but are divided into many partitions, and the data of each partition exists in the memory of the cluster executor.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/10/24/1a0430c01b5748afb25af5fd33bb4ef2.png" alt=""></p>
<p>SparkContext is the main entry point of Spark, if you consider Spark cluster as server, Spark Driver is the client, SparkContext is the core of the client; as the comment says SparkContext is used to connect to Spark cluster, create RDD, accumlator, broadcast variables, which is equivalent to the main function of the application.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/10/24/9e22d30a85b34c398d9810f4fcb29484.png" alt=""></p>
<p>Only one active SparkContext can exist in each JVM, and you must call stop() to close the previous SparkContext before creating a new one.</p>
<h2 id="spark-session">Spark Session</h2>
<p>Before Spark 2.0, SparkContext was the structure for all Spark functions, and the driver connected to the cluster (via resource manager) through SparkContext, because before 2.0, RDD was the foundation of Spark. If you need to create a SparkContext, you need SparkConf to configure the content of SparkContext through Conf.</p>
<p>After Spark2.0, Spark Session is also an entry point for Spark, in order to introduce dataframe and dataset APIs, while retaining the functionality of the original SparkContext, if you want to use the HIVE, SQL, Streaming APIs, you need Spark Session is the entry point.</p>
<p>SparkSession not only provides access to all the spark functions that sparkContext has, but also provides APIs for handling DataFrame and DataSet.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/10/24/adf15c249f5b4bf6b82073f2c6986667.png" alt=""></p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/10/24/4e04e5083e1c4d9f8cb42b706d4ce9f8.png" alt=""></p>
<p>Here&rsquo;s how to create a SparkSession.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">val</span> <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span>
<span class="o">.</span><span class="n">builder</span><span class="p">()</span>
<span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;Sparktest&#34;</span><span class="p">)</span>
<span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&#34;spark.some.config.option&#34;</span><span class="p">,</span> <span class="s2">&#34;some-value&#34;</span><span class="p">)</span>
<span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p>The following are the parameters of SparkContext.</p>
<ul>
<li>master - It is the URL of the cluster to connect to.</li>
<li>appName - The name of your job.</li>
<li>sparkHome - The Spark installation directory.</li>
<li>pyFiles - The .zip or .py files to send to the cluster and add to the PYTHONPATH.</li>
<li>environment - Work node environment variable.</li>
<li>batchSize - The number of Python objects represented as a single Java object. Set 1 to disable batching, 0 to automatically select batch size based on object size, or -1 to use unlimited batch size.</li>
<li>serializer - The RDD serializer.</li>
<li>Conf - An object of L {SparkConf} to set all Spark properties.</li>
<li>gateway - Use the existing gateway and JVM, otherwise initialize the new JVM.</li>
<li>JSC - JavaSparkContext instance.</li>
<li>profiler_cls - A custom class of Profiler used for performance analysis (default is profiler.BasicProfiler).</li>
</ul>
<p>In the above parameters, master and appname are mainly used.</p>
<p>Here&rsquo;s how we can create a SparkSession using Hive support.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">val</span> <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span>
<span class="o">.</span><span class="n">builder</span><span class="p">()</span>
<span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;SparkHivetest&#34;</span><span class="p">)</span>
<span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&#34;spark.sql.warehouse.dir&#34;</span><span class="p">,</span> <span class="n">warehouseLocation</span><span class="p">)</span>
<span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span>
<span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="rdd-dataset-and-dataframe">RDD, Dataset and DataFrame</h2>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/10/24/13110af7b6ba40cc969548bdf2f5d74b.png" alt=""></p>
<h3 id="rdd">RDD</h3>
<p>An RDD is an immutable collection of distributed elements of your data, distributed across nodes in a cluster, that can be processed in parallel by several underlying APIs that provide transformation and processing.</p>
<p>Scenarios for using RDDs:</p>
<ul>
<li>You want to be able to perform the most basic transformations, processing and control of your data set.</li>
<li>Your data is unstructured, such as streaming media or character streams.</li>
<li>You want to process your data through functional programming rather than domain-specific representations.</li>
<li>you don&rsquo;t want to define a schema as in columnar processing, processing or accessing data attributes by name or field.</li>
<li>you do not care for some of the optimization and performance benefits that can be gained by structured and semi-structured data processing through DataFrame and Dataset.</li>
</ul>
<p>Pros.</p>
<ul>
<li>Powerful, with many built-in function operations, group, map, filter, etc., to facilitate the handling of structured or unstructured data</li>
<li>object-oriented programming, direct storage of java objects, type conversion is also safe</li>
</ul>
<p>Disadvantages.</p>
<ul>
<li>because it is basically the same as hadoop universal, so there is no optimization for special scenarios, such as for structured data processing compared to sql to very troublesome</li>
<li>the default is the java serial number method, serialization results are relatively large, and the data is stored in the java heap memory, resulting in more frequent gc</li>
</ul>
<h3 id="dataframe">DataFrame</h3>
<p>DataFrame is a distributed data set based on RDD, similar to the two-dimensional tables in traditional databases. dataFrame introduces schema.</p>
<p>RDD and DataFrame comparison.</p>
<ul>
<li>Similarities: Both are immutable distributed elastic datasets.</li>
<li>Differences: DataFrame datasets are stored by specified columns, i.e. structured data. Similar to a table in a traditional database.</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/10/24/ad3e4230f4a742dd9f3a7b83bd70de22.png" alt=""></p>
<p>The above figure visualizes the difference between DataFrame and RDD.</p>
<ul>
<li>The RDD [Person] on the left has Person as the type parameter, but the Spark framework itself does not know the internal structure of the Person class. The DataFrame on the right side, however, provides detailed structure information, so that Spark SQL can clearly know which columns are contained in the dataset and what the name and type of each column are. dataFrame has more information about the structure of the data, i.e. schema.</li>
<li>RDD is a distributed collection of Java objects. dataFrame is a distributed collection of Row objects.</li>
<li>DataFrame provides richer arithmetic than RDD, but the more important feature is to improve the execution efficiency, reduce data reading and the optimization of execution plan, such as filter push down, crop, etc.</li>
</ul>
<p>Advantages.</p>
<ul>
<li>Structured data processing is very convenient, supporting kv data such as Avro, CSV, elastic search, and Cassandra, as well as traditional data tables such as HIVE tables, MySQL, etc.</li>
<li>targeted optimization, because the data structure meta information spark has been saved, serialization does not need to bring meta information, greatly reducing the size of serialization, and the data is saved in off-heap memory, reducing the number of gc.</li>
<li>hive compatible, support hql, udf, etc.</li>
</ul>
<p>Disadvantages.</p>
<ul>
<li>No type conversion safety check at compile time, runtime to determine if there is a problem</li>
<li>for object support is not friendly, rdd internal data stored directly in java objects, dataframe memory storage is row objects and can not be custom objects</li>
</ul>
<h3 id="dataset">DataSet</h3>
<p>A Dataset is a strongly typed domain-specific object that can be transformed in parallel by functional or relational operations. Each Dataset has an untyped view called a DataFrame, which is a dataset of rows. This DataFrame is a Dataset of type Row, i.e. Dataset[Row].</p>
<p>You can think of a DataFrame as an alias for a collection of some generic object Dataset[Row], and a row is a generic untyped JVM object. In contrast, a Dataset is a collection of JVM objects with explicit type definitions, specified by the Case Class you define in Scala or the Class in Java.</p>
<p>Datasets are &ldquo;lazy&rdquo;, triggering computation only when an action is performed. Essentially, a dataset represents a logical plan that describes the computation required to produce the data. When an action is performed, Spark&rsquo;s query optimizer optimizes the logical plan and generates an efficient parallel and distributed physical plan.</p>
<p>The main difference between a DataSet and an RDD is that a DataSet is a domain-specific collection of objects; however, an RDD is a collection of any objects. the DataSet API is always strongly typed; and it is possible to optimize using these patterns, however, an RDD is not.</p>
<p>Advantages.</p>
<ul>
<li>dataset integrates the advantages of rdd and dataframe, supporting both structured and unstructured data</li>
<li>Same support for custom object storage as rdd</li>
<li>Same as dataframe, supports sql queries for structured data</li>
<li>Out-of-heap memory storage, gc friendly</li>
<li>Type conversion safe, code friendly</li>
<li>Officially recommended to use dataset</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/10/24/387c4be095ec4b809ff87d513eda5ca5.png" alt=""></p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/10/24/fe088c3776e447cbbfb0463917435340.png" alt=""></p>
<h2 id="spark-dataframe-and-pandas-dataframe">Spark DataFrame and Pandas DataFrame</h2>
<h3 id="origin-of-dataframe">Origin of DataFrame</h3>
<p>The earliest &ldquo;DataFrame&rdquo; (which began to be called &ldquo;data frame&rdquo;), originated from the S language developed by Bell Labs. data frame&quot; was released in 1990, and its concepts are detailed in Chapter 3 of &ldquo;Statistical Models of the S Language&rdquo;, which highlights the matrix origin of the dataframe. The book describes DataFrame as looking very much like a matrix and supporting matrix-like operations; at the same time, it looks very much like a relational table.</p>
<p>The R language, an open source version of the S language, released its first stable version in 2000 and implemented dataframes. pandas was developed in 2009, and the concept of DataFrame was introduced in Python. These DataFrames are all homogeneous and share the same semantics and data model.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/10/24/ffc0a855556e4316aaaef47f5e15f07f.png" alt=""></p>
<h3 id="dataframe-data-model">DataFrame Data Model</h3>
<p>The need for a DataFrame comes from viewing data as a matrix and a table. However, matrices contain only one data type, which is too restrictive, and relational tables require that the data must first have a schema defined; for a DataFrame, its column types can be inferred at runtime and do not need to be known in advance, nor are all columns required to be of one type. Thus, a DataFrame can be thought of as a combination of a relational system, a matrix, or even a spreadsheet program (typically Excel).</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/10/24/542610c9326546b2817515b1ce1795ec.png" alt=""></p>
<p>Compared to relational systems, DataFrames have several particularly interesting properties that make DataFrames unique.</p>
<p><strong>Guaranteed order, column and row symmetry</strong></p>
<p>First, DataFrames are ordered in both row and column directions; and rows and columns are first-class citizens and are not treated differently.</p>
<p>Take pandas for example, when a DataFrame is created, the data is ordered in both rows and columns; therefore, you can use position to select data in both rows and columns.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">In</span> <span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>                                                     

<span class="n">In</span> <span class="p">[</span><span class="mi">2</span><span class="p">]:</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                                                      

<span class="n">In</span> <span class="p">[</span><span class="mi">3</span><span class="p">]:</span> <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>                                 

<span class="n">In</span> <span class="p">[</span><span class="mi">4</span><span class="p">]:</span> <span class="n">df</span>                                                                      
<span class="n">Out</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span> 
          <span class="mi">0</span>         <span class="mi">1</span>         <span class="mi">2</span>         <span class="mi">3</span>
<span class="mi">0</span>  <span class="mf">0.736385</span>  <span class="mf">0.271232</span>  <span class="mf">0.940270</span>  <span class="mf">0.926548</span>
<span class="mi">1</span>  <span class="mf">0.319533</span>  <span class="mf">0.891928</span>  <span class="mf">0.471176</span>  <span class="mf">0.583895</span>
<span class="mi">2</span>  <span class="mf">0.440825</span>  <span class="mf">0.500724</span>  <span class="mf">0.402782</span>  <span class="mf">0.109702</span>
<span class="mi">3</span>  <span class="mf">0.300279</span>  <span class="mf">0.483571</span>  <span class="mf">0.639299</span>  <span class="mf">0.778849</span>
<span class="mi">4</span>  <span class="mf">0.341113</span>  <span class="mf">0.813870</span>  <span class="mf">0.054731</span>  <span class="mf">0.059262</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">5</span><span class="p">]:</span> <span class="n">df</span><span class="o">.</span><span class="n">iat</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>  <span class="c1"># 第二行第二列元素                                                             </span>
<span class="n">Out</span><span class="p">[</span><span class="mi">5</span><span class="p">]:</span> <span class="mf">0.40278182653648853</span>

<span class="n">因为行和列的对称关系</span><span class="err">，</span><span class="n">因此聚合函数在两个方向上都可以计算</span><span class="err">，</span><span class="n">只需指定</span> <span class="n">axis</span> <span class="n">即可</span><span class="err">。</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="n">df</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># 默认 axis == 0，在行方向上做聚合，因此结果是4个元素                                                                </span>
<span class="n">Out</span><span class="p">[</span><span class="mi">6</span><span class="p">]:</span> 
<span class="mi">0</span>    <span class="mf">2.138135</span>
<span class="mi">1</span>    <span class="mf">2.961325</span>
<span class="mi">2</span>    <span class="mf">2.508257</span>
<span class="mi">3</span>    <span class="mf">2.458257</span>
<span class="n">dtype</span><span class="p">:</span> <span class="n">float64</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="n">df</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># axis == 1，在列方向上做聚合，因此是5个元素                                                        </span>
<span class="n">Out</span><span class="p">[</span><span class="mi">7</span><span class="p">]:</span> 
<span class="mi">0</span>    <span class="mf">2.874434</span>
<span class="mi">1</span>    <span class="mf">2.266533</span>
<span class="mi">2</span>    <span class="mf">1.454032</span>
<span class="mi">3</span>    <span class="mf">2.201998</span>
<span class="mi">4</span>    <span class="mf">1.268976</span>
<span class="n">dtype</span><span class="p">:</span> <span class="n">float64</span>
</code></pre></td></tr></table>
</div>
</div><p>Those familiar with numpy (the numerical computation library containing definitions of multidimensional arrays and matrices) can see that this feature is very familiar, and thus the matrix nature of DataFrame can be seen.</p>
<p><strong>Rich API</strong></p>
<p>The DataFrame API is very rich, spanning relational (e.g. filter, join), linear algebra (e.g. transpose, dot) and spreadsheet-like (e.g. pivot) operations.</p>
<p>Again using pandas as an example, a DataFrame can do transpose operations to get rows and columns to line up.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">In</span> <span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="n">df</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>                                                          
<span class="n">Out</span><span class="p">[</span><span class="mi">8</span><span class="p">]:</span> 
          <span class="mi">0</span>         <span class="mi">1</span>         <span class="mi">2</span>         <span class="mi">3</span>         <span class="mi">4</span>
<span class="mi">0</span>  <span class="mf">0.736385</span>  <span class="mf">0.319533</span>  <span class="mf">0.440825</span>  <span class="mf">0.300279</span>  <span class="mf">0.341113</span>
<span class="mi">1</span>  <span class="mf">0.271232</span>  <span class="mf">0.891928</span>  <span class="mf">0.500724</span>  <span class="mf">0.483571</span>  <span class="mf">0.813870</span>
<span class="mi">2</span>  <span class="mf">0.940270</span>  <span class="mf">0.471176</span>  <span class="mf">0.402782</span>  <span class="mf">0.639299</span>  <span class="mf">0.054731</span>
<span class="mi">3</span>  <span class="mf">0.926548</span>  <span class="mf">0.583895</span>  <span class="mf">0.109702</span>  <span class="mf">0.778849</span>  <span class="mf">0.059262</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>Intuitive syntax for interactive analysis</strong></p>
<p>Users can continuously explore DataFrame data, query results can be reused by subsequent results, and very complex operations can be very easily combined programmatically, making it well suited for interactive analysis.</p>
<p><strong>Heterogeneous data allowed in columns</strong></p>
<p>The DataFrame type system allows for the presence of heterogeneous data in a column, for example, an int column allows for the presence of string type data, which may be dirty data. This makes DataFrame very flexible.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">In</span> <span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>                                                        

<span class="n">In</span> <span class="p">[</span><span class="mi">11</span><span class="p">]:</span> <span class="n">df2</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;a&#39;</span>                                                   

<span class="n">In</span> <span class="p">[</span><span class="mi">12</span><span class="p">]:</span> <span class="n">df2</span>                                                                    
<span class="n">Out</span><span class="p">[</span><span class="mi">12</span><span class="p">]:</span> 
          <span class="mi">0</span>         <span class="mi">1</span>         <span class="mi">2</span>         <span class="mi">3</span>
<span class="mi">0</span>         <span class="n">a</span>  <span class="mf">0.271232</span>  <span class="mf">0.940270</span>  <span class="mf">0.926548</span>
<span class="mi">1</span>  <span class="mf">0.319533</span>  <span class="mf">0.891928</span>  <span class="mf">0.471176</span>  <span class="mf">0.583895</span>
<span class="mi">2</span>  <span class="mf">0.440825</span>  <span class="mf">0.500724</span>  <span class="mf">0.402782</span>  <span class="mf">0.109702</span>
<span class="mi">3</span>  <span class="mf">0.300279</span>  <span class="mf">0.483571</span>  <span class="mf">0.639299</span>  <span class="mf">0.778849</span>
<span class="mi">4</span>  <span class="mf">0.341113</span>  <span class="mf">0.813870</span>  <span class="mf">0.054731</span>  <span class="mf">0.059262</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>Data Model</strong></p>
<p>We can now formally define what a DataFrame really is.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/10/24/f2991113939e436799cc3068ec284af6.png" alt=""></p>
<p>A DataFrame consists of a two-dimensional array of mixed types, row labels, column labels, and types (types or domains). On each column, the type is optional and can be inferred at runtime. In terms of rows, a DataFrame can be viewed as a mapping of row labels to rows, with guaranteed order between rows; in terms of columns, it can be viewed as a mapping of column types to column labels to columns, again with guaranteed order between columns.</p>
<p>The existence of row labels and column labels makes it very convenient to select data.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">In</span> <span class="p">[</span><span class="mi">13</span><span class="p">]:</span> <span class="n">df</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="s1">&#39;2020-4-15&#39;</span><span class="p">,</span> <span class="n">periods</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>                       

<span class="n">In</span> <span class="p">[</span><span class="mi">14</span><span class="p">]:</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;c1&#39;</span><span class="p">,</span> <span class="s1">&#39;c2&#39;</span><span class="p">,</span> <span class="s1">&#39;c3&#39;</span><span class="p">,</span> <span class="s1">&#39;c4&#39;</span><span class="p">]</span>                                  

<span class="n">In</span> <span class="p">[</span><span class="mi">15</span><span class="p">]:</span> <span class="n">df</span>                                                                     
<span class="n">Out</span><span class="p">[</span><span class="mi">15</span><span class="p">]:</span> 
                  <span class="n">c1</span>        <span class="n">c2</span>        <span class="n">c3</span>        <span class="n">c4</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">15</span>  <span class="mf">0.736385</span>  <span class="mf">0.271232</span>  <span class="mf">0.940270</span>  <span class="mf">0.926548</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">16</span>  <span class="mf">0.319533</span>  <span class="mf">0.891928</span>  <span class="mf">0.471176</span>  <span class="mf">0.583895</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">17</span>  <span class="mf">0.440825</span>  <span class="mf">0.500724</span>  <span class="mf">0.402782</span>  <span class="mf">0.109702</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">18</span>  <span class="mf">0.300279</span>  <span class="mf">0.483571</span>  <span class="mf">0.639299</span>  <span class="mf">0.778849</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">19</span>  <span class="mf">0.341113</span>  <span class="mf">0.813870</span>  <span class="mf">0.054731</span>  <span class="mf">0.059262</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">16</span><span class="p">]:</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;2020-4-16&#39;</span><span class="p">:</span> <span class="s1">&#39;2020-4-18&#39;</span><span class="p">,</span> <span class="s1">&#39;c2&#39;</span><span class="p">:</span> <span class="s1">&#39;c3&#39;</span><span class="p">]</span>  <span class="c1"># 注意这里的切片是闭区间                         </span>
<span class="n">Out</span><span class="p">[</span><span class="mi">16</span><span class="p">]:</span> 
                  <span class="n">c2</span>        <span class="n">c3</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">16</span>  <span class="mf">0.891928</span>  <span class="mf">0.471176</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">17</span>  <span class="mf">0.500724</span>  <span class="mf">0.402782</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">18</span>  <span class="mf">0.483571</span>  <span class="mf">0.639299</span>
</code></pre></td></tr></table>
</div>
</div><p>Here index and columns are the row and column labels respectively. We can easily select a period of time (row selection) and several columns (column selection) of data. Of course, this is based on the fact that the data is stored sequentially.</p>
<p>This sequential storage makes DataFrame very suitable for statistical work.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">In</span> <span class="p">[</span><span class="mi">17</span><span class="p">]:</span> <span class="n">df3</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 把 df 的数据整体下移一格，行列索引保持不变                                                      </span>

<span class="n">In</span> <span class="p">[</span><span class="mi">18</span><span class="p">]:</span> <span class="n">df3</span>                                                                    
<span class="n">Out</span><span class="p">[</span><span class="mi">18</span><span class="p">]:</span> 
                  <span class="n">c1</span>        <span class="n">c2</span>        <span class="n">c3</span>        <span class="n">c4</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">15</span>       <span class="n">NaN</span>       <span class="n">NaN</span>       <span class="n">NaN</span>       <span class="n">NaN</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">16</span>  <span class="mf">0.736385</span>  <span class="mf">0.271232</span>  <span class="mf">0.940270</span>  <span class="mf">0.926548</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">17</span>  <span class="mf">0.319533</span>  <span class="mf">0.891928</span>  <span class="mf">0.471176</span>  <span class="mf">0.583895</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">18</span>  <span class="mf">0.440825</span>  <span class="mf">0.500724</span>  <span class="mf">0.402782</span>  <span class="mf">0.109702</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">19</span>  <span class="mf">0.300279</span>  <span class="mf">0.483571</span>  <span class="mf">0.639299</span>  <span class="mf">0.778849</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">19</span><span class="p">]:</span> <span class="n">df</span> <span class="o">-</span> <span class="n">df3</span>  <span class="c1"># 数据减法会自动按标签对齐，因此这一步可以用来计算环比                                                             </span>
<span class="n">Out</span><span class="p">[</span><span class="mi">19</span><span class="p">]:</span> 
                  <span class="n">c1</span>        <span class="n">c2</span>        <span class="n">c3</span>        <span class="n">c4</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">15</span>       <span class="n">NaN</span>       <span class="n">NaN</span>       <span class="n">NaN</span>       <span class="n">NaN</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">16</span> <span class="o">-</span><span class="mf">0.416852</span>  <span class="mf">0.620697</span> <span class="o">-</span><span class="mf">0.469093</span> <span class="o">-</span><span class="mf">0.342653</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">17</span>  <span class="mf">0.121293</span> <span class="o">-</span><span class="mf">0.391205</span> <span class="o">-</span><span class="mf">0.068395</span> <span class="o">-</span><span class="mf">0.474194</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">18</span> <span class="o">-</span><span class="mf">0.140546</span> <span class="o">-</span><span class="mf">0.017152</span>  <span class="mf">0.236517</span>  <span class="mf">0.669148</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">19</span>  <span class="mf">0.040834</span>  <span class="mf">0.330299</span> <span class="o">-</span><span class="mf">0.584568</span> <span class="o">-</span><span class="mf">0.719587</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">21</span><span class="p">]:</span> <span class="p">(</span><span class="n">df</span> <span class="o">-</span> <span class="n">df3</span><span class="p">)</span><span class="o">.</span><span class="n">bfill</span><span class="p">()</span>  <span class="c1"># 第一行的空数据按下一行填充                                                    </span>
<span class="n">Out</span><span class="p">[</span><span class="mi">21</span><span class="p">]:</span> 
                  <span class="n">c1</span>        <span class="n">c2</span>        <span class="n">c3</span>        <span class="n">c4</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">15</span> <span class="o">-</span><span class="mf">0.416852</span>  <span class="mf">0.620697</span> <span class="o">-</span><span class="mf">0.469093</span> <span class="o">-</span><span class="mf">0.342653</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">16</span> <span class="o">-</span><span class="mf">0.416852</span>  <span class="mf">0.620697</span> <span class="o">-</span><span class="mf">0.469093</span> <span class="o">-</span><span class="mf">0.342653</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">17</span>  <span class="mf">0.121293</span> <span class="o">-</span><span class="mf">0.391205</span> <span class="o">-</span><span class="mf">0.068395</span> <span class="o">-</span><span class="mf">0.474194</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">18</span> <span class="o">-</span><span class="mf">0.140546</span> <span class="o">-</span><span class="mf">0.017152</span>  <span class="mf">0.236517</span>  <span class="mf">0.669148</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">19</span>  <span class="mf">0.040834</span>  <span class="mf">0.330299</span> <span class="o">-</span><span class="mf">0.584568</span> <span class="o">-</span><span class="mf">0.719587</span>
</code></pre></td></tr></table>
</div>
</div><p>As we can see from the example, just because the data is stored in order, we can keep the index unchanged and move down one row as a whole, so that yesterday&rsquo;s data goes to today&rsquo;s row, and then when we take the original data and subtract the displaced data, because DataFrame will automatically do alignment by label, so for a date, it is equivalent to subtracting the previous day&rsquo;s data from the day&rsquo;s data, so that we can do something like ring-by-ring operation. This is incredibly convenient. I&rsquo;m afraid that for a relational system, you&rsquo;d need to find a column to use as a join condition, and then do the subtraction, etc. Finally, for empty data, we can also fill in the previous row (ffill) or the next row (bfill). Trying to achieve the same effect in a relational system would require a lot of work.</p>
<h3 id="sparks-dataframe">Spark&rsquo;s DataFrame</h3>
<p>Spark brings the concept of &ldquo;DataFrame&rdquo; to the Big Data space. Spark DataFrame only contains the semantics of relational tables, the schema needs to be determined, and the data is not guaranteed to be sequential.</p>
<h3 id="difference-between-pandas-dataframe-and-spark-dataframe">Difference between Pandas DataFrame and Spark DataFrame</h3>
<p>TODO</p>
<h2 id="dataframereader-class-and-dataframewriter-class">DataFrameReader class and DataFrameWriter class</h2>
<h3 id="dataframereader-class">DataFrameReader class</h3>
<p>Reading data from an external storage system and returning a DataFrame object is usually accessed using SparkSession.read. The common syntax is to first call the format() function to specify the format of the input data, and then call the load() function to load the data from the data source and return the DataFrame object.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/people.json&#39;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>For the different formats, the DataFrameReader class has subdivided functions to load the data.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">df_csv</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/ages.csv&#39;</span><span class="p">)</span>
<span class="n">df_json</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/people.json&#39;</span><span class="p">)</span>
<span class="n">df_txt</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/text-test.txt&#39;</span><span class="p">)</span>
<span class="n">df_parquet</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/parquet_partitioned&#39;</span><span class="p">)</span>

<span class="c1"># read a table as a DataFrame</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/parquet_partitioned&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s1">&#39;tmpTable&#39;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s1">&#39;tmpTable&#39;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>DataFrame can also be constructed from JDBC URLs via jdbc</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">jdbc</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">table</span><span class="p">,</span> <span class="n">column</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lowerBound</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">upperBound</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">numPartitions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">predicates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">properties</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="dataframewriter-class">DataFrameWriter class</h3>
<p>Used to write a DataFrame to an external storage system, accessible via DataFrame.write.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;parquet&#39;</span><span class="p">)</span>  
    <span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&#34;overwrite&#34;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s1">&#39;bucketed_table&#39;</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><p>Function Comments.</p>
<ul>
<li>format(source): Specify the format of the source of the underlying output</li>
<li>mode(saveMode): Specify the behavior of the data storage when the data or table already exists. Save modes are: append, overwrite, error, and ignore.</li>
<li>saveAsTable(name, format=None, mode=None, partitionBy=None, **options): store the DataFrame as a table</li>
<li>save(path=None, format=None, mode=None, partitionBy=None, **options): store the DataFrame to the data source</li>
</ul>
<p>For different formats, the DataFrameWriter class has subdivision functions to load the data.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">txt</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>

<span class="c1">#wirte data to external database via jdbc</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">jdbc</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">table</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">properties</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>Storing the DataFrame content to the source.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&#34;append&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><p>To store the contents of a DataFrame into a table.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;db_name.table_name&#39;</span><span class="p">,</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;delta&#39;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="pyspark-interaction-with-hive-database">PySpark Interaction with Hive Database</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># 从SQL查询中创建DataFrame</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&#34;SELECT field1 AS f1, field2 as f2 from table1&#34;</span><span class="p">)</span>

<span class="c1"># 直接把dataframe的内容写入到目标hive表</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="p">()</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&#34;overwrite&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s2">&#34;tableName&#34;</span><span class="p">);</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;col1&#34;</span><span class="p">),</span><span class="n">df</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;col2&#34;</span><span class="p">))</span><span class="o">.</span><span class="n">write</span><span class="p">()</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&#34;overwrite&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s2">&#34;schemaName.tableName&#34;</span><span class="p">);</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="p">()</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="n">SaveMode</span><span class="o">.</span><span class="n">Overwrite</span><span class="p">)</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s2">&#34;dbName.tableName&#34;</span><span class="p">);</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="spark-dataframe-operations">Spark DataFrame operations</h2>
<p>Spark dataframe is immutable, so each return is a new dataframe</p>
<p><strong>Column operation</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># add a new column</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;newCol&#34;</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">oldCol</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># replace the old column</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;oldCol&#34;</span><span class="p">,</span><span class="n">newCol</span><span class="p">)</span>

<span class="c1"># rename the column</span>
<span class="n">data</span><span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&#34;oldName&#34;</span><span class="p">,</span><span class="s2">&#34;newName&#34;</span><span class="p">)</span>

<span class="c1"># change column data type</span>
<span class="n">data</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&#34;oldColumn&#34;</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">oldColumn</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&#34;integer&#34;</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>Conditional filtering of data</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># filter data by pass a string</span>
<span class="n">temp1</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s2">&#34;col &gt; 1000&#34;</span><span class="p">)</span>

<span class="c1"># filter data by pass a column of boolean value</span>
<span class="n">temp2</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">col</span> <span class="o">&gt;</span> <span class="mi">1000</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>Select data</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># select based on column name</span>
<span class="n">temp1</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;col1&#34;</span><span class="p">,</span><span class="s2">&#34;col2&#34;</span><span class="p">)</span>
<span class="n">temp1</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;col1*100 as newCol1&#34;</span><span class="p">)</span>

<span class="c1"># select based on column object</span>
<span class="n">temp2</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">col1</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">col2</span><span class="p">)</span>
<span class="n">temp2</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">col1</span><span class="o">+</span><span class="mf">1.</span><span class="n">alias</span><span class="p">(</span><span class="n">newCol1</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>aggregate function</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># get the minimum value of a column</span>
<span class="n">data</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="s2">&#34;col1&#34;</span><span class="p">)</span>

<span class="c1"># group by on certain column and do calculation</span>
<span class="n">data</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&#34;col1&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s2">&#34;col2&#34;</span><span class="p">)</span>

<span class="c1"># agg function</span>
<span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="nn">F</span>
<span class="n">data</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&#34;a&#34;</span><span class="p">,</span><span class="s2">&#34;b&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">stddev</span><span class="p">(</span><span class="s2">&#34;c&#34;</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>Consolidated Data Sheet</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="n">newData</span> <span class="o">=</span> <span class="n">data1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="s2">&#34;col&#34;</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="s2">&#34;leftouter&#34;</span><span class="p">)</span>
<span class="n">newData</span> <span class="o">=</span> <span class="n">data1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">data1</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">data2</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">])</span>
</code></pre></td></tr></table>
</div>
</div>
    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/jupyter/">jupyter</a>
          <a href="/tags/pyspark/">pyspark</a>
          <a href="/tags/hive/">hive</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2021-10/python-compress-decompress/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Compressing and Decompressing with Python</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/2021-10/decision-tree-visualizations/">
            <span class="next-text nav-default">Decision tree visualization methods and techniques</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="https://www.sobyte.net/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2021 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
