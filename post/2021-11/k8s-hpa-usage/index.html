<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Kubernetes HPA Usage Explained - SoByte</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6356451834813761" crossorigin="anonymous"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E8GRRGBTEZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E8GRRGBTEZ');
</script>


<meta name="author" content="" /><meta name="description" content="In the previous study, we used a kubectl scale command to implement Pod scaling, but this is after all a completely manual operation. To this end, Kubernetes also provides us with such a resource object: Horizontal Pod Autoscaling, or HPA for short, which monitors and analyzes the load changes of all Pods controlled by some controllers to determine whether the number of copies of Pods needs to be adjusted. The" /><meta name="keywords" content="k8s, Hpa" />






<meta name="generator" content="Hugo 0.92.2 with theme even" />


<link rel="canonical" href="https://www.sobyte.net/post/2021-11/k8s-hpa-usage/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Kubernetes HPA Usage Explained" />
<meta property="og:description" content="In the previous study, we used a kubectl scale command to implement Pod scaling, but this is after all a completely manual operation. To this end, Kubernetes also provides us with such a resource object: Horizontal Pod Autoscaling, or HPA for short, which monitors and analyzes the load changes of all Pods controlled by some controllers to determine whether the number of copies of Pods needs to be adjusted. The" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.sobyte.net/post/2021-11/k8s-hpa-usage/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-11-21T20:21:02+08:00" />
<meta property="article:modified_time" content="2021-11-21T20:21:02+08:00" />

<meta itemprop="name" content="Kubernetes HPA Usage Explained">
<meta itemprop="description" content="In the previous study, we used a kubectl scale command to implement Pod scaling, but this is after all a completely manual operation. To this end, Kubernetes also provides us with such a resource object: Horizontal Pod Autoscaling, or HPA for short, which monitors and analyzes the load changes of all Pods controlled by some controllers to determine whether the number of copies of Pods needs to be adjusted. The"><meta itemprop="datePublished" content="2021-11-21T20:21:02+08:00" />
<meta itemprop="dateModified" content="2021-11-21T20:21:02+08:00" />
<meta itemprop="wordCount" content="5741">
<meta itemprop="keywords" content="k8s," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kubernetes HPA Usage Explained"/>
<meta name="twitter:description" content="In the previous study, we used a kubectl scale command to implement Pod scaling, but this is after all a completely manual operation. To this end, Kubernetes also provides us with such a resource object: Horizontal Pod Autoscaling, or HPA for short, which monitors and analyzes the load changes of all Pods controlled by some controllers to determine whether the number of copies of Pods needs to be adjusted. The"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">SoByte</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="/ukraine/">
        <li class="mobile-menu-item">UKRAINE</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">SoByte</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/ukraine/">UKRAINE</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Kubernetes HPA Usage Explained</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-11-21 20:21:02 </span>
        <div class="post-category">
            <a href="/categories/skills/"> skills </a>
            </div>
          <span class="more-meta"> 5741 words </span>
          <span class="more-meta"> 12 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#metrics-server">Metrics Server</a>
          <ul>
            <li><a href="#aggregator-api">Aggregator API</a></li>
            <li><a href="#installation">Installation</a></li>
          </ul>
        </li>
        <li><a href="#cpu-based">CPU-based</a></li>
        <li><a href="#memory-based">Memory-based</a></li>
        <li><a href="#based-on-custom-metrics">Based on custom metrics</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>In the previous study, we used a <code>kubectl scale</code> command to implement Pod scaling, but this is after all a completely manual operation. To this end, Kubernetes also provides us with such a resource object: <code>Horizontal Pod Autoscaling</code>, or <code>HPA</code> for short, which monitors and analyzes the load changes of all Pods controlled by some controllers to determine whether the number of copies of Pods needs to be adjusted. The basic principle of HPA is.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/11/21/a603eeb5a85141f3973840bd907a3d45.png" alt=""></p>
<p>We can simply create an HPA resource object with the <code>kubectl autoscale</code> command, and the <code>HPA Controller</code> will poll once by default <code>30s</code> (which can be set with the <code>-kube-controller-manager</code> <code>-horizontal-pod-autoscaler- sync-period</code> parameter of <code>-kube-controller-manager</code>) to query the Pod resource utilization in the specified resource and compare it with the value and metrics set at creation time to achieve the auto-scaling feature.</p>
<h2 id="metrics-server">Metrics Server</h2>
<p>In the first version of HPA, we needed <code>Heapster</code> to provide CPU and memory metrics, and after HPA v2, we needed to install Metrcis Server, <code>Metrics Server</code> to expose monitoring data through the standard Kubernetes API.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">https://10.96.0.1/apis/metrics.k8s.io/v1beta1/namespaces/&lt;namespace-name&gt;/pods/&lt;pod-name&gt;
</code></pre></td></tr></table>
</div>
</div><p>For example, when we access the API above, we can get the resource data of the Pod, which is actually collected from the <code>Summary API</code> of the kubelet. However, it should be noted that we can get resource monitoring data through the standard API here, not because <code>Metrics Server</code> is part of the APIServer, but through the <code>Aggregator</code> aggregation plugin provided by Kubernetes, which runs independently of the APIServer.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/11/21/dadacf98937e4fbabc6e5756b3918198.png" alt=""></p>
<h3 id="aggregator-api">Aggregator API</h3>
<p><code>Aggregator</code> allows developers to write a service of their own, register it with the Kubernetes APIServer, so we can use our own API like the native APIServer provides, we run our service inside the Kubernetes cluster, and then The Kubernetes <code>Aggregator</code> can be forwarded to the Service we wrote by the Service name. This aggregation layer brings a number of benefits.</p>
<ul>
<li>Increased API extensibility, developers can write their own API services to expose the APIs they want.</li>
<li>Enriching the API, the core kubernetes team blocked many new API proposals by allowing developers to expose their APIs as separate services, without the need for cumbersome community review.</li>
<li>Develop experimental APIs in phases, so that new APIs can be developed in a separate aggregation service, and when it is stable, it is easy to merge in the APIServer.</li>
<li>Ensure that new APIs follow Kubernetes conventions. Without the mechanisms proposed here, community members may be forced to roll their own stuff, which is likely to result in inconsistencies between community members and community conventions.</li>
</ul>
<h3 id="installation">Installation</h3>
<p>So now we need to install the <code>Metrics Server</code> service in the cluster if we want to use HPA. To install <code>Metrics Server</code> we need to turn on <code>Aggregator</code> because <code>Metrics Server</code> is scaled through this agent, but our cluster is built through Kubeadm and is already turned on by default. If the cluster is installed in binary way, you need to configure kube-apsierver separately adding the parameters shown below.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">--requestheader-client-ca-file<span class="o">=</span>&lt;path to aggregator CA cert&gt;
--requestheader-allowed-names<span class="o">=</span>aggregator
--requestheader-extra-headers-prefix<span class="o">=</span>X-Remote-Extra-
--requestheader-group-headers<span class="o">=</span>X-Remote-Group
--requestheader-username-headers<span class="o">=</span>X-Remote-User
--proxy-client-cert-file<span class="o">=</span>&lt;path to aggregator proxy cert&gt;
--proxy-client-key-file<span class="o">=</span>&lt;path to aggregator proxy key&gt;
</code></pre></td></tr></table>
</div>
</div><p>If <code>kube-proxy</code> is not running on the same host as the APIServer, then you need to make sure that the following kube-apsierver parameters are enabled.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">--enable-aggregator-routing<span class="o">=</span><span class="nb">true</span>
</code></pre></td></tr></table>
</div>
</div><p>For the way these certificates are generated, we can check the official documentation: <a href="https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/concepts/auth.md">https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/concepts/auth.md</a>.</p>
<p>Once the <code>Aggregator</code> aggregation layer is started, it&rsquo;s time to install <code>Metrics Server</code> and we can get the official list of installation resources for this repository at</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ git clone https://github.com/kubernetes-incubator/metrics-server
$ <span class="nb">cd</span> metrics-server
$ kubectl apply -f deploy/1.8+/
</code></pre></td></tr></table>
</div>
</div><p>Before deployment, modify the image address of <code>metrcis-server/deploy/1.8+/metrics-server-deployment.yaml</code> to</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">metrics-server</span><span class="w">
</span><span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">gcr.azk8s.cn/google_containers/metrics-server-amd64:v0.3.6</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>When the deployment is complete, you can check the Pod logs to see if they are OK.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl get pods -n kube-system -l k8s-app<span class="o">=</span>metrics-server
NAME                              READY   STATUS    RESTARTS   AGE
metrics-server-6886856d7c-g5k6q   1/1     Running   <span class="m">0</span>          2m39s
$ kubectl logs -f metrics-server-6886856d7c-g5k6q -n kube-system
......
E1119 09:05:57.234312       <span class="m">1</span> manager.go:111<span class="o">]</span> unable to fully collect metrics: <span class="o">[</span>unable to fully scrape metrics from <span class="nb">source</span> kubelet_summary:ydzs-node1: unable to fetch metrics from Kubelet ydzs-node1 <span class="o">(</span>ydzs-node1<span class="o">)</span>: Get https://ydzs-node1:10250/stats/summary?only_cpu_and_memory<span class="o">=</span>true: dial tcp: lookup ydzs-node1 on 10.96.0.10:53: no such host, unable to fully scrape metrics from <span class="nb">source</span> kubelet_summary:ydzs-node4: unable to fetch metrics from Kubelet ydzs-node4 <span class="o">(</span>ydzs-node4<span class="o">)</span>: Get https://ydzs-node4:10250/stats/summary?only_cpu_and_memory<span class="o">=</span>true: dial tcp: lookup ydzs-node4 on 10.96.0.10:53: no such host, unable to fully scrape metrics from <span class="nb">source</span> kubelet_summary:ydzs-node3: unable to fetch metrics from Kubelet ydzs-node3 <span class="o">(</span>ydzs-node3<span class="o">)</span>: Get https://ydzs-node3:10250/stats/summary?only_cpu_and_memory<span class="o">=</span>true: dial tcp: lookup ydzs-node3 on 10.96.0.10:53: no such host, unable to fully scrape metrics from <span class="nb">source</span> kubelet_summary:ydzs-master: unable to fetch metrics from Kubelet ydzs-master <span class="o">(</span>ydzs-master<span class="o">)</span>: Get https://ydzs-master:10250/stats/summary?only_cpu_and_memory<span class="o">=</span>true: dial tcp: lookup ydzs-master on 10.96.0.10:53: no such host, unable to fully scrape metrics from <span class="nb">source</span> kubelet_summary:ydzs-node2: unable to fetch metrics from Kubelet ydzs-node2 <span class="o">(</span>ydzs-node2<span class="o">)</span>: Get https://ydzs-node2:10250/stats/summary?only_cpu_and_memory<span class="o">=</span>true: dial tcp: lookup ydzs-node2 on 10.96.0.10:53: no such host<span class="o">]</span>
</code></pre></td></tr></table>
</div>
</div><p>We can find some error messages in the Pod: <code>xxx: no such host</code>, we can see that this error message is generally determined to be caused by DNS failure to resolve, we can see that Metrics Server will get information through port 10250 of the kubelet, using the hostname, when we deploy the cluster When we deploy the cluster, we add the hostname and ip mapping of the node in <code>/etc/hosts</code>, but our Metrics Server Pod does not have this hosts information internally, so of course, it does not recognize the hostname, to solve this problem, there are two methods.</p>
<p>The first method is to add hostname resolution to the DNS service inside the cluster, for example, if we are using CoreDNS in our cluster, we can modify the Configmap information of CoreDNS and add the hosts information as follows.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="l">$ kubectl edit configmap coredns -n kube-system</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">Corefile</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span><span class="sd">    .:53 {
</span><span class="sd">        errors
</span><span class="sd">        health
</span><span class="sd">        hosts {  # 添加集群节点hosts隐射信息
</span><span class="sd">          10.151.30.11 ydzs-master
</span><span class="sd">          10.151.30.57 ydzs-node3
</span><span class="sd">          10.151.30.59 ydzs-node4
</span><span class="sd">          10.151.30.22 ydzs-node1
</span><span class="sd">          10.151.30.23 ydzs-node2
</span><span class="sd">          fallthrough
</span><span class="sd">        }
</span><span class="sd">        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span><span class="sd">           pods insecure
</span><span class="sd">           upstream
</span><span class="sd">           fallthrough in-addr.arpa ip6.arpa
</span><span class="sd">        }
</span><span class="sd">        prometheus :9153
</span><span class="sd">        proxy . /etc/resolv.conf
</span><span class="sd">        cache 30
</span><span class="sd">        reload
</span><span class="sd">    }</span><span class="w">    
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">creationTimestamp</span><span class="p">:</span><span class="w"> </span><span class="ld">2019-05-18T11:07:46Z</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">coredns</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Another way is to modify the <code>kubelet-preferred-address-types</code> parameter in the metrics-server startup parameters, as follows.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">args</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- --<span class="l">cert-dir=/tmp</span><span class="w">
</span><span class="w"></span>- --<span class="l">secure-port=4443</span><span class="w">
</span><span class="w"></span>- --<span class="l">kubelet-preferred-address-types=InternalIP</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>We use the second way here and reinstall.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">$ kubectl get pods -n kube-system -l k8s-app=metrics-server
NAME                              READY   STATUS        RESTARTS   AGE
metrics-server-6dcfdf89b5-tvdcp   1/1     Running       0          33s
$ kubectl logs -f metric-metrics-server-58fc94d9f-jlxcb -n kube-system
......
E1119 09:08:49.805959       1 manager.go:111] unable to fully collect metrics: [unable to fully scrape metrics from source kubelet_summary:ydzs-node3: unable to fetch metrics from Kubelet ydzs-node3 (10.151.30.57): Get https://10.151.30.57:10250/stats/summary?only_cpu_and_memory=true: x509: cannot validate certificate for 10.151.30.57 because it doesn&#39;t contain any IP SANs, unable to fully scrape metrics from source kubelet_summary:ydzs-node4: unable to fetch metrics from Kubelet ydzs-node4 (10.151.30.59): Get https://10.151.30.59:10250/stats/summary?only_cpu_and_memory=true: x509: cannot validate certificate for 10.151.30.59 because it doesn&#39;t contain any IP SANs, unable to fully scrape metrics from source kubelet_summary:ydzs-node2: unable to fetch metrics from Kubelet ydzs-node2 (10.151.30.23): Get https://10.151.30.23:10250/stats/summary?only_cpu_and_memory=true: x509: cannot validate certificate for 10.151.30.23 because it doesn&#39;t contain any IP SANs, unable to fully scrape metrics from source kubelet_summary:ydzs-master: unable to fetch metrics from Kubelet ydzs-master (10.151.30.11): Get https://10.151.30.11:10250/stats/summary?only_cpu_and_memory=true: x509: cannot validate certificate for 10.151.30.11 because it doesn&#39;t contain any IP SANs, unable to fully scrape metrics from source kubelet_summary:ydzs-node1: unable to fetch metrics from Kubelet ydzs-node1 (10.151.30.22): Get https://10.151.30.22:10250/stats/summary?only_cpu_and_memory=true: x509: cannot validate certificate for 10.151.30.22 because it doesn&#39;t contain any IP SANs]
</code></pre></td></tr></table>
</div>
</div><p>Because the CA certificate is not signed with the IPs of each node when the cluster is deployed, so when <code>Metrics Server</code> requests it by IP, the signed certificate does not have a corresponding IP (error: <code>x509: cannot validate certificate for 10.151.30.22 because it doesn't contain any IP SANs</code>), we can add a <code>-kubelet-insecure-tls</code> parameter to skip the certificate verification: <code>-kubelet-insecure-tls</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">args</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- --<span class="l">cert-dir=/tmp</span><span class="w">
</span><span class="w"></span>- --<span class="l">secure-port=4443</span><span class="w">
</span><span class="w"></span>- --<span class="l">kubelet-insecure-tls</span><span class="w">
</span><span class="w"></span>- --<span class="l">kubelet-preferred-address-types=InternalIP</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Then reinstall it again and it will work! This can be verified with the following command.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl apply -f deploy/1.8+/
$ kubectl get pods -n kube-system -l k8s-app<span class="o">=</span>metrics-server
NAME                              READY   STATUS    RESTARTS   AGE
metrics-server-5d4dbb78bb-6klw6   1/1     Running   <span class="m">0</span>          14s
$ kubectl logs -f metrics-server-5d4dbb78bb-6klw6 -n kube-system
I1119 09:10:44.249092       <span class="m">1</span> serving.go:312<span class="o">]</span> Generated self-signed cert <span class="o">(</span>/tmp/apiserver.crt, /tmp/apiserver.key<span class="o">)</span>
I1119 09:10:45.264076       <span class="m">1</span> secure_serving.go:116<span class="o">]</span> Serving securely on <span class="o">[</span>::<span class="o">]</span>:4443
$ kubectl get apiservice <span class="p">|</span> grep metrics
v1beta1.metrics.k8s.io                 kube-system/metrics-server   True        9m
$ kubectl get --raw <span class="s2">&#34;/apis/metrics.k8s.io/v1beta1/nodes&#34;</span>
<span class="o">{</span><span class="s2">&#34;kind&#34;</span>:<span class="s2">&#34;NodeMetricsList&#34;</span>,<span class="s2">&#34;apiVersion&#34;</span>:<span class="s2">&#34;metrics.k8s.io/v1beta1&#34;</span>,<span class="s2">&#34;metadata&#34;</span>:<span class="o">{</span><span class="s2">&#34;selfLink&#34;</span>:<span class="s2">&#34;/apis/metrics.k8s.io/v1beta1/nodes&#34;</span><span class="o">}</span>,<span class="s2">&#34;items&#34;</span>:<span class="o">[{</span><span class="s2">&#34;metadata&#34;</span>:<span class="o">{</span><span class="s2">&#34;name&#34;</span>:<span class="s2">&#34;ydzs-node3&#34;</span>,<span class="s2">&#34;selfLink&#34;</span>:<span class="s2">&#34;/apis/metrics.k8s.io/v1beta1/nodes/ydzs-node3&#34;</span>,<span class="s2">&#34;creationTimestamp&#34;</span>:<span class="s2">&#34;2019-11-19T09:11:53Z&#34;</span><span class="o">}</span>,<span class="s2">&#34;timestamp&#34;</span>:<span class="s2">&#34;2019-11-19T09:11:38Z&#34;</span>,<span class="s2">&#34;window&#34;</span>:<span class="s2">&#34;30s&#34;</span>,<span class="s2">&#34;usage&#34;</span>:<span class="o">{</span><span class="s2">&#34;cpu&#34;</span>:<span class="s2">&#34;240965441n&#34;</span>,<span class="s2">&#34;memory&#34;</span>:<span class="s2">&#34;3004360Ki&#34;</span><span class="o">}}</span>,<span class="o">{</span><span class="s2">&#34;metadata&#34;</span>:<span class="o">{</span><span class="s2">&#34;name&#34;</span>:<span class="s2">&#34;ydzs-node4&#34;</span>,<span class="s2">&#34;selfLink&#34;</span>:<span class="s2">&#34;/apis/metrics.k8s.io/v1beta1/nodes/ydzs-node4&#34;</span>,<span class="s2">&#34;creationTimestamp&#34;</span>:<span class="s2">&#34;2019-11-19T09:11:53Z&#34;</span><span class="o">}</span>,<span class="s2">&#34;timestamp&#34;</span>:<span class="s2">&#34;2019-11-19T09:11:37Z&#34;</span>,<span class="s2">&#34;window&#34;</span>:<span class="s2">&#34;30s&#34;</span>,<span class="s2">&#34;usage&#34;</span>:<span class="o">{</span><span class="s2">&#34;cpu&#34;</span>:<span class="s2">&#34;167036681n&#34;</span>,<span class="s2">&#34;memory&#34;</span>:<span class="s2">&#34;2574664Ki&#34;</span><span class="o">}}</span>,<span class="o">{</span><span class="s2">&#34;metadata&#34;</span>:<span class="o">{</span><span class="s2">&#34;name&#34;</span>:<span class="s2">&#34;ydzs-master&#34;</span>,<span class="s2">&#34;selfLink&#34;</span>:<span class="s2">&#34;/apis/metrics.k8s.io/v1beta1/nodes/ydzs-master&#34;</span>,<span class="s2">&#34;creationTimestamp&#34;</span>:<span class="s2">&#34;2019-11-19T09:11:53Z&#34;</span><span class="o">}</span>,<span class="s2">&#34;timestamp&#34;</span>:<span class="s2">&#34;2019-11-19T09:11:38Z&#34;</span>,<span class="s2">&#34;window&#34;</span>:<span class="s2">&#34;30s&#34;</span>,<span class="s2">&#34;usage&#34;</span>:<span class="o">{</span><span class="s2">&#34;cpu&#34;</span>:<span class="s2">&#34;350907350n&#34;</span>,<span class="s2">&#34;memory&#34;</span>:<span class="s2">&#34;2986716Ki&#34;</span><span class="o">}}</span>,<span class="o">{</span><span class="s2">&#34;metadata&#34;</span>:<span class="o">{</span><span class="s2">&#34;name&#34;</span>:<span class="s2">&#34;ydzs-node1&#34;</span>,<span class="s2">&#34;selfLink&#34;</span>:<span class="s2">&#34;/apis/metrics.k8s.io/v1beta1/nodes/ydzs-node1&#34;</span>,<span class="s2">&#34;creationTimestamp&#34;</span>:<span class="s2">&#34;2019-11-19T09:11:53Z&#34;</span><span class="o">}</span>,<span class="s2">&#34;timestamp&#34;</span>:<span class="s2">&#34;2019-11-19T09:11:39Z&#34;</span>,<span class="s2">&#34;window&#34;</span>:<span class="s2">&#34;30s&#34;</span>,<span class="s2">&#34;usage&#34;</span>:<span class="o">{</span><span class="s2">&#34;cpu&#34;</span>:<span class="s2">&#34;1319638039n&#34;</span>,<span class="s2">&#34;memory&#34;</span>:<span class="s2">&#34;2094376Ki&#34;</span><span class="o">}}</span>,<span class="o">{</span><span class="s2">&#34;metadata&#34;</span>:<span class="o">{</span><span class="s2">&#34;name&#34;</span>:<span class="s2">&#34;ydzs-node2&#34;</span>,<span class="s2">&#34;selfLink&#34;</span>:<span class="s2">&#34;/apis/metrics.k8s.io/v1beta1/nodes/ydzs-node2&#34;</span>,<span class="s2">&#34;creationTimestamp&#34;</span>:<span class="s2">&#34;2019-11-19T09:11:53Z&#34;</span><span class="o">}</span>,<span class="s2">&#34;timestamp&#34;</span>:<span class="s2">&#34;2019-11-19T09:11:36Z&#34;</span>,<span class="s2">&#34;window&#34;</span>:<span class="s2">&#34;30s&#34;</span>,<span class="s2">&#34;usage&#34;</span>:<span class="o">{</span><span class="s2">&#34;cpu&#34;</span>:<span class="s2">&#34;320381888n&#34;</span>,<span class="s2">&#34;memory&#34;</span>:<span class="s2">&#34;3270368Ki&#34;</span><span class="o">}}]}</span>
$ kubectl top nodes
NAME          CPU<span class="o">(</span>cores<span class="o">)</span>   CPU%   MEMORY<span class="o">(</span>bytes<span class="o">)</span>   MEMORY%   
ydzs-master   351m         17%    2916Mi          79%       
ydzs-node1    1320m        33%    2045Mi          26%       
ydzs-node2    321m         8%     3193Mi          41%       
ydzs-node3    241m         6%     2933Mi          37%       
ydzs-node4    168m         4%     2514Mi          32% 
</code></pre></td></tr></table>
</div>
</div><p>Now we can get the resource data with the <code>kubectl top</code> command, which proves that <code>Metrics Server</code> has been installed successfully.</p>
<h2 id="cpu-based">CPU-based</h2>
<p>Now we use Deployment to create an Nginx Pod, and then use <code>HPA</code> to automate the scaling. The list of resources is shown below: (hpa-demo.yaml)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hpa-demo</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Then create the Deployment directly.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl apply -f hpa-demo.yaml
deployment.apps/hpa-demo created
$ kubectl get pods -l <span class="nv">app</span><span class="o">=</span>nginx
NAME                        READY   STATUS    RESTARTS   AGE
hpa-demo-85ff79dd56-pz8th   1/1     Running   <span class="m">0</span>          21s
</code></pre></td></tr></table>
</div>
</div><p>Now let&rsquo;s create an <code>HPA</code> resource object, which can be created using the <code>kubectl autoscale</code> command.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl autoscale deployment hpa-demo --cpu-percent<span class="o">=</span><span class="m">10</span> --min<span class="o">=</span><span class="m">1</span> --max<span class="o">=</span><span class="m">10</span>
horizontalpodautoscaler.autoscaling/hpa-demo autoscaled
$ kubectl get hpa
NAME       REFERENCE             TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   &lt;unknown&gt;/10%   <span class="m">1</span>         <span class="m">10</span>        <span class="m">1</span>          16s
</code></pre></td></tr></table>
</div>
</div><p>This command creates an HPA with the associated resource hpa-demo, with a minimum number of Pod copies of 1 and a maximum of 10. The HPA dynamically increases or decreases the number of Pods according to a set cpu usage rate (10%).</p>
<p>Of course, we can still create HPA resource objects by creating YAML files. If we don&rsquo;t know how to write one, we can look at the YAML file for the HPA created on the command line above.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="l">$ kubectl get hpa hpa-demo -o yaml</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">autoscaling/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">HorizontalPodAutoscaler</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">autoscaling.alpha.kubernetes.io/conditions</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;[{&#34;type&#34;:&#34;AbleToScale&#34;,&#34;status&#34;:&#34;True&#34;,&#34;lastTransitionTime&#34;:&#34;2019-11-19T09:15:12Z&#34;,&#34;reason&#34;:&#34;SucceededGetScale&#34;,&#34;message&#34;:&#34;the
</span><span class="s1">      HPA controller was able to get the target&#39;&#39;s current scale&#34;},{&#34;type&#34;:&#34;ScalingActive&#34;,&#34;status&#34;:&#34;False&#34;,&#34;lastTransitionTime&#34;:&#34;2019-11-19T09:15:12Z&#34;,&#34;reason&#34;:&#34;FailedGetResourceMetric&#34;,&#34;message&#34;:&#34;the
</span><span class="s1">      HPA was unable to compute the replica count: missing request for cpu&#34;}]&#39;</span><span class="w">
</span><span class="w">  </span><span class="nt">creationTimestamp</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;2019-11-19T09:14:56Z&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hpa-demo</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w">  </span><span class="nt">resourceVersion</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;3094084&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">selfLink</span><span class="p">:</span><span class="w"> </span><span class="l">/apis/autoscaling/v1/namespaces/default/horizontalpodautoscalers/hpa-demo</span><span class="w">
</span><span class="w">  </span><span class="nt">uid</span><span class="p">:</span><span class="w"> </span><span class="l">b84d79f1-75b0-46e0-95b5-4cbe3509233b</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">maxReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="w">  </span><span class="nt">minReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">  </span><span class="nt">scaleTargetRef</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hpa-demo</span><span class="w">
</span><span class="w">  </span><span class="nt">targetCPUUtilizationPercentage</span><span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="w"></span><span class="nt">status</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">currentReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">  </span><span class="nt">desiredReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">0</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>We can then create our own YAML-based HPA description file based on the YAML file above. However, we find some Fail messages in the above information, so let&rsquo;s take a look at the HPA object.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl describe hpa hpa-demo
Name:                                                  hpa-demo
Namespace:                                             default
Labels:                                                &lt;none&gt;
Annotations:                                           &lt;none&gt;
CreationTimestamp:                                     Tue, <span class="m">19</span> Nov <span class="m">2019</span> 17:14:56 +0800
Reference:                                             Deployment/hpa-demo
Metrics:                                               <span class="o">(</span> current / target <span class="o">)</span>
  resource cpu on pods  <span class="o">(</span>as a percentage of request<span class="o">)</span>:  &lt;unknown&gt; / 10%
Min replicas:                                          <span class="m">1</span>
Max replicas:                                          <span class="m">10</span>
Deployment pods:                                       <span class="m">1</span> current / <span class="m">0</span> desired
Conditions:
  Type           Status  Reason                   Message
  ----           ------  ------                   -------
  AbleToScale    True    SucceededGetScale        the HPA controller was able to get the target<span class="err">&#39;</span>s current scale
  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: missing request <span class="k">for</span> cpu
Events:
  Type     Reason                        Age                From                       Message
  ----     ------                        ----               ----                       -------
  Warning  FailedGetResourceMetric       14s <span class="o">(</span>x4 over 60s<span class="o">)</span>  horizontal-pod-autoscaler  missing request <span class="k">for</span> cpu
  Warning  FailedComputeMetricsReplicas  14s <span class="o">(</span>x4 over 60s<span class="o">)</span>  horizontal-pod-autoscaler  invalid metrics <span class="o">(</span><span class="m">1</span> invalid out of 1<span class="o">)</span>, first error is: failed to get cpu utilization: missing request <span class="k">for</span> cpu
</code></pre></td></tr></table>
</div>
</div><p>We can see that the event message above has a <code>failed to get cpu utilization: missing request for cpu</code> error message. This is because the Pod object we created above does not have a request resource declaration, which causes the HPA to not read the CPU metric information.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hpa-demo</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">50Mi</span><span class="w">
</span><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">50m</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Then re-update the Deployment and re-create the HPA object.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl apply -f hpa.yaml 
deployment.apps/hpa-demo configured
$ kubectl get pods -o wide -l <span class="nv">app</span><span class="o">=</span>nginx
NAME                        READY   STATUS    RESTARTS   AGE     IP            NODE         NOMINATED NODE   READINESS GATES
hpa-demo-69968bb59f-twtdp   1/1     Running   <span class="m">0</span>          4m11s   10.244.4.97   ydzs-node4   &lt;none&gt;           &lt;none&gt;
$ kubectl delete hpa hpa-demo
horizontalpodautoscaler.autoscaling <span class="s2">&#34;hpa-demo&#34;</span> deleted
$ kubectl autoscale deployment hpa-demo --cpu-percent<span class="o">=</span><span class="m">10</span> --min<span class="o">=</span><span class="m">1</span> --max<span class="o">=</span><span class="m">10</span>
horizontalpodautoscaler.autoscaling/hpa-demo autoscaled
$ kubectl describe hpa hpa-demo                                          
Name:                                                  hpa-demo
Namespace:                                             default
Labels:                                                &lt;none&gt;
Annotations:                                           &lt;none&gt;
CreationTimestamp:                                     Tue, <span class="m">19</span> Nov <span class="m">2019</span> 17:23:49 +0800
Reference:                                             Deployment/hpa-demo
Metrics:                                               <span class="o">(</span> current / target <span class="o">)</span>
  resource cpu on pods  <span class="o">(</span>as a percentage of request<span class="o">)</span>:  0% <span class="o">(</span>0<span class="o">)</span> / 10%
Min replicas:                                          <span class="m">1</span>
Max replicas:                                          <span class="m">10</span>
Deployment pods:                                       <span class="m">1</span> current / <span class="m">1</span> desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization <span class="o">(</span>percentage of request<span class="o">)</span>
  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range
Events:           &lt;none&gt;
$ kubectl get hpa                                                        
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   0%/10%    <span class="m">1</span>         <span class="m">10</span>        <span class="m">1</span>          52s
</code></pre></td></tr></table>
</div>
</div><p>Now that you can see that the HPA resource object is working, let&rsquo;s test it by increasing the load, creating a busybox Pod, and cycling through the Pod created above.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl run -it --image busybox test-hpa --restart<span class="o">=</span>Never --rm /bin/sh
If you don<span class="err">&#39;</span>t see a <span class="nb">command</span> prompt, try pressing enter.
/ <span class="c1"># while true; do wget -q -O- http://10.244.4.97; done</span>
</code></pre></td></tr></table>
</div>
</div><p>As you can see in the figure below, the HPA is already working.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-text" data-lang="text">$  kubectl get hpa
NAME       REFERENCE             TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   338%/10%   1         10        1          5m15s
$ kubectl get pods -l app=nginx --watch 
NAME                        READY   STATUS              RESTARTS   AGE
hpa-demo-69968bb59f-8hjnn   1/1     Running             0          22s
hpa-demo-69968bb59f-9ss9f   1/1     Running             0          22s
hpa-demo-69968bb59f-bllsd   1/1     Running             0          22s
hpa-demo-69968bb59f-lnh8k   1/1     Running             0          37s
hpa-demo-69968bb59f-r8zfh   1/1     Running             0          22s
hpa-demo-69968bb59f-twtdp   1/1     Running             0          6m43s
hpa-demo-69968bb59f-w792g   1/1     Running             0          37s
hpa-demo-69968bb59f-zlxkp   1/1     Running             0          37s
hpa-demo-69968bb59f-znp6q   0/1     ContainerCreating   0          6s
hpa-demo-69968bb59f-ztnvx   1/1     Running             0          6s
</code></pre></td></tr></table>
</div>
</div><p>We can see that a lot of new Pods have been automatically pulled up, finally settling on the 10 Pods we set above, while checking the number of copies of the resource hpa-demo, the number of copies has changed from the original 1 to 10</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl get deployment hpa-demo
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
hpa-demo   10/10   <span class="m">10</span>           <span class="m">10</span>          17m
</code></pre></td></tr></table>
</div>
</div><p>View the objects of the HPA resource to understand the working process.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl describe hpa hpa-demo
Name:                                                  hpa-demo
Namespace:                                             default
Labels:                                                &lt;none&gt;
Annotations:                                           &lt;none&gt;
CreationTimestamp:                                     Tue, <span class="m">19</span> Nov <span class="m">2019</span> 17:23:49 +0800
Reference:                                             Deployment/hpa-demo
Metrics:                                               <span class="o">(</span> current / target <span class="o">)</span>
  resource cpu on pods  <span class="o">(</span>as a percentage of request<span class="o">)</span>:  0% <span class="o">(</span>0<span class="o">)</span> / 10%
Min replicas:                                          <span class="m">1</span>
Max replicas:                                          <span class="m">10</span>
Deployment pods:                                       <span class="m">10</span> current / <span class="m">10</span> desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization <span class="o">(</span>percentage of request<span class="o">)</span>
  ScalingLimited  True    TooManyReplicas      the desired replica count is more than the maximum replica count
Events:
  Type    Reason             Age    From                       Message
  ----    ------             ----   ----                       -------
  Normal  SuccessfulRescale  5m45s  horizontal-pod-autoscaler  New size: 4<span class="p">;</span> reason: cpu resource utilization <span class="o">(</span>percentage of request<span class="o">)</span> above target
  Normal  SuccessfulRescale  5m30s  horizontal-pod-autoscaler  New size: 8<span class="p">;</span> reason: cpu resource utilization <span class="o">(</span>percentage of request<span class="o">)</span> above target
  Normal  SuccessfulRescale  5m14s  horizontal-pod-autoscaler  New size: 10<span class="p">;</span> reason: cpu resource utilization <span class="o">(</span>percentage of request<span class="o">)</span> above target
</code></pre></td></tr></table>
</div>
</div><p>Again, let&rsquo;s shut down busybox at this point to reduce the load and wait a while to observe the HPA and Deployment objects.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl get hpa
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   0%/10%    <span class="m">1</span>         <span class="m">10</span>        <span class="m">1</span>          14m
$ kubectl get deployment hpa-demo
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
hpa-demo   1/1     <span class="m">1</span>            <span class="m">1</span>           24m
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Starting with Kubernetes <code>v1.12</code> we can set a duration by setting the <code>-kube-controller-manager</code> component&rsquo;s <code>-horizontal-pod-autoscaler-downscale-stabilization</code> parameter to specify how long after the current After the current operation is completed, <code>HPA</code> must wait for how long before another scaling operation is performed. The default is 5 minutes, which means that by default you need to wait 5 minutes before auto-scaling starts.</p>
</blockquote>
<p>You can see that the number of replicas has changed from 10 to 1. Currently we are only demonstrating CPU usage as a metric, and we will learn to automatically scale up and down Pods based on custom monitoring metrics later in the course.</p>
<h2 id="memory-based">Memory-based</h2>
<p>The <code>HorizontalPodAutoscaler</code> is a resource of the Kubernetes autoscaling API group, which only supports scaling based on CPU metrics in the current stable version of <code>autoscaling/v1</code>. In the Beta version <code>autoscaling/v2beta2</code>, scaling based on memory and custom metrics was introduced. So we need to use the API of the Beta version here.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/11/21/421d054c123840fd822bec17015148e0.png" alt=""></p>
<p>Now we use Deployment to create an Nginx Pod, and then use HPA to automate the scaling. The list of resources is shown below: (hpa-mem-demo.yaml)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hpa-mem-demo</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">increase-mem-script</span><span class="w">
</span><span class="w">        </span><span class="nt">configMap</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">increase-mem-config</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">        </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">increase-mem-script</span><span class="w">
</span><span class="w">          </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/script</span><span class="w">
</span><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">50Mi</span><span class="w">
</span><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">50m</span><span class="w">
</span><span class="w">        </span><span class="nt">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">privileged</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Here there are some differences with the previous common application, we mount a ConfigMap resource object named <code>increase-mem-config</code> to the container, the configuration file is used to increase the memory footprint of the container later in the script, the configuration file is shown below: (increase-mem-cm.yaml)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">increase-mem-config</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">increase-mem.sh</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span><span class="sd">    #!/bin/bash  
</span><span class="sd">    mkdir /tmp/memory  
</span><span class="sd">    mount -t tmpfs -o size=40M tmpfs /tmp/memory  
</span><span class="sd">    dd if=/dev/zero of=/tmp/memory/block  
</span><span class="sd">    sleep 60 
</span><span class="sd">    rm /tmp/memory/block  
</span><span class="sd">    umount /tmp/memory  
</span><span class="sd">    rmdir /tmp/memory</span><span class="w">    
</span></code></pre></td></tr></table>
</div>
</div><p>Since the script to increase memory here uses the <code>mount</code> command, which needs to be declared in privileged mode, we added the <code>securityContext.privileged=true</code> configuration. Now we can just create the resource object above.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl apply -f increase-mem-cm.yaml
$ kubectl apply -f hpa-mem-demo.yaml 
$ kubectl get pods -l <span class="nv">app</span><span class="o">=</span>nginx
NAME                            READY   STATUS    RESTARTS   AGE
hpa-mem-demo-66944b79bf-tqrn9   1/1     Running   <span class="m">0</span>          35s
</code></pre></td></tr></table>
</div>
</div><p>Then a memory-based HPA resource object needs to be created: (hpa-mem.yaml)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">autoscaling/v2beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">HorizontalPodAutoscaler</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-hpa</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">scaleTargetRef</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hpa-mem-demo</span><span class="w">
</span><span class="w">  </span><span class="nt">minReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">  </span><span class="nt">maxReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Resource</span><span class="w">
</span><span class="w">    </span><span class="nt">resource</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">memory</span><span class="w">
</span><span class="w">      </span><span class="nt">targetAverageUtilization</span><span class="p">:</span><span class="w"> </span><span class="m">60</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Note that the <code>apiVersion</code> used here is <code>autoscaling/v2beta1</code>, and then the <code>metrics</code> attribute specifies the memory configuration, so you can create the above resource object directly.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl apply -f hpa-mem.yaml 
horizontalpodautoscaler.autoscaling/nginx-hpa created
$ kubectl get hpa
NAME        REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx-hpa   Deployment/hpa-mem-demo   2%/60%    <span class="m">1</span>         <span class="m">5</span>         <span class="m">1</span>          12s
</code></pre></td></tr></table>
</div>
</div><p>This proves that the HPA resource object has been successfully deployed. Next, we pressure test the application by pressing the memory up and directly executing the <code>increase-mem.sh</code> script that we mounted into the container above.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl <span class="nb">exec</span> -it hpa-mem-demo-66944b79bf-tqrn9 /bin/bash
root@hpa-mem-demo-66944b79bf-tqrn9:/# ls /etc/script/
increase-mem.sh
root@hpa-mem-demo-66944b79bf-tqrn9:/# <span class="nb">source</span> /etc/script/increase-mem.sh 
dd: writing to <span class="s1">&#39;/tmp/memory/block&#39;</span>: No space left on device
81921+0 records in
81920+0 records out
<span class="m">41943040</span> bytes <span class="o">(</span><span class="m">42</span> MB, <span class="m">40</span> MiB<span class="o">)</span> copied, 0.584029 s, 71.8 MB/s
</code></pre></td></tr></table>
</div>
</div><p>Then open another terminal to observe the changes in the HPA resource object.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl get hpa
NAME        REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx-hpa   Deployment/hpa-mem-demo   83%/60%   <span class="m">1</span>         <span class="m">5</span>         <span class="m">1</span>          5m3s
$ kubectl describe hpa nginx-hpa
Name:                                                     nginx-hpa
Namespace:                                                default
Labels:                                                   &lt;none&gt;
Annotations:                                              kubectl.kubernetes.io/last-applied-configuration:
                                                            <span class="o">{</span><span class="s2">&#34;apiVersion&#34;</span>:<span class="s2">&#34;autoscaling/v2beta1&#34;</span>,<span class="s2">&#34;kind&#34;</span>:<span class="s2">&#34;HorizontalPodAutoscaler&#34;</span>,<span class="s2">&#34;metadata&#34;</span>:<span class="o">{</span><span class="s2">&#34;annotations&#34;</span>:<span class="o">{}</span>,<span class="s2">&#34;name&#34;</span>:<span class="s2">&#34;nginx-hpa&#34;</span>,<span class="s2">&#34;namespace&#34;</span>:<span class="s2">&#34;default&#34;</span>...
CreationTimestamp:                                        Tue, <span class="m">07</span> Apr <span class="m">2020</span> 13:13:59 +0800
Reference:                                                Deployment/hpa-mem-demo
Metrics:                                                  <span class="o">(</span> current / target <span class="o">)</span>
  resource memory on pods  <span class="o">(</span>as a percentage of request<span class="o">)</span>:  3% <span class="o">(</span>1740800<span class="o">)</span> / 60%
Min replicas:                                             <span class="m">1</span>
Max replicas:                                             <span class="m">5</span>
Deployment pods:                                          <span class="m">2</span> current / <span class="m">2</span> desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from memory resource utilization <span class="o">(</span>percentage of request<span class="o">)</span>
  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range
Events:
  Type     Reason                        Age                    From                       Message
  ----     ------                        ----                   ----                       -------
  Warning  FailedGetResourceMetric       5m26s <span class="o">(</span>x3 over 5m58s<span class="o">)</span>  horizontal-pod-autoscaler  unable to get metrics <span class="k">for</span> resource memory: no metrics returned from resource metrics API
  Warning  FailedComputeMetricsReplicas  5m26s <span class="o">(</span>x3 over 5m58s<span class="o">)</span>  horizontal-pod-autoscaler  invalid metrics <span class="o">(</span><span class="m">1</span> invalid out of 1<span class="o">)</span>, first error is: failed to get memory utilization: unable to get metrics <span class="k">for</span> resource memory: no metrics returned from resource metrics API
  Normal   SuccessfulRescale             77s                    horizontal-pod-autoscaler  New size: 2<span class="p">;</span> reason: memory resource utilization <span class="o">(</span>percentage of request<span class="o">)</span> above target
$ kubectl top pod hpa-mem-demo-66944b79bf-tqrn9
NAME                            CPU<span class="o">(</span>cores<span class="o">)</span>   MEMORY<span class="o">(</span>bytes<span class="o">)</span>
hpa-mem-demo-66944b79bf-tqrn9   0m           41Mi
</code></pre></td></tr></table>
</div>
</div><p>You can see that the memory usage has exceeded the 60% threshold we set, and that the HPA resource object has triggered an automatic expansion to two copies.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl get pods -l <span class="nv">app</span><span class="o">=</span>nginx
NAME                            READY   STATUS    RESTARTS   AGE
hpa-mem-demo-66944b79bf-8m4d9   1/1     Running   <span class="m">0</span>          2m51s
hpa-mem-demo-66944b79bf-tqrn9   1/1     Running   <span class="m">0</span>          8m11s
</code></pre></td></tr></table>
</div>
</div><p>When the memory is freed, the controller-manager scales after 5 minutes by default, and the memory-based HPA operation is completed here.</p>
<h2 id="based-on-custom-metrics">Based on custom metrics</h2>
<p>In addition to automatic scaling based on CPU and memory, we can also do it based on custom monitoring metrics. This is done using the <code>Prometheus Adapter</code>, which is used to monitor the application load and various metrics of the cluster itself, and the <code>Prometheus Adapter</code> can help us use the metrics collected by Prometheus and use them to develop a scaling strategy, which are exposed through the APIServer These metrics are exposed through the APIServer, and the HPA resource object can be easily used directly.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/11/21/f3d7aec4a2ef475ba536873715fddc1c.png" alt=""></p>
<p>First, we deploy a sample application on which to test the Prometheus metrics autoscaling, with the following resource manifest file: (hpa-prome-demo.yaml)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hpa-prom-demo</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-server</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-server</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-demo</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">cnych/nginx-vts:v1.0</span><span class="w">
</span><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">50m</span><span class="w">
</span><span class="w">          </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">50m</span><span class="w">
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hpa-prom-demo</span><span class="w">
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">prometheus.io/scrape</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;true&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">prometheus.io/port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;80&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">prometheus.io/path</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;/status/format/prometheus&#34;</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-server</span><span class="w">
</span><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">NodePort</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Here we are deploying an application that exposes the nginx-vts metric on the <code>/status/format/prometheus</code> endpoint on port 80. We have already configured the auto-discovery of Endpoints in Prometheus, so we directly configure it in the <code>annotations</code> of the Service object. so that we can collect the metric data in Prometheus. For testing purposes, we will use a Service of type NodePort here, and now create the above resource object directly as follows.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl apply -f hpa-prome-demo.yaml
deployment.apps/hpa-prom-demo created
service/hpa-prom-demo created
$ kubectl get pods -l <span class="nv">app</span><span class="o">=</span>nginx-server 
NAME                             READY   STATUS    RESTARTS   AGE
hpa-prom-demo-755bb56f85-lvksr   1/1     Running   <span class="m">0</span>          4m52s
$ kubectl get svc 
NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>        AGE
hpa-prom-demo   NodePort    10.101.210.158   &lt;none&gt;        80:32408/TCP   5m44s
......
</code></pre></td></tr></table>
</div>
</div><p>After deployment we can test if the application works and if the indicator data interface can be obtained properly using the following commands.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ curl http://k8s.qikqiak.com:32408
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body <span class="o">{</span>
        width: 35em<span class="p">;</span>
        margin: <span class="m">0</span> auto<span class="p">;</span>
        font-family: Tahoma, Verdana, Arial, sans-serif<span class="p">;</span>
    <span class="o">}</span>
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&#34;http://nginx.org/&#34;</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&#34;http://nginx.com/&#34;</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you <span class="k">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
$ curl http://k8s.qikqiak.com:32408/status/format/prometheus 
<span class="c1"># HELP nginx_vts_info Nginx info</span>
<span class="c1"># TYPE nginx_vts_info gauge</span>
nginx_vts_info<span class="o">{</span><span class="nv">hostname</span><span class="o">=</span><span class="s2">&#34;hpa-prom-demo-755bb56f85-lvksr&#34;</span>,version<span class="o">=</span><span class="s2">&#34;1.13.12&#34;</span><span class="o">}</span> <span class="m">1</span>
<span class="c1"># HELP nginx_vts_start_time_seconds Nginx start time</span>
<span class="c1"># TYPE nginx_vts_start_time_seconds gauge</span>
nginx_vts_start_time_seconds 1586240091.623
<span class="c1"># HELP nginx_vts_main_connections Nginx connections</span>
<span class="c1"># TYPE nginx_vts_main_connections gauge</span>
......
</code></pre></td></tr></table>
</div>
</div><p>Of the above metrics, we are more interested in the <code>nginx_vts_server_requests_total</code> metric, which represents the total number of requests and is a <code>Counter</code> type metric that we will use to determine whether our application needs to be automatically scaled up or down.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/11/21/223fe61a22a74eb99186523bf51d3830.png" alt=""></p>
<p>Next we install Prometheus-Adapter into the cluster and add a rule to track requests from Pods. We can use any metric from Prometheus for HPA, but only if you get it (including the metric name and its corresponding value) via a query statement.</p>
<p>Here we define a rule as shown below.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="nt">seriesQuery</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;nginx_vts_server_requests_total&#39;</span><span class="w">
</span><span class="w">  </span><span class="nt">seriesFilters</span><span class="p">:</span><span class="w"> </span><span class="p">[]</span><span class="w">
</span><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">overrides</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">kubernetes_namespace</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">resource</span><span class="p">:</span><span class="w"> </span><span class="l">namespace</span><span class="w">
</span><span class="w">      </span><span class="nt">kubernetes_pod_name</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">resource</span><span class="p">:</span><span class="w"> </span><span class="l">pod</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matches</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;^(.*)_total&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">as</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;${1}_per_second&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">metricsQuery</span><span class="p">:</span><span class="w"> </span><span class="l">(sum(rate(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}[1m])) by (&lt;&lt;.GroupBy&gt;&gt;))</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>This is a Prometheus query with parameters, where.</p>
<ul>
<li><code>seriesQuery</code> : a query statement for Prometheus, all metrics queried by this query statement can be used in HPA</li>
<li><code>seriesFilters</code> : The query may contain indicators that are not needed and can be filtered out by it.</li>
<li><code>resources</code>: The metrics queried by <code>seriesQuery</code> are only metrics. If you need to query the metrics of a pod, you must use its name and namespace as the label of the metrics to query, <code>resources</code> is to associate the label of the metrics with the resource type of k8s, the most common ones are pod and namespace. There are two ways to add tags, one is <code>overrides</code> and the other is <code>template</code>.
<ul>
<li><code>overrides</code>: It associates the tags in the metrics with k8s resources. The above example associates the pod and namespace tags in the metrics with the pods and namespaces in k8s, because both pods and namespaces are part of the core api group, so there is no need to specify an api group. When we query the metrics for a pod, it automatically adds the pod name and namespace as tags to the query criteria. For example, <code>nginx: {group: &quot;apps&quot;, resource: &quot;deployment&quot;}</code> is written to associate the nginx tag in the metric with the <code>deployment</code> resource in the apps api group.</li>
<li>template: via the go template form. For example, <code>template: &quot;kube_&lt;&lt;.Group&gt;&gt;_&lt;&lt;.Resource&gt;&gt;&quot;</code> means that if <code>&lt;&lt;.Group&gt;&gt;</code> is apps and <code>&lt;&lt;.Resource&gt;&gt;</code> is deployment, then it is associating the <code>kube_apps_deployment</code> tag in the metric with the deployment resource. and the deployment resource.</li>
</ul>
</li>
<li><code>name</code>: This is used to rename the metric. The reason for renaming the metric is that some metrics are incremental, such as those ending in total. These metrics are meaningless for HPA, we usually calculate its rate and use the rate as the value, then the name can not end with total, so we need to rename it.
<ul>
<li><code>matches</code> : match metric names by regular expressions, can be grouped</li>
<li><code>as</code> : The default value is <code>$1</code>, which is the first grouping. An empty <code>as</code> means use the default value.</li>
</ul>
</li>
<li><code>metricsQuery</code> : This is the query statement of Prometheus, the previous <code>seriesQuery</code> query is to get the HPA metrics. When we want to check the value of a metric, we have to do it with the query statement it specifies. You can see that the query statement uses rates and grouping, which is the solution to the problem of only increasing the metric mentioned above.
<ul>
<li><code>Series</code>: indicates the name of the indicator</li>
<li><code>LabelMatchers</code> : additional labels, currently only <code>pod</code> and <code>namespace</code> are available, so we have to use <code>resources</code> to associate them before</li>
<li><code>GroupBy</code>: This is the pod name, which also needs to be associated with <code>resources</code>.</li>
</ul>
</li>
</ul>
<p>Next, we deploy the Prometheus Adapter via Helm Chart by creating a new <code>hpa-prome-adapter-values.yaml</code> file to override the default Values values, as follows.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">default</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">  </span><span class="nt">custom</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">seriesQuery</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;nginx_vts_server_requests_total&#39;</span><span class="w">
</span><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"> 
</span><span class="w">      </span><span class="nt">overrides</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">kubernetes_namespace</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">resource</span><span class="p">:</span><span class="w"> </span><span class="l">namespace</span><span class="w">
</span><span class="w">        </span><span class="nt">kubernetes_pod_name</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">resource</span><span class="p">:</span><span class="w"> </span><span class="l">pod</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">matches</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;^(.*)_total&#34;</span><span class="w">
</span><span class="w">      </span><span class="nt">as</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;${1}_per_second&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">metricsQuery</span><span class="p">:</span><span class="w"> </span><span class="l">(sum(rate(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}[1m])) by (&lt;&lt;.GroupBy&gt;&gt;))</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">prometheus</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="l">http://thanos-querier.kube-mon.svc.cluster.local</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Here we add a rules rule and specify the address of Prometheus, we are using the Thanos deployed Promethues cluster here, so we use the address of Querier. Install it with one click using the following command.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ helm install prometheus-adapter stable/prometheus-adapter -n kube-mon -f hpa-prome-adapter-values.yaml
NAME: prometheus-adapter
LAST DEPLOYED: Tue Apr  <span class="m">7</span> 15:26:36 <span class="m">2020</span>
NAMESPACE: kube-mon
STATUS: deployed
REVISION: <span class="m">1</span>
TEST SUITE: None
NOTES:
prometheus-adapter has been deployed.
In a few minutes you should be able to list metrics using the following command<span class="o">(</span>s<span class="o">)</span>:

  kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1
</code></pre></td></tr></table>
</div>
</div><p>After waiting a short while for the installation to complete, you can use the following command to check if it has taken effect.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl get pods -n kube-mon -l <span class="nv">app</span><span class="o">=</span>prometheus-adapter
NAME                                  READY   STATUS    RESTARTS   AGE
prometheus-adapter-58b559fc7d-l2j6t   1/1     Running   <span class="m">0</span>          3m21s
$  kubectl get --raw<span class="o">=</span><span class="s2">&#34;/apis/custom.metrics.k8s.io/v1beta1&#34;</span> <span class="p">|</span> jq
<span class="o">{</span>
  <span class="s2">&#34;kind&#34;</span>: <span class="s2">&#34;APIResourceList&#34;</span>,
  <span class="s2">&#34;apiVersion&#34;</span>: <span class="s2">&#34;v1&#34;</span>,
  <span class="s2">&#34;groupVersion&#34;</span>: <span class="s2">&#34;custom.metrics.k8s.io/v1beta1&#34;</span>,
  <span class="s2">&#34;resources&#34;</span>: <span class="o">[</span>
    <span class="o">{</span>
      <span class="s2">&#34;name&#34;</span>: <span class="s2">&#34;namespaces/nginx_vts_server_requests_per_second&#34;</span>,
      <span class="s2">&#34;singularName&#34;</span>: <span class="s2">&#34;&#34;</span>,
      <span class="s2">&#34;namespaced&#34;</span>: false,
      <span class="s2">&#34;kind&#34;</span>: <span class="s2">&#34;MetricValueList&#34;</span>,
      <span class="s2">&#34;verbs&#34;</span>: <span class="o">[</span>
        <span class="s2">&#34;get&#34;</span>
      <span class="o">]</span>
    <span class="o">}</span>,
    <span class="o">{</span>
      <span class="s2">&#34;name&#34;</span>: <span class="s2">&#34;pods/nginx_vts_server_requests_per_second&#34;</span>,
      <span class="s2">&#34;singularName&#34;</span>: <span class="s2">&#34;&#34;</span>,
      <span class="s2">&#34;namespaced&#34;</span>: true,
      <span class="s2">&#34;kind&#34;</span>: <span class="s2">&#34;MetricValueList&#34;</span>,
      <span class="s2">&#34;verbs&#34;</span>: <span class="o">[</span>
        <span class="s2">&#34;get&#34;</span>
      <span class="o">]</span>
    <span class="o">}</span>
  <span class="o">]</span>
<span class="o">}</span>
</code></pre></td></tr></table>
</div>
</div><p>We can see that the <code>nginx_vts_server_requests_per_second</code> metric is available. Now, let&rsquo;s check the current value of the metric.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl get --raw <span class="s2">&#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/nginx_vts_server_requests_per_second&#34;</span> <span class="p">|</span> jq .
<span class="o">{</span>
  <span class="s2">&#34;kind&#34;</span>: <span class="s2">&#34;MetricValueList&#34;</span>,
  <span class="s2">&#34;apiVersion&#34;</span>: <span class="s2">&#34;custom.metrics.k8s.io/v1beta1&#34;</span>,
  <span class="s2">&#34;metadata&#34;</span>: <span class="o">{</span>
    <span class="s2">&#34;selfLink&#34;</span>: <span class="s2">&#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/nginx_vts_server_requests_per_second&#34;</span>
  <span class="o">}</span>,
  <span class="s2">&#34;items&#34;</span>: <span class="o">[</span>
    <span class="o">{</span>
      <span class="s2">&#34;describedObject&#34;</span>: <span class="o">{</span>
        <span class="s2">&#34;kind&#34;</span>: <span class="s2">&#34;Pod&#34;</span>,
        <span class="s2">&#34;namespace&#34;</span>: <span class="s2">&#34;default&#34;</span>,
        <span class="s2">&#34;name&#34;</span>: <span class="s2">&#34;hpa-prom-demo-755bb56f85-lvksr&#34;</span>,
        <span class="s2">&#34;apiVersion&#34;</span>: <span class="s2">&#34;/v1&#34;</span>
      <span class="o">}</span>,
      <span class="s2">&#34;metricName&#34;</span>: <span class="s2">&#34;nginx_vts_server_requests_per_second&#34;</span>,
      <span class="s2">&#34;timestamp&#34;</span>: <span class="s2">&#34;2020-04-07T09:45:45Z&#34;</span>,
      <span class="s2">&#34;value&#34;</span>: <span class="s2">&#34;527m&#34;</span>,
      <span class="s2">&#34;selector&#34;</span>: null
    <span class="o">}</span>
  <span class="o">]</span>
<span class="o">}</span>
</code></pre></td></tr></table>
</div>
</div><p>A message like the one above indicates that the configuration has been successful. Next, we deploy a HAP resource object for the above custom metrics as follows: (hpa-prome.yaml)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">autoscaling/v2beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">HorizontalPodAutoscaler</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-custom-hpa</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">scaleTargetRef</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">hpa-prom-demo</span><span class="w">
</span><span class="w">  </span><span class="nt">minReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">2</span><span class="w">
</span><span class="w">  </span><span class="nt">maxReplicas</span><span class="p">:</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Pods</span><span class="w">
</span><span class="w">    </span><span class="nt">pods</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">metricName</span><span class="p">:</span><span class="w"> </span><span class="l">nginx_vts_server_requests_per_second</span><span class="w">
</span><span class="w">      </span><span class="nt">targetAverageValue</span><span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>If the number of requests exceeds 10 per second, the application will be scaled up. Create the resource object above directly.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl apply -f hpa-prome.yaml
horizontalpodautoscaler.autoscaling/nginx-custom-hpa created
$ kubectl describe hpa nginx-custom-hpa
Name:                                              nginx-custom-hpa
Namespace:                                         default
Labels:                                            &lt;none&gt;
Annotations:                                       kubectl.kubernetes.io/last-applied-configuration:
                                                     <span class="o">{</span><span class="s2">&#34;apiVersion&#34;</span>:<span class="s2">&#34;autoscaling/v2beta1&#34;</span>,<span class="s2">&#34;kind&#34;</span>:<span class="s2">&#34;HorizontalPodAutoscaler&#34;</span>,<span class="s2">&#34;metadata&#34;</span>:<span class="o">{</span><span class="s2">&#34;annotations&#34;</span>:<span class="o">{}</span>,<span class="s2">&#34;name&#34;</span>:<span class="s2">&#34;nginx-custom-hpa&#34;</span>,<span class="s2">&#34;namespace&#34;</span>:<span class="s2">&#34;d...
</span><span class="s2">CreationTimestamp:                                 Tue, 07 Apr 2020 17:54:55 +0800
</span><span class="s2">Reference:                                         Deployment/hpa-prom-demo
</span><span class="s2">Metrics:                                           ( current / target )
</span><span class="s2">  &#34;</span>nginx_vts_server_requests_per_second<span class="s2">&#34; on pods:  &lt;unknown&gt; / 10
</span><span class="s2">Min replicas:                                      2
</span><span class="s2">Max replicas:                                      5
</span><span class="s2">Deployment pods:                                   1 current / 2 desired
</span><span class="s2">Conditions:
</span><span class="s2">  Type         Status  Reason            Message
</span><span class="s2">  ----         ------  ------            -------
</span><span class="s2">  AbleToScale  True    SucceededRescale  the HPA controller was able to update the target scale to 2
</span><span class="s2">Events:
</span><span class="s2">  Type    Reason             Age   From                       Message
</span><span class="s2">  ----    ------             ----  ----                       -------
</span><span class="s2">  Normal  SuccessfulRescale  7s    horizontal-pod-autoscaler  New size: 2; reason: Current number of replicas below Spec.MinReplicas
</span></code></pre></td></tr></table>
</div>
</div><p>You can see that the HPA object is already in effect and will apply the minimum number of copies 2, so a new Pod copy will be added:.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl get pods -l <span class="nv">app</span><span class="o">=</span>nginx-server
NAME                             READY   STATUS    RESTARTS   AGE
hpa-prom-demo-755bb56f85-s5dzf   1/1     Running   <span class="m">0</span>          67s
hpa-prom-demo-755bb56f85-wbpfr   1/1     Running   <span class="m">0</span>          3m30s
</code></pre></td></tr></table>
</div>
</div><p>Next, we also pressure test the application.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ <span class="k">while</span> true<span class="p">;</span> <span class="k">do</span> wget -q -O- http://k8s.qikqiak.com:32408<span class="p">;</span> <span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><p>Open another terminal to observe the changes in the HPA object.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl get hpa
NAME               REFERENCE                  TARGETS     MINPODS   MAXPODS   REPLICAS   AGE
nginx-custom-hpa   Deployment/hpa-prom-demo   14239m/10   <span class="m">2</span>         <span class="m">5</span>         <span class="m">2</span>          4m27s
$ kubectl describe hpa nginx-custom-hpa
Name:                                              nginx-custom-hpa
Namespace:                                         default
Labels:                                            &lt;none&gt;
Annotations:                                       kubectl.kubernetes.io/last-applied-configuration:
                                                     <span class="o">{</span><span class="s2">&#34;apiVersion&#34;</span>:<span class="s2">&#34;autoscaling/v2beta1&#34;</span>,<span class="s2">&#34;kind&#34;</span>:<span class="s2">&#34;HorizontalPodAutoscaler&#34;</span>,<span class="s2">&#34;metadata&#34;</span>:<span class="o">{</span><span class="s2">&#34;annotations&#34;</span>:<span class="o">{}</span>,<span class="s2">&#34;name&#34;</span>:<span class="s2">&#34;nginx-custom-hpa&#34;</span>,<span class="s2">&#34;namespace&#34;</span>:<span class="s2">&#34;d...
</span><span class="s2">CreationTimestamp:                                 Tue, 07 Apr 2020 17:54:55 +0800
</span><span class="s2">Reference:                                         Deployment/hpa-prom-demo
</span><span class="s2">Metrics:                                           ( current / target )
</span><span class="s2">  &#34;</span>nginx_vts_server_requests_per_second<span class="s2">&#34; on pods:  14308m / 10
</span><span class="s2">Min replicas:                                      2
</span><span class="s2">Max replicas:                                      5
</span><span class="s2">Deployment pods:                                   3 current / 3 desired
</span><span class="s2">Conditions:
</span><span class="s2">  Type            Status  Reason              Message
</span><span class="s2">  ----            ------  ------              -------
</span><span class="s2">  AbleToScale     True    ReadyForNewScale    recommended size matches current size
</span><span class="s2">  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from pods metric nginx_vts_server_requests_per_second
</span><span class="s2">  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
</span><span class="s2">Events:
</span><span class="s2">  Type    Reason             Age   From                       Message
</span><span class="s2">  ----    ------             ----  ----                       -------
</span><span class="s2">  Normal  SuccessfulRescale  5m2s  horizontal-pod-autoscaler  New size: 2; reason: Current number of replicas below Spec.MinReplicas
</span><span class="s2">  Normal  SuccessfulRescale  61s   horizontal-pod-autoscaler  New size: 3; reason: pods metric nginx_vts_server_requests_per_second above target
</span></code></pre></td></tr></table>
</div>
</div><p>You can see that the data of indicator <code>nginx_vts_server_requests_per_second</code> has exceeded the threshold and triggered the expansion action, the number of replicas became 3, but it is difficult to continue the expansion, this is because our <code>while</code> command above is not fast enough, 3 replicas can fully meet the threshold of no more than 10 requests per second.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2021/11/21/136bc17c104d41fd8f7b89f07297a0ee.png" alt=""></p>
<p>If we need to test better, we can use some compression testing tools, such as ab, fortio, etc. When we interrupt the test, the default is to automatically shrink the capacity after 5 minutes.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">$ kubectl describe hpa nginx-custom-hpa
Name:                                              nginx-custom-hpa
Namespace:                                         default
Labels:                                            &lt;none&gt;
Annotations:                                       kubectl.kubernetes.io/last-applied-configuration:
                                                     <span class="o">{</span><span class="s2">&#34;apiVersion&#34;</span>:<span class="s2">&#34;autoscaling/v2beta1&#34;</span>,<span class="s2">&#34;kind&#34;</span>:<span class="s2">&#34;HorizontalPodAutoscaler&#34;</span>,<span class="s2">&#34;metadata&#34;</span>:<span class="o">{</span><span class="s2">&#34;annotations&#34;</span>:<span class="o">{}</span>,<span class="s2">&#34;name&#34;</span>:<span class="s2">&#34;nginx-custom-hpa&#34;</span>,<span class="s2">&#34;namespace&#34;</span>:<span class="s2">&#34;d...
</span><span class="s2">CreationTimestamp:                                 Tue, 07 Apr 2020 17:54:55 +0800
</span><span class="s2">Reference:                                         Deployment/hpa-prom-demo
</span><span class="s2">Metrics:                                           ( current / target )
</span><span class="s2">  &#34;</span>nginx_vts_server_requests_per_second<span class="s2">&#34; on pods:  533m / 10
</span><span class="s2">Min replicas:                                      2
</span><span class="s2">Max replicas:                                      5
</span><span class="s2">Deployment pods:                                   2 current / 2 desired
</span><span class="s2">Conditions:
</span><span class="s2">  Type            Status  Reason            Message
</span><span class="s2">  ----            ------  ------            -------
</span><span class="s2">  AbleToScale     True    ReadyForNewScale  recommended size matches current size
</span><span class="s2">  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count from pods metric nginx_vts_server_requests_per_second
</span><span class="s2">  ScalingLimited  True    TooFewReplicas    the desired replica count is less than the minimum replica count
</span><span class="s2">Events:
</span><span class="s2">  Type    Reason             Age   From                       Message
</span><span class="s2">  ----    ------             ----  ----                       -------
</span><span class="s2">  Normal  SuccessfulRescale  23m   horizontal-pod-autoscaler  New size: 2; reason: Current number of replicas below Spec.MinReplicas
</span><span class="s2">  Normal  SuccessfulRescale  19m   horizontal-pod-autoscaler  New size: 3; reason: pods metric nginx_vts_server_requests_per_second above target
</span><span class="s2">  Normal  SuccessfulRescale  4m2s  horizontal-pod-autoscaler  New size: 2; reason: All metrics below target
</span></code></pre></td></tr></table>
</div>
</div><p>At this point we are done with the automatic scaling of the application using custom metrics. If Prometheus is installed outside of our Kubernetes cluster, simply ensure that the queried endpoint is accessible from the cluster and update it in the adapter&rsquo;s deployment manifest. In more complex scenarios, you can get multiple metrics to use in combination to develop a scaling strategy.</p>

    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/k8s/">k8s</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2021-11/envoy-usage-demo/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Envoy simple example</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/2021-11/use-nodelocal-dns-cache/">
            <span class="next-text nav-default">Using NodeLocal DNSCache in a Kubernetes Cluster</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="https://www.sobyte.net/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2021 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
