<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>M1 mac system deep learning framework Pytorch secondary anime animation style migration filter AnimeGANv2 &#43; Ffmpeg (image &#43; video) quick practice - SoByte</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6356451834813761" crossorigin="anonymous"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E8GRRGBTEZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E8GRRGBTEZ');
</script>


<meta name="author" content="" /><meta name="description" content="Some time ago, the industry&amp;rsquo;s most famous anime style transformation filter library AnimeGAN released its latest v2 version, and it has been the talk of the town for a while. When it comes to secondary yuan, the largest domestic user base is undoubtedly the Jitterbug client, which has a built-in animation conversion filter &amp;ldquo;Transformation Comic&amp;rdquo; that allows users to convert their actual appearance to secondary yuan &amp;ldquo;style&amp;rdquo; during live broadcasts." /><meta name="keywords" content="ffmpeg, pytorch" />






<meta name="generator" content="Hugo 0.92.2 with theme even" />


<link rel="canonical" href="https://www.sobyte.net/post/2022-01/ffmpeg-pytorch/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="M1 mac system deep learning framework Pytorch secondary anime animation style migration filter AnimeGANv2 &#43; Ffmpeg (image &#43; video) quick practice" />
<meta property="og:description" content="Some time ago, the industry&rsquo;s most famous anime style transformation filter library AnimeGAN released its latest v2 version, and it has been the talk of the town for a while. When it comes to secondary yuan, the largest domestic user base is undoubtedly the Jitterbug client, which has a built-in animation conversion filter &ldquo;Transformation Comic&rdquo; that allows users to convert their actual appearance to secondary yuan &ldquo;style&rdquo; during live broadcasts." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.sobyte.net/post/2022-01/ffmpeg-pytorch/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-01-01T14:27:42+08:00" />
<meta property="article:modified_time" content="2022-01-01T14:27:42+08:00" />

<meta itemprop="name" content="M1 mac system deep learning framework Pytorch secondary anime animation style migration filter AnimeGANv2 &#43; Ffmpeg (image &#43; video) quick practice">
<meta itemprop="description" content="Some time ago, the industry&rsquo;s most famous anime style transformation filter library AnimeGAN released its latest v2 version, and it has been the talk of the town for a while. When it comes to secondary yuan, the largest domestic user base is undoubtedly the Jitterbug client, which has a built-in animation conversion filter &ldquo;Transformation Comic&rdquo; that allows users to convert their actual appearance to secondary yuan &ldquo;style&rdquo; during live broadcasts."><meta itemprop="datePublished" content="2022-01-01T14:27:42+08:00" />
<meta itemprop="dateModified" content="2022-01-01T14:27:42+08:00" />
<meta itemprop="wordCount" content="1770">
<meta itemprop="keywords" content="ffmpeg,pytorch," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="M1 mac system deep learning framework Pytorch secondary anime animation style migration filter AnimeGANv2 &#43; Ffmpeg (image &#43; video) quick practice"/>
<meta name="twitter:description" content="Some time ago, the industry&rsquo;s most famous anime style transformation filter library AnimeGAN released its latest v2 version, and it has been the talk of the town for a while. When it comes to secondary yuan, the largest domestic user base is undoubtedly the Jitterbug client, which has a built-in animation conversion filter &ldquo;Transformation Comic&rdquo; that allows users to convert their actual appearance to secondary yuan &ldquo;style&rdquo; during live broadcasts."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">SoByte</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="/ukraine/">
        <li class="mobile-menu-item">UKRAINE</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">SoByte</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/ukraine/">UKRAINE</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">M1 mac system deep learning framework Pytorch secondary anime animation style migration filter AnimeGANv2 &#43; Ffmpeg (image &#43; video) quick practice</h1>

      <div class="post-meta">
        <span class="post-time"> 2022-01-01 14:27:42 </span>
        <div class="post-category">
            <a href="/categories/skills/"> skills </a>
            </div>
          <span class="more-meta"> 1770 words </span>
          <span class="more-meta"> 4 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents"></nav>
  </div>
</div>
    <div class="post-content">
      <p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/01/01/93eb262df3b546bbaf9b04a84578f000.png" alt="image"></p>
<p>Some time ago, the industry&rsquo;s most famous anime style transformation filter library AnimeGAN released its latest v2 version, and it has been the talk of the town for a while. When it comes to secondary yuan, the largest domestic user base is undoubtedly the Jitterbug client, which has a built-in animation conversion filter &ldquo;Transformation Comic&rdquo; that allows users to convert their actual appearance to secondary yuan &ldquo;style&rdquo; during live broadcasts. For secondary fans, the self-indulgent way of &ldquo;breaking the next-dimensional wall and transforming into a paper man&rdquo; is a tried and tested way:</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/01/01/bafbac7151db48afa3353ad96c4f5821.png" alt="image"></p>
<p>But it is inevitable to see more aesthetic fatigue, a thousand people a &ldquo;cone face&rdquo;, the same &ldquo;Kazran&rdquo; type big eyes, so people are more or less the same feeling, not too much, lost reality.</p>
<p>The CartoonGan-based AnimeGAN anime style filter is able to retain the characteristics of the original image while retaining both the coolness of the secondary yuan and the realism of the tertiary yuan, which is quite a bit of a combination of soft and rigid, light feeling.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/01/01/47496472aeac490397d31cdc07635df8.png" alt="image"></p>
<p>And the AnimeGAN project team has released the demo interface online, so you can run the model directly: <a href="https://huggingface.co/spaces/akhaliq/AnimeGANv2">https://huggingface.co/spaces/akhaliq/AnimeGANv2</a> However, due to bandwidth and online resource bottlenecks, the online migration queue is often in a queue state, and the upload of some original images may also The uploading of some original images may also cause leakage of personal privacy.</p>
<p>So this time we build AnimeGANV2 based on Pytorch deep learning framework in Mac os Monterey with M1 chip to convert still pictures and dynamic videos.</p>
<p>As we know, the current cpu version of Pytorch on the M1 chip mac is Python 3.8. This time we use the native installation package to install it, first go to the Python website and download Python 3.8.10 universal2 stable version: <a href="https://www.python.org/downloads/release/python-3810/">https://www.python.org/downloads/release/python-3810/</a></p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/01/01/ca86a62204c94f34bc523cab54bbae4e.png" alt="image"></p>
<p>Just double-click to install, then go to the terminal and type the command to install Pytorch:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">pip3.8 install torch torchvision torchaudio
</code></pre></td></tr></table>
</div>
</div><p>Here we install the latest stable version 1.10 by default, then go to the Python 3.8 command line and import the torch library at</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="o">(</span>base<span class="o">)</span> ➜  video git:<span class="o">(</span>main<span class="o">)</span> ✗ python3.8
Python 3.8.10 <span class="o">(</span>v3.8.10:3d8993a744, May  <span class="m">3</span> 2021, 09:09:08<span class="o">)</span> 
<span class="o">[</span>Clang 12.0.5 <span class="o">(</span>clang-1205.0.22.9<span class="o">)]</span> on darwin
Type <span class="s2">&#34;help&#34;</span>, <span class="s2">&#34;copyright&#34;</span>, <span class="s2">&#34;credits&#34;</span> or <span class="s2">&#34;license&#34;</span> <span class="k">for</span> more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt;
</code></pre></td></tr></table>
</div>
</div><p>After making sure that Pytorch is available, clone the official project</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">git clone https://github.com/bryandlee/animegan2-pytorch.git
</code></pre></td></tr></table>
</div>
</div><p>AnimeGAN is also based on Generative adversarial network, the principle is that we have a certain amount of original pictures on hand, we can call them cubic pictures, the real picture features will exist a distribution, such as: normal distribution, uniform distribution, or more complex forms of distribution, then the purpose of GAN is to generate a batch of data close to the real distribution by generator to generate a batch of data close to the real distribution. These data can be understood as quadratic optimization, but will retain some features of the third dimension, such as larger eyes, face shape closer to the drawing style of the filter model, etc. In our processing, this generator tends to use a neural network, because it can represent more complex data distribution situations.</p>
<p>After successful download, you can see four different weighting models in the weights folder, where celeba_distill.pt and paprika.pt are used to transform landscape images, while face_paint_512_v1.pt and face_paint_512_v2.pt are more focused on portrait transformation.</p>
<p>First install the image processing library Pillow:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">pip3.8 install Pillow
</code></pre></td></tr></table>
</div>
</div><p>This is followed by a new test_img.py file.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">ssl</span>
<span class="n">ssl</span><span class="o">.</span><span class="n">_create_default_https_context</span> <span class="o">=</span> <span class="n">ssl</span><span class="o">.</span><span class="n">_create_unverified_context</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;bryandlee/animegan2-pytorch:main&#34;</span><span class="p">,</span> <span class="s2">&#34;generator&#34;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="s2">&#34;celeba_distill&#34;</span><span class="p">)</span>
<span class="c1">#model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;face_paint_512_v1&#34;)</span>
<span class="c1">#model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;face_paint_512_v2&#34;)</span>
<span class="c1">#model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;paprika&#34;)</span>


<span class="n">face2paint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;bryandlee/animegan2-pytorch:main&#34;</span><span class="p">,</span> <span class="s2">&#34;face2paint&#34;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&#34;Arc.jpg&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&#34;RGB&#34;</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">face2paint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span>

<span class="n">out</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p>Here the Arc de Triomphe photo as an example, respectively use celeba_distill and paprika filter to view the effect, note that local requests need to turn off ssl certificate detection, while the first run need to download the online model parameters</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/01/01/5af487ff4a2e4f4b85799da614c087d6.png" alt="image"></p>
<p>Here the image size parameter refers to the total number of width and height channels, the next is the character portrait animation style conversion, adjust the imported model generator type, the input image to change into a character portrait:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">ssl</span>
<span class="n">ssl</span><span class="o">.</span><span class="n">_create_default_https_context</span> <span class="o">=</span> <span class="n">ssl</span><span class="o">.</span><span class="n">_create_unverified_context</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1">#model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;celeba_distill&#34;)</span>
<span class="c1">#model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;face_paint_512_v1&#34;)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;bryandlee/animegan2-pytorch:main&#34;</span><span class="p">,</span> <span class="s2">&#34;generator&#34;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="s2">&#34;face_paint_512_v2&#34;</span><span class="p">)</span>
<span class="c1">#model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;paprika&#34;)</span>


<span class="n">face2paint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;bryandlee/animegan2-pytorch:main&#34;</span><span class="p">,</span> <span class="s2">&#34;face2paint&#34;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&#34;11.png&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&#34;RGB&#34;</span><span class="p">)</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">face2paint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span>


<span class="n">out</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/01/01/4fb8679803dc4a48837469779ab35eef.png" alt="image"></p>
<p>As you can see, the v1 filter is relatively more strongly stylized, while v2 relatively retains the characteristics of the original picture on the basis of stylization, which originates from the third dimension without being confined to the experience, overhead but not flowing in vain, and is higher than Jitterbug&rsquo;s cartoon filter.</p>
<p>Let&rsquo;s take a look at the anime filter conversion of dynamic video, video in a broad sense, is a multi-picture burst of playback, but depends on the video frame rate problem, frame rate is also known as FPS (Frames PerSecond) abbreviation - frame / second, refers to the number of frames per second to refresh the picture, but also It can be interpreted as the number of times per second the graphics processor can refresh. The higher the frame rate, the smoother and more realistic the animation will be, and the more frames per second (FPS), the smoother the action displayed will be.</p>
<p>Here you can convert coherent videos to pictures in FPS with third party software, in m1 mac os system, the famous video processing software:Ffmpeg is recommended</p>
<p>Homebrew for installation using the arm architecture.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">brew install ffmpeg
</code></pre></td></tr></table>
</div>
</div><p>After successful installation, type the ffmpeg command in the terminal to view the version:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="o">(</span>base<span class="o">)</span> ➜  animegan2-pytorch git:<span class="o">(</span>main<span class="o">)</span> ✗ ffmpeg   
ffmpeg version 4.4.1 Copyright <span class="o">(</span>c<span class="o">)</span> 2000-2021 the FFmpeg developers
  built with Apple clang version 13.0.0 <span class="o">(</span>clang-1300.0.29.3<span class="o">)</span>
  configuration: --prefix<span class="o">=</span>/opt/homebrew/Cellar/ffmpeg/4.4.1_3 --enable-shared --enable-pthreads --enable-version3 --cc<span class="o">=</span>clang --host-cflags<span class="o">=</span> --host-ldflags<span class="o">=</span> --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libbluray --enable-libdav1d --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev<span class="o">=</span>jack --enable-avresample --enable-videotoolbox
</code></pre></td></tr></table>
</div>
</div><p>There is no problem with the installation, then prepare a video file and create a new video_img.py:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">import os

<span class="c1"># 视频转图片</span>
os.system<span class="o">(</span><span class="s2">&#34;ffmpeg -i ./视频.mp4 -r 15 -s 1280,720 -ss 00:00:20 -to 00:00:22 ./myvideo/%03d.png&#34;</span><span class="o">)</span>
</code></pre></td></tr></table>
</div>
</div><p>Here we use Python3&rsquo;s built-in os module to run the ffmpeg command directly, for the video in the current directory, converting it at 15 frames per second, with the -s parameter representing the video resolution, the -ss parameter controlling the start position and end position of the video, and finally the directory to export the image to.</p>
<p>After running the script, enter the myvideo directory.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="o">(</span>base<span class="o">)</span> ➜  animegan2-pytorch git:<span class="o">(</span>main<span class="o">)</span> ✗ <span class="nb">cd</span> myvideo 
<span class="o">(</span>base<span class="o">)</span> ➜  myvideo git:<span class="o">(</span>main<span class="o">)</span> ✗ ls
001.png    004.png    007.png    010.png    013.png    016.png    019.png    022.png    025.png    028.png   
002.png    005.png    008.png    011.png    014.png    017.png    020.png    023.png    026.png    029.png   
003.png    006.png    009.png    012.png    015.png    018.png    021.png    024.png    027.png    030.png   
<span class="o">(</span>base<span class="o">)</span> ➜  myvideo git:<span class="o">(</span>main<span class="o">)</span> ✗
</code></pre></td></tr></table>
</div>
</div><p>As you can see, the images have been converted according to the number of frames as subscript file names.</p>
<p>Next, the images need to be batch converted using the AnimeGAN filter.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">ssl</span>
<span class="n">ssl</span><span class="o">.</span><span class="n">_create_default_https_context</span> <span class="o">=</span> <span class="n">ssl</span><span class="o">.</span><span class="n">_create_unverified_context</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">os</span>

<span class="n">img_list</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s2">&#34;./myvideo/&#34;</span><span class="p">)</span>


<span class="c1"># model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;celeba_distill&#34;)</span>
<span class="c1"># model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;face_paint_512_v1&#34;)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;bryandlee/animegan2-pytorch:main&#34;</span><span class="p">,</span> <span class="s2">&#34;generator&#34;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="s2">&#34;face_paint_512_v2&#34;</span><span class="p">)</span>
<span class="c1"># #model = torch.hub.load(&#34;bryandlee/animegan2-pytorch:main&#34;, &#34;generator&#34;, pretrained=&#34;paprika&#34;)</span>

<span class="n">face2paint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;bryandlee/animegan2-pytorch:main&#34;</span><span class="p">,</span> <span class="s2">&#34;face2paint&#34;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">img_list</span><span class="p">:</span>

    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&#34;.png&#34;</span><span class="p">:</span>

        <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&#34;./myvideo/&#34;</span><span class="o">+</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&#34;RGB&#34;</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">face2paint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span>

        <span class="n">out</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
        <span class="n">out</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&#34;./myimg/&#34;</span><span class="o">+</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># exit(-1)</span>
</code></pre></td></tr></table>
</div>
</div><p>For each conversion, the original image is kept and the filtered image is stored in the relative directory myimg, then a new img_video.py is created to reconvert it to a video.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># 图片转视频</span>
<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s2">&#34;ffmpeg -y -r 15 -i  ./myimg/</span><span class="si">%03d</span><span class="s2">.png -vcodec libx264 ./myvideo/test.mp4&#34;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>The rate is still 15 frames per second, the same as the original video.</p>
<p>If the original video has an audio track, you can first split the audio track: img_video.py to reconvert it to video:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="c1"># 抽离音频</span>
import os
os.system<span class="o">(</span><span class="s2">&#34;ffmpeg -y -i ./lisa.mp4 -ss 00:00:20 -to 00:00:22 -vn -y -acodec copy ./myvideo/3.aac&#34;</span><span class="o">)</span>
</code></pre></td></tr></table>
</div>
</div><p>After converting the anime filter, merge the converted video with the audio track of the original video:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="c1"># 合并音视频</span>

<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s2">&#34;ffmpeg -y -i ./myvideo/test.mp4 -i ./myvideo/3.aac -vcodec copy -acodec copy ./myvideo/output.mp4&#34;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>Test cases for the original video.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/01/01/cab5c3dba02c4228bc619753a37c1a45.gif" alt="image"></p>
<p>Post-conversion effect.</p>
<p><img src="https://cdn.jsdelivr.net/gh/b0xt/sobyte-images/2022/01/01/9ff7f539e20c43cdae4537258c0311e1.gif" alt="image"></p>
<p>With the support of the m1 chip, the efficiency of the cpu-based version of Pytorch is still good, but unfortunately the gpu version of Pytorch adapted to the m1 chip we still need to wait a while, in the last month, Pytorch project team member soumith gave this response.</p>
<blockquote>
<p>So, here&rsquo;s an update.</p>
<p>We plan to get the M1 GPU supported. @albanD, @ezyang and a few core-devs have been looking into it. I can&rsquo;t confirm/deny the involvement of any other folks right now.</p>
<p>So, what we have so far is that we had a prototype that was just about okay. We took the wrong approach (more graph-matching-ish), and the user-experience wasn&rsquo;t great &ndash; some operations were really fast, some were really slow, there wasn&rsquo;t a smooth experience overall. One had to guess-work which of their workflows would be fast.</p>
<p>So, we&rsquo;re completely re-writing it using a new approach, which I think is a lot closer to your good ole PyTorch, but it is going to take some time. I don&rsquo;t think we&rsquo;re going to hit a public alpha in the next ~4 months.</p>
<p>We will open up development of this backend as soon as we can.</p>
</blockquote>
<p>It can be seen that the project team should be completely reconstructed for the m1 chip Pytorch underlying, the public beta version will not be launched in the near future, perhaps the second half of next year will be released, or very much worth looking forward to.</p>

    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/ffmpeg/">ffmpeg</a>
          <a href="/tags/pytorch/">pytorch</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2022-01/origin-of-linux-tty/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Origin of Linux tty</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/2021-12/mount-docker-without-creating-root-file/">
            <span class="next-text nav-default">Avoid files gaining root privileges when docker mounts</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="https://www.sobyte.net/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2021 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
