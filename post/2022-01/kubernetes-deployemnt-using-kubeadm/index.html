<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Deploy a Kubernetes cluster - SoByte</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6356451834813761" crossorigin="anonymous"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E8GRRGBTEZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E8GRRGBTEZ');
</script>


<meta name="author" content="" /><meta name="description" content="Native Kubernetes Cluster Installation Tool Kubernetes clusters on the cloud, basically all cloud vendors support one-click deployment. The main focus here is on local deployment, or baremetal deployment. The approach presented in this article is suitable for development testing, and there may still be issues with security, stability, long-term availability, and other solutions. kubernetes is a component-based system with a lot of flexibility in the installation process, and many components" /><meta name="keywords" content="k8s, Using Kubeadm" />






<meta name="generator" content="Hugo 0.92.2 with theme even" />


<link rel="canonical" href="https://www.sobyte.net/post/2022-01/kubernetes-deployemnt-using-kubeadm/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Deploy a Kubernetes cluster" />
<meta property="og:description" content="Native Kubernetes Cluster Installation Tool Kubernetes clusters on the cloud, basically all cloud vendors support one-click deployment. The main focus here is on local deployment, or baremetal deployment. The approach presented in this article is suitable for development testing, and there may still be issues with security, stability, long-term availability, and other solutions. kubernetes is a component-based system with a lot of flexibility in the installation process, and many components" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.sobyte.net/post/2022-01/kubernetes-deployemnt-using-kubeadm/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-01-29T17:21:31+08:00" />
<meta property="article:modified_time" content="2022-01-29T17:21:31+08:00" />

<meta itemprop="name" content="Deploy a Kubernetes cluster">
<meta itemprop="description" content="Native Kubernetes Cluster Installation Tool Kubernetes clusters on the cloud, basically all cloud vendors support one-click deployment. The main focus here is on local deployment, or baremetal deployment. The approach presented in this article is suitable for development testing, and there may still be issues with security, stability, long-term availability, and other solutions. kubernetes is a component-based system with a lot of flexibility in the installation process, and many components"><meta itemprop="datePublished" content="2022-01-29T17:21:31+08:00" />
<meta itemprop="dateModified" content="2022-01-29T17:21:31+08:00" />
<meta itemprop="wordCount" content="3996">
<meta itemprop="keywords" content="k8s," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deploy a Kubernetes cluster"/>
<meta name="twitter:description" content="Native Kubernetes Cluster Installation Tool Kubernetes clusters on the cloud, basically all cloud vendors support one-click deployment. The main focus here is on local deployment, or baremetal deployment. The approach presented in this article is suitable for development testing, and there may still be issues with security, stability, long-term availability, and other solutions. kubernetes is a component-based system with a lot of flexibility in the installation process, and many components"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">SoByte</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="/ukraine/">
        <li class="mobile-menu-item">UKRAINE</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">SoByte</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/ukraine/">UKRAINE</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Deploy a Kubernetes cluster</h1>

      <div class="post-meta">
        <span class="post-time"> 2022-01-29 17:21:31 </span>
        <div class="post-category">
            <a href="/categories/tutorials/"> tutorials </a>
            </div>
          <span class="more-meta"> 3996 words </span>
          <span class="more-meta"> 8 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#native-kubernetes-cluster-installation-tool">Native Kubernetes Cluster Installation Tool</a></li>
        <li><a href="#1-environment-preparation-for-the-node">1. Environment preparation for the node</a>
          <ul>
            <li><a href="#11-iptables-setup">1.1 iptables setup</a></li>
            <li><a href="#12-open-node-port">1.2 Open node port</a></li>
          </ul>
        </li>
        <li><a href="#2-installing-containerd">2. Installing containerd</a></li>
        <li><a href="#3-install-kubeletkubeadmkubectl">3. install kubelet/kubeadm/kubectl</a></li>
        <li><a href="#4-create-load-balancing-for-the-master-kube-apiserver-to-achieve-high-availability">4. Create load balancing for the master kube-apiserver to achieve high availability</a></li>
        <li><a href="#5-creating-a-cluster-with-kubeadm">5. Creating a cluster with kubeadm</a>
          <ul>
            <li><a href="#51-frequently-asked-questions">5.1 Frequently Asked Questions</a></li>
          </ul>
        </li>
        <li><a href="#6-verifying-the-high-availability-of-the-cluster">6. Verifying the high availability of the cluster</a></li>
        <li><a href="#7-installing-network-plugins">7. Installing Network Plugins</a>
          <ul>
            <li><a href="#71-installing-cilium">7.1 Installing Cilium</a></li>
            <li><a href="#72-installing-calico">7.2 Installing Calico</a></li>
          </ul>
        </li>
        <li><a href="#8-check-cluster-status">8. check cluster status</a></li>
        <li><a href="#9-install-metrics-server">9. Install metrics-server</a></li>
        <li><a href="#10-add-regular-backup-capability-to-etcd">10. Add regular backup capability to etcd</a></li>
        <li><a href="#11-installing-volume-provisioner">11. Installing Volume Provisioner</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h2 id="native-kubernetes-cluster-installation-tool">Native Kubernetes Cluster Installation Tool</h2>
<blockquote>
<p>Kubernetes clusters on the cloud, basically all cloud vendors support one-click deployment. The main focus here is on local deployment, or baremetal deployment.</p>
<p>The approach presented in this article is suitable for development testing, and there may still be issues with security, stability, long-term availability, and other solutions.</p>
</blockquote>
<p>kubernetes is a component-based system with a lot of flexibility in the installation process, and many components have multiple implementations, each with their own characteristics, making it dizzying for beginners.</p>
<p>It is not easy to install and configure these components one by one and get them to work together.</p>
<p>The following are some of the tools that support Baremetal deployment.</p>
<ol>
<li><a href="https://kuboard.cn/install/install-k8s.html">kubeadm</a>: The community&rsquo;s cluster installation tool, which is now very mature.
<ol>
<li>Difficulty of use: easy</li>
</ol>
</li>
<li><a href="https://github.com/k3s-io/k3s">k3s</a>: lightweight kubernetes, small resource requirements, very simple to deploy, suitable for development testing or edge environments
<ol>
<li>support airgap offline deployment</li>
<li>easy to use: super easy</li>
</ol>
</li>
<li><a href="https://github.com/alibaba/sealer">alibaba/sealer</a>: support for packaging the entire kubernetes into an image for delivery, and very easy to deploy.
<ol>
<li>ease of use: super easy</li>
<li>This project is still in development, but it seems that many toB companies are already using it for k8s application delivery.</li>
</ol>
</li>
<li><a href="https://github.com/kubernetes-sigs/kubespray">kubespray</a>: suitable for self-built production-level clusters, is a large and comprehensive kubernetes installation solution, automatic installation of container runtime, k8s, network plug-ins and other components, and there are many options for each component. But it feels a bit complicated.
<ol>
<li>difficulty of use: medium</li>
<li>support airgap offline deployment, but I have tried it before is a pit, now I do not know how it is</li>
<li>the underlying use of kubeadm deployment cluster</li>
</ol>
</li>
</ol>
<p>In order to learn Kubernetes, I use the following official kubeadm for deployment (do not ask why not binary deployment, ask is lazy), container runtime using containerd, network plug-ins are currently the most trendy eBPF-based Cilium.</p>
<p>kubernetes officially introduces two topologies for highly available clusters: &ldquo;Stacked etcd topology&rdquo; and &ldquo;External etcd topology&rdquo;, for simplicity, this article uses the first &ldquo;stacked Etcd topology&rdquo; to create a three-master highly available cluster.</p>
<p>Reference.</p>
<ul>
<li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">Kubernetes Docs - Installing kubeadm</a></li>
<li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm">Kubernetes Docs - Creating Highly Available clusters with kubeadm</a> /high-availability/)</li>
</ul>
<h2 id="1-environment-preparation-for-the-node">1. Environment preparation for the node</h2>
<p>First prepare three Linux virtual machines, system selected on demand, and then adjust the settings of these three machines:.</p>
<ul>
<li>Node configuration.
<ul>
<li>master: no less than 2c/3g, hard disk 20G
<ul>
<li>master node performance is also affected by the number of cluster Pods, the above configuration should be able to support up to 100 Pods per Worker node.
The above configuration should be able to support up to 100 Pods per worker node. * worker: depends on the demand, recommended not less than 2c/4g, hard disk not less than 20G, 40G if sufficient resources are recommended.</li>
</ul>
</li>
</ul>
</li>
<li>In the same network and interoperable (usually the same LAN)</li>
<li>hostname and mac/ip address of each host and <code>/sys/class/dmi/id/product_uuid</code>, must be unique
<ul>
<li>Here the most problematic, usually hostname conflicts!</li>
</ul>
</li>
<li><strong>Must</strong> turn off swap for kubelet to work properly!</li>
</ul>
<p>For convenience, I directly used <a href="https://github.com/ryan4yin/pulumi-libvirt#examples">ryan4yin/pulumi-libvirt</a> to automatically create five virtual machines and set the ip/hostname.</p>
<p>This article uses the KVM cloud image of opensuse leap 15.3 to test the installation.</p>
<h3 id="11-iptables-setup">1.1 iptables setup</h3>
<p>The current container network for kubernetes uses the bridge mode by default. In this mode, you need to enable <code>iptables</code> to take over the traffic on the bridge.</p>
<p>Configure it as follows.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">sudo modprobe br_netfilter
cat <span class="s">&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span><span class="s">br_netfilter
</span><span class="s">EOF</span>

cat <span class="s">&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span><span class="s">net.bridge.bridge-nf-call-ip6tables = 1
</span><span class="s">net.bridge.bridge-nf-call-iptables = 1
</span><span class="s">EOF</span>
sudo sysctl --system

</code></pre></td></tr></table>
</div>
</div><h3 id="12-open-node-port">1.2 Open node port</h3>
<blockquote>
<p>For LAN environment, it is recommended to close the firewall directly. This way all ports are available and convenient.</p>
<p>Usually for our cloud clusters, the firewall is also turned off, but the client ip is restricted by the &ldquo;security group&rdquo; provided by the cloud service</p>
</blockquote>
<p>The control-plane node, i.e. master, needs to open the following ports.</p>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>Direction</th>
<th>Port Range</th>
<th>Purpose</th>
<th>Used By</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>6443*</td>
<td>Kubernetes API server</td>
<td>All</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>2379-2380</td>
<td>etcd server client API</td>
<td>kube-apiserver, etcd</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10250</td>
<td>kubelet API</td>
<td>Self, Control plane</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10251</td>
<td>kube-scheduler</td>
<td>Self</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10252</td>
<td>kube-controller-manager</td>
<td>Self</td>
</tr>
</tbody>
</table>
<p>The Worker node needs to develop the following ports.</p>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>Direction</th>
<th>Port Range</th>
<th>Purpose</th>
<th>Used By</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10250</td>
<td>kubelet API</td>
<td>Self, Control plane</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>30000-32767</td>
<td>NodePort Services†</td>
<td>All</td>
</tr>
</tbody>
</table>
<p>In addition, when we test locally, we may prefer to use <code>NodePort</code> on ports <code>80</code>, <code>443</code>, <code>8080</code>, etc. We need to modify the <code>-service-node-port-range</code> parameter of kube-apiserver to customize the port range of NodePort, and the corresponding worker nodes should also open these ports.</p>
<h2 id="2-installing-containerd">2. Installing containerd</h2>
<p>First, the environment configuration.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">cat <span class="s">&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span class="s">overlay
</span><span class="s">br_netfilter
</span><span class="s">nf_conntrack
</span><span class="s">EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter
sudo modprobe nf_conntrack

<span class="c1"># Setup required sysctl params, these persist across reboots.</span>
cat <span class="s">&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span class="s">net.bridge.bridge-nf-call-iptables  = 1
</span><span class="s">net.ipv4.ip_forward                 = 1
</span><span class="s">net.bridge.bridge-nf-call-ip6tables = 1
</span><span class="s">EOF</span>

<span class="c1"># Apply sysctl params without reboot</span>
sudo sysctl --system

</code></pre></td></tr></table>
</div>
</div><p>Install containerd+nerdctl:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">wget https://github.com/containerd/nerdctl/releases/download/v0.11.1/nerdctl-full-0.11.1-linux-amd64.tar.gz
tar -axvf nerdctl-full-0.11.1-linux-amd64.tar.gz
<span class="c1"># 这里简单起见，rootless 相关的东西也一起装进去了，测试嘛就无所谓了...</span>
mv bin/* /usr/local/bin/
mv lib/systemd/system/containerd.service /usr/lib/systemd/system/

systemctl <span class="nb">enable</span> containerd
systemctl start containerd

</code></pre></td></tr></table>
</div>
</div><p><code>nerdctl</code> is a command line tool for containerd, but its containers and images are completely isolated from Kubernetes containers and images and are not interoperable!</p>
<p>Currently, you can only view and pull Kubernetes containers and mirrors through <code>crictl</code>, and the next section will cover the installation of crictl.</p>
<h2 id="3-install-kubeletkubeadmkubectl">3. install kubelet/kubeadm/kubectl</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="c1"># 一些全局都需要用的变量</span>
<span class="nv">CNI_VERSION</span><span class="o">=</span><span class="s2">&#34;v0.8.2&#34;</span>
<span class="nv">CRICTL_VERSION</span><span class="o">=</span><span class="s2">&#34;v1.17.0&#34;</span>
<span class="c1"># kubernetes 的版本号</span>
<span class="c1"># RELEASE=&#34;$(curl -sSL https://dl.k8s.io/release/stable.txt)&#34;</span>
<span class="nv">RELEASE</span><span class="o">=</span><span class="s2">&#34;1.22.1&#34;</span>
<span class="c1"># kubelet 配置文件的版本号</span>
<span class="nv">RELEASE_VERSION</span><span class="o">=</span><span class="s2">&#34;v0.4.0&#34;</span>
<span class="c1"># 架构</span>
<span class="nv">ARCH</span><span class="o">=</span><span class="s2">&#34;amd64&#34;</span>
<span class="c1">#　安装目录</span>
<span class="nv">DOWNLOAD_DIR</span><span class="o">=</span>/usr/local/bin


<span class="c1"># CNI 插件</span>
sudo mkdir -p /opt/cni/bin
curl -L <span class="s2">&#34;https://github.com/containernetworking/plugins/releases/download/</span><span class="si">${</span><span class="nv">CNI_VERSION</span><span class="si">}</span><span class="s2">/cni-plugins-linux-</span><span class="si">${</span><span class="nv">ARCH</span><span class="si">}</span><span class="s2">-</span><span class="si">${</span><span class="nv">CNI_VERSION</span><span class="si">}</span><span class="s2">.tgz&#34;</span> <span class="p">|</span> sudo tar -C /opt/cni/bin -xz

<span class="c1"># crictl 相关工具</span>
curl -L <span class="s2">&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span class="si">${</span><span class="nv">CRICTL_VERSION</span><span class="si">}</span><span class="s2">/crictl-</span><span class="si">${</span><span class="nv">CRICTL_VERSION</span><span class="si">}</span><span class="s2">-linux-</span><span class="si">${</span><span class="nv">ARCH</span><span class="si">}</span><span class="s2">.tar.gz&#34;</span> <span class="p">|</span> sudo tar -C <span class="nv">$DOWNLOAD_DIR</span> -xz

<span class="c1"># kubelet/kubeadm/kubectl</span>
<span class="nb">cd</span> <span class="nv">$DOWNLOAD_DIR</span>
sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/<span class="si">${</span><span class="nv">RELEASE</span><span class="si">}</span>/bin/linux/<span class="si">${</span><span class="nv">ARCH</span><span class="si">}</span>/<span class="o">{</span>kubeadm,kubelet,kubectl<span class="o">}</span>
sudo chmod +x <span class="o">{</span>kubeadm,kubelet,kubectl<span class="o">}</span>

<span class="c1"># kubelet/kubeadm 配置</span>
curl -sSL <span class="s2">&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span class="si">${</span><span class="nv">RELEASE_VERSION</span><span class="si">}</span><span class="s2">/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service&#34;</span> <span class="p">|</span> sed <span class="s2">&#34;s:/usr/bin:</span><span class="si">${</span><span class="nv">DOWNLOAD_DIR</span><span class="si">}</span><span class="s2">:g&#34;</span> <span class="p">|</span> sudo tee /etc/systemd/system/kubelet.service
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL <span class="s2">&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span class="si">${</span><span class="nv">RELEASE_VERSION</span><span class="si">}</span><span class="s2">/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf&#34;</span> <span class="p">|</span> sed <span class="s2">&#34;s:/usr/bin:</span><span class="si">${</span><span class="nv">DOWNLOAD_DIR</span><span class="si">}</span><span class="s2">:g&#34;</span> <span class="p">|</span> sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

systemctl <span class="nb">enable</span> --now kubelet
<span class="c1"># 验证 kubelet 启动起来了，但是目前还没有初始化配置，过一阵就会重启一次</span>
systemctl status kubelet

</code></pre></td></tr></table>
</div>
</div><p>Try crictl:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="nb">export</span> <span class="nv">CONTAINER_RUNTIME_ENDPOINT</span><span class="o">=</span><span class="s1">&#39;unix:///var/run/containerd/containerd.sock&#39;</span>
<span class="c1"># 列出所有 pods，现在应该啥也没</span>
crictl  pods

<span class="c1"># 列出所有镜像</span>
crictl images

</code></pre></td></tr></table>
</div>
</div><h2 id="4-create-load-balancing-for-the-master-kube-apiserver-to-achieve-high-availability">4. Create load balancing for the master kube-apiserver to achieve high availability</h2>
<p>According to the official kubeadm documentation <a href="https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#kube-vip">Kubeadm Docs - High Availability Considerations</a>, the most well-known load balancing approach to achieve high availability for kube-apiserver is keepalived+haproxy, while simpler tools like kube-vip can also be considered.</p>
<p>For simplicity, let&rsquo;s use kube-vip directly, refer to the official documentation of kube-vip: <a href="https://kube-vip.io/install_static/">Kube-vip as a Static Pod with Kubelet</a>.</p>
<blockquote>
<p>P.S. I&rsquo;ve also seen some installation tools ditch keepalived and run a nginx on each node to do load balancing, with all the master addresses written in the configuration&hellip;</p>
</blockquote>
<p>First, use the following command to generate the kube-vip configuration file, using ARP as an example (we recommend switching to BGP for production environments).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">cat <span class="s">&lt;&lt;EOF | sudo tee add-kube-vip.sh
</span><span class="s"># 你的虚拟机网卡，opensuse/centos 等都是 eth0，但是 ubuntu 可能是 ens3
</span><span class="s">export INTERFACE=eth0
</span><span class="s">
</span><span class="s"># 用于实现高可用的 vip，需要和前面的网络接口在同一网段内，否则就无法路由了。
</span><span class="s">export VIP=192.168.122.200
</span><span class="s">
</span><span class="s"># 生成 static-pod 的配置文件
</span><span class="s">mkdir -p /etc/kubernetes/manifests
</span><span class="s">nerdctl run --rm --network=host --entrypoint=/kube-vip ghcr.io/kube-vip/kube-vip:v0.3.8 \
</span><span class="s">  manifest pod \
</span><span class="s">  --interface $INTERFACE \
</span><span class="s">  --vip $VIP \
</span><span class="s">  --controlplane \
</span><span class="s">  --services \
</span><span class="s">  --arp \
</span><span class="s">  --leaderElection | tee  /etc/kubernetes/manifests/kube-vip.yaml
</span><span class="s">EOF</span>

bash add-kube-vip.sh

</code></pre></td></tr></table>
</div>
</div><p>All three master nodes need to run the above command (the worker does not) to create the static-pod configuration file for kube-vip. After kubeadm initialization, kubelet will automatically pull them up as static pods.</p>
<h2 id="5-creating-a-cluster-with-kubeadm">5. Creating a cluster with kubeadm</h2>
<p>All you need to do is run this command.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="c1"># 极简配置：</span>
cat <span class="s">&lt;&lt;EOF | sudo tee kubeadm-config.yaml
</span><span class="s">apiVersion: kubeadm.k8s.io/v1beta3
</span><span class="s">kind: InitConfiguration
</span><span class="s">nodeRegistration:
</span><span class="s">  criSocket: &#34;/var/run/containerd/containerd.sock&#34;
</span><span class="s">  imagePullPolicy: IfNotPresent
</span><span class="s">---
</span><span class="s">kind: ClusterConfiguration
</span><span class="s">apiVersion: kubeadm.k8s.io/v1beta3
</span><span class="s">kubernetesVersion: v1.22.1
</span><span class="s">clusterName: kubernetes
</span><span class="s">certificatesDir: /etc/kubernetes/pki
</span><span class="s">imageRepository: k8s.gcr.io
</span><span class="s">controlPlaneEndpoint: &#34;192.168.122.200:6443&#34;  # 填 apiserver 的 vip 地址，或者整个域名也行，但是就得加 /etc/hosts 或者内网 DNS 解析
</span><span class="s">networking:
</span><span class="s">  serviceSubnet: &#34;10.96.0.0/16&#34;
</span><span class="s">  podSubnet: &#34;10.244.0.0/16&#34;
</span><span class="s">etcd:
</span><span class="s">  local:
</span><span class="s">    dataDir: /var/lib/etcd
</span><span class="s">---
</span><span class="s">apiVersion: kubelet.config.k8s.io/v1beta1
</span><span class="s">kind: KubeletConfiguration
</span><span class="s">cgroupDriver: systemd
</span><span class="s"># 让 kubelet 从 certificates.k8s.io 申请由集群 CA Root 签名的 tls 证书，而非直接使用自签名证书
</span><span class="s"># 如果不启用这个， 安装 metrics-server 时就会遇到证书报错，后面会详细介绍。
</span><span class="s">serverTLSBootstrap: true
</span><span class="s">EOF</span>

<span class="c1"># 查看 kubeadm 默认的完整配置，供参考</span>
kubeadm config print init-defaults &gt; init.default.yaml

<span class="c1"># 执行集群的初始化，这会直接将当前节点创建为 master</span>
<span class="c1"># 成功运行的前提：前面该装的东西都装好了，而且 kubelet 已经在后台运行了</span>
<span class="c1"># `--upload-certs` 会将生成的集群证书上传到 kubeadm 服务器，在两小时内加入集群的 master 节点会自动拉证书，主要是方便集群创建。</span>
kubeadm init --config kubeadm-config.yaml --upload-certs

</code></pre></td></tr></table>
</div>
</div><p>kubeadm should report an error indicating that some of your dependencies do not exist, so install the dependencies first.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">sudo zypper in -y socat ebtables conntrack-tools
</code></pre></td></tr></table>
</div>
</div><p>Re-run the previous kubeadm command again and it should execute properly, it does the following operations.</p>
<ul>
<li>Pull the container image of the control plane</li>
<li>Generate a ca root certificate</li>
<li>Generate tls certificates for etcd/apiserver etc. using the root certificate</li>
<li>Generate kubeconfig configuration for each component of the control plane</li>
<li>Generate static pod configuration, kubelet will automatically pull up kube-proxy and all other k8s master components based on these configurations</li>
</ul>
<p>After running, it will give you three commands.</p>
<ul>
<li>
<p>Place <code>kubeconfig</code> under <code>$HOME/.kube/config</code>, which <code>kubectl</code> needs to use to connect to the kube-apiserver</p>
</li>
<li>
<p>The command to add the control-plane node to the cluster:</p>
<ul>
<li>Because we added the static-pod configuration of kube-vip in advance, the preflight-check here will report an error, so you need to add this parameter to ignore the error - <code>--ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests</code></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">kubeadm join 192.168.122.200:6443 --token &lt;token&gt; <span class="se">\
</span><span class="se"></span>  --discovery-token-ca-cert-hash sha256:&lt;hash&gt; <span class="se">\
</span><span class="se"></span>  --control-plane --certificate-key &lt;key&gt; <span class="se">\
</span><span class="se"></span>  --ignore-preflight-errors<span class="o">=</span>DirAvailable--etc-kubernetes-manifests
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>The command to join a worker node to a cluster:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">kubeadm join 192.168.122.200:6443 --token &lt;token&gt; <span class="se">\
</span><span class="se"></span>  --discovery-token-ca-cert-hash sha256:&lt;hash&gt; 
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<p>After running the first part of the <code>kubeconfig</code> processing command, you can use kubectl to view the status of the cluster.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">k8s-master-0:~/kubeadm <span class="c1"># kubectl get no</span>
NAME           STATUS     ROLES                  AGE   VERSION
k8s-master-0   NotReady   control-plane,master   79s   v1.22.1
k8s-master-0:~/kubeadm <span class="c1"># kubectl get po --all-namespaces</span>
NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE
kube-system   coredns-78fcd69978-6tlnw               0/1     Pending   <span class="m">0</span>          83s
kube-system   coredns-78fcd69978-hxtvs               0/1     Pending   <span class="m">0</span>          83s
kube-system   etcd-k8s-master-0                      1/1     Running   <span class="m">6</span>          90s
kube-system   kube-apiserver-k8s-master-0            1/1     Running   <span class="m">4</span>          90s
kube-system   kube-controller-manager-k8s-master-0   1/1     Running   <span class="m">4</span>          90s
kube-system   kube-proxy-6w2bx                       1/1     Running   <span class="m">0</span>          83s
kube-system   kube-scheduler-k8s-master-0            1/1     Running   <span class="m">7</span>          97s
</code></pre></td></tr></table>
</div>
</div><p>Now run the join cluster command printed out earlier on the other nodes and you have a highly available cluster set up.</p>
<p>After all the nodes have joined the cluster, you should have three control plane masters and two workers when viewed through kubectl.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">k8s-master-0:~/kubeadm <span class="c1"># kubectl get node</span>
NAME           STATUS     ROLES                  AGE     VERSION
k8s-master-0   NotReady   control-plane,master   26m     v1.22.1
k8s-master-1   NotReady   control-plane,master   7m2s    v1.22.1
k8s-master-2   NotReady   control-plane,master   2m10s   v1.22.1
k8s-worker-0   NotReady   &lt;none&gt;                 97s     v1.22.1
k8s-worker-1   NotReady   &lt;none&gt;                 86s     v1.22.1
</code></pre></td></tr></table>
</div>
</div><p>Right now they are all in the NotReady state and will not be ready until we get the network plug-in installed.</p>
<p>Now look at the certificate issuance status of the cluster again.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">❯ kubectl get csr --sort-by<span class="o">=</span><span class="s1">&#39;{.spec.username}&#39;</span>
NAME        AGE     SIGNERNAME                                    REQUESTOR                  REQUESTEDDURATION   CONDITION
csr-95hll   6m58s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-tklnr   7m5s    kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-w92jv   9m15s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-rv7sj   8m11s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
csr-nxkgx   10m     kubernetes.io/kube-apiserver-client-kubelet   system:node:k8s-master-0   &lt;none&gt;              Approved,Issued
csr-cd22c   10m     kubernetes.io/kubelet-serving                 system:node:k8s-master-0   &lt;none&gt;              Pending
csr-wjrnr   9m53s   kubernetes.io/kubelet-serving                 system:node:k8s-master-0   &lt;none&gt;              Pending
csr-sjq42   9m8s    kubernetes.io/kubelet-serving                 system:node:k8s-master-1   &lt;none&gt;              Pending
csr-xtv8f   8m56s   kubernetes.io/kubelet-serving                 system:node:k8s-master-1   &lt;none&gt;              Pending
csr-f2dsf   8m3s    kubernetes.io/kubelet-serving                 system:node:k8s-master-2   &lt;none&gt;              Pending
csr-xl8dg   6m58s   kubernetes.io/kubelet-serving                 system:node:k8s-worker-0   &lt;none&gt;              Pending
csr-p9g24   6m52s   kubernetes.io/kubelet-serving                 system:node:k8s-worker-1   &lt;none&gt;              Pending

</code></pre></td></tr></table>
</div>
</div><p>You can see that several of the <code>kubernetes.io/kubelet-serving</code> certificates are still pending, because we set <code>serverTLSBootstrap: true</code> in the kubeadm configuration file to allow Kubelet to request CA-signed certificates from the cluster, instead of self-signing result.</p>
<p>The main purpose of setting this parameter is to allow components such as metrics-server to communicate with kubelet using the https protocol and to avoid adding the parameter <code>--kubelet-insecure-tls</code> for metrics-server.</p>
<p>Currently kubeadm does not support automatic approval of certificates requested by kubelets, we need to approve them manually:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="c1"># 批准 Kubelet 申请的所有证书</span>
kubectl certificate approve csr-cd22c csr-wjrnr csr-sjq42 csr-xtv8f csr-f2dsf csr-xl8dg csr-p9g24
</code></pre></td></tr></table>
</div>
</div><p>Until these certificates are approved, all functions that require calls to the kubelet api will not be available, such as</p>
<ul>
<li>View pod logs</li>
<li>Get node metrics</li>
<li>etc.</li>
</ul>
<h3 id="51-frequently-asked-questions">5.1 Frequently Asked Questions</h3>
<h4 id="511-using-domestic-mirror-sources">5.1.1 Using domestic mirror sources</h4>
<p>If you do not have a VPN, kubeadm&rsquo;s default mirror repository cannot be pulled from within China. If you have high reliability requirements, it is better to build your own private mirror repository and push the mirrors to the private repository.</p>
<p>You can list all the mirror addresses you need to use with the following command.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">❯ kubeadm config images list --kubernetes-version v1.22.1
k8s.gcr.io/kube-apiserver:v1.22.1
k8s.gcr.io/kube-controller-manager:v1.22.1
k8s.gcr.io/kube-scheduler:v1.22.1
k8s.gcr.io/kube-proxy:v1.22.1
k8s.gcr.io/pause:3.5
k8s.gcr.io/etcd:3.5.0-0
k8s.gcr.io/coredns/coredns:v1.8.4

</code></pre></td></tr></table>
</div>
</div><p>Use a tool or script such as <code>skopeo</code> to copy the above image to your private repository, or for convenience (test environment) consider looking for a synchronized image address online. Add the image address to <code>kubeadm-config.yaml</code> and deploy it.</p>
<h4 id="512-resetting-the-cluster-configuration">5.1.2 Resetting the cluster configuration</h4>
<p>If anything goes wrong during the cluster creation process, you can restore the configuration by running <code>kubeadm reset</code> on all nodes, and then go through the kubeadm cluster creation process again.</p>
<p>However, a few things should be noted.</p>
<ul>
<li><code>kubeadm reset</code> will clear all static-pod configuration files including kube-vip configuration, so the master node needs to re-run the kube-vip command given earlier and generate the kube-vip configuration.</li>
<li><code>kubeadm reset</code> will not reset the network interface configuration, the master node needs to manually clean up the vip added by kube-vip: <code>ip addr del 192.168.122.200/32 dev eth0</code> .</li>
<li>If you wish to reinstall the cluster after installing the network plugin, the sequence is as follows.
<ul>
<li>delete -f xxx.yaml<code>via</code>kubectl delete -f xxx.yaml<code>/</code>helm uninstall` Delete all other application configurations except network</li>
<li>Delete the network plugin</li>
<li>Reboot all nodes first, or manually reset the network configuration of all nodes
<ul>
<li>Recommend reboot, because I don&rsquo;t know how to reset manually&hellip; tried <code>systemctl restart network</code> and it doesn&rsquo;t clean up all virtual network interfaces.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>After doing so, re-run the cluster installation and you should be fine.</p>
<h2 id="6-verifying-the-high-availability-of-the-cluster">6. Verifying the high availability of the cluster</h2>
<p>Although the network plug-in is not yet installed and all nodes of the cluster are not yet ready, we can already verify the high availability of the cluster simply by using the kubectl command.</p>
<p>First, we install the authentication file <code>$HOME/.kube/config</code> and kunbectl that we placed on k8s-master-0 on another machine, such as my host machine.</p>
<p>Then run the <code>kubectl get node</code> command on the host machine to verify the high availability of the cluster.</p>
<ul>
<li>When all three master nodes are running normally, the kubectl command also works</li>
<li>pause or stop one of the masters, the kubectl command still works fine</li>
<li>pause the second master again, the kubectl command should get stuck and time out, and it won&rsquo;t work</li>
<li>resume restore one of the two stopped masters, and the kubectl command will work again</li>
</ul>
<p>This completes kubeadm&rsquo;s work, and the next step is to install the network plugin and the cluster will be available.</p>
<h2 id="7-installing-network-plugins">7. Installing Network Plugins</h2>
<p>There are many kinds of network plugins available in the community, the more well-known ones with good performance are Calico and Cilium, where Cilium focuses on high performance and observability based on eBPF.</p>
<p>Here are the installation methods of these two plug-ins respectively. (Note that only one of the network plug-ins can be installed, not repeated.)</p>
<p>You need to install helm locally in advance, I use the host here, so you only need to install it on the host:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="c1"># 一行命令安装，也可以自己手动下载安装包，都行</span>
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 <span class="p">|</span> bash

<span class="c1"># 或者 opensuse 直接用包管理器安装</span>
sudo zypper in helm

</code></pre></td></tr></table>
</div>
</div><h3 id="71-installing-cilium">7.1 Installing Cilium</h3>
<blockquote>
<p>Official documentation: <a href="https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-kubeadm/">https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-kubeadm/</a></p>
</blockquote>
<p>cilium provides a high-performance and highly observable k8s clustering network via eBPF. cilium also provides a more efficient implementation than kube-proxy and can completely replace kube-proxy.</p>
<p>Let&rsquo;s start with kube-proxy mode and familiarize ourselves with cilium.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">helm repo add cilium https://helm.cilium.io/
helm search repo cilium/cilium -l <span class="p">|</span> head

helm install cilium cilium/cilium --version 1.10.4 --namespace kube-system

</code></pre></td></tr></table>
</div>
</div><p>You can check the progress of cilium installation via <code>kubectl get pod -A</code>, when all pods are ready, the cluster is ready~.</p>
<p>cilium also provides a dedicated client.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz<span class="o">{</span>,.sha256sum<span class="o">}</span>
sha256sum --check cilium-linux-amd64.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
rm cilium-linux-amd64.tar.gz<span class="o">{</span>,.sha256sum<span class="o">}</span>

</code></pre></td></tr></table>
</div>
</div><p>Then use the cilium client to check the status of the web plugin:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"> $ cilium status --wait
    /¯¯<span class="se">\
</span><span class="se"></span> /¯¯<span class="se">\_</span>_/¯¯<span class="se">\ </span>   Cilium:         OK
 <span class="se">\_</span>_/¯¯<span class="se">\_</span>_/    Operator:       OK
 /¯¯<span class="se">\_</span>_/¯¯<span class="se">\ </span>   Hubble:         disabled
 <span class="se">\_</span>_/¯¯<span class="se">\_</span>_/    ClusterMesh:    disabled
    <span class="se">\_</span>_/

DaemonSet         cilium             Desired: 5, Ready: 5/5, Available: 5/5
Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2
Containers:       cilium             Running: <span class="m">5</span>
                  cilium-operator    Running: <span class="m">2</span>
Cluster Pods:     2/2 managed by Cilium
Image versions    cilium             quay.io/cilium/cilium:v1.10.4@sha256:7d354052ccf2a7445101d78cebd14444c7c40129ce7889f2f04b89374dbf8a1d: <span class="m">5</span>
                  cilium-operator    quay.io/cilium/operator-generic:v1.10.4@sha256:c49a14e34634ff1a494c84b718641f27267fb3a0291ce3d74352b44f8a8d2f93: <span class="m">2</span>

</code></pre></td></tr></table>
</div>
</div><p>cilium also provides commands to automatically create pods for connectivity testing of clustered networks:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">❯ cilium connectivity <span class="nb">test</span>
ℹ️  Monitor aggregation detected, will skip some flow validation steps
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Creating namespace <span class="k">for</span> connectivity check...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying echo-same-node service...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying same-node deployment...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying client deployment...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying client2 deployment...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying echo-other-node service...
✨ <span class="o">[</span>kubernetes<span class="o">]</span> Deploying other-node deployment...
...
ℹ️  Expose Relay locally with:
   cilium hubble <span class="nb">enable</span>
   cilium status --wait
   cilium hubble port-forward<span class="p">&amp;</span>
🏃 Running tests...
...
---------------------------------------------------------------------------------------------------------------------
✅ All <span class="m">11</span> tests <span class="o">(</span><span class="m">134</span> actions<span class="o">)</span> successful, <span class="m">0</span> tests skipped, <span class="m">0</span> scenarios skipped.

</code></pre></td></tr></table>
</div>
</div><p>As you can observe by <code>kubectl get po -A</code>, this test command automatically creates a <code>cilium-test</code> namespace and creates several pods for detailed testing at startup.</p>
<p>The whole test process will last about 5 minutes or so, and the pods will not be automatically deleted after the test is completed, but will be deleted manually using the following command.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">kubectl delete namespace cilium-test

</code></pre></td></tr></table>
</div>
</div><h3 id="72-installing-calico">7.2 Installing Calico</h3>
<blockquote>
<p>Official documentation: <a href="https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises">https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises</a></p>
</blockquote>
<p>It&rsquo;s only two or three lines of command. The installation is really easy, so I don&rsquo;t bother to introduce it, just read the official documentation.</p>
<p>But there are actually quite a lot of details about calico, so we recommend reading through its official documentation to understand the architecture of calico.</p>
<h2 id="8-check-cluster-status">8. check cluster status</h2>
<p>The official dashboard doesn&rsquo;t work very well, so we recommend installing a local k9s directly, which is particularly cool.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">sudo zypper in k9s

</code></pre></td></tr></table>
</div>
</div><p>Then it&rsquo;s time to have fun.</p>
<h2 id="9-install-metrics-server">9. Install metrics-server</h2>
<blockquote>
<p>Possible issues with this step: <a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs">Enabling signed kubelet serving certificates</a></p>
</blockquote>
<p>If you need to use HPA and simple cluster monitoring, then metrics-server must be installed, so let&rsquo;s install it now.</p>
<p>First, running kubectl&rsquo;s monitoring command should report an error.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">❯ kubectl top node
error: Metrics API not available

</code></pre></td></tr></table>
</div>
</div><p>You shouldn&rsquo;t see any monitoring metrics in k9s either.</p>
<p>Now install it via helm.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm search repo metrics-server/metrics-server -l <span class="p">|</span> head

helm upgrade --install metrics-server metrics-server/metrics-server --version 3.5.0 --namespace kube-system

</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>metrics-server will only deploy one instance by default, if you want high availability, please refer to the official configuration: <a href="https://github.com/kubernetes-sigs/metrics-server/tree/master/manifests/high-availability">metrics-server - high-availability manifests</a></p>
</blockquote>
<p>Once the metrics-server is up and running, you can use the <code>kubectl top</code> command.</p>
<h2 id="10-add-regular-backup-capability-to-etcd">10. Add regular backup capability to etcd</h2>
<p>See <a href="https://github.com/ryan4yin/knowledge/blob/master/datastore/etcd/etcd%20%E7%9A%84%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D.md">backup and restore for etcd</a></p>
<h2 id="11-installing-volume-provisioner">11. Installing Volume Provisioner</h2>
<p>As we learn to use stateful applications such as Prometheus/MinIO/Tekton, they will by default declare the required data volumes via PVC.</p>
<p>To support this capability, we need to deploy a Volume Provisioner in the cluster.</p>
<p>For on-cloud environments, it is OK to directly access the Volume Provisioner provided by the cloud provider, which is convenient, hassle-free, and reliable enough.</p>
<p>For bare-metal environments, the more famous one should be rook-ceph, but this thing is complicated to deploy and difficult to maintain, not suitable for testing and learning, and not suitable for production environments.</p>
<p>For development, test environments, or personal clusters, it is recommended to use.</p>
<ul>
<li>local data volume, suitable for data can be lost, and does not require distributed scenarios, such as development and testing environments
<ul>
<li><a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner</a></li>
<li><a href="https://github.com/rancher/local-path-provisioner">https://github.com/rancher/local-path-provisioner</a></li>
</ul>
</li>
<li>NFS data volumes, suitable for scenarios where data can be lost, performance requirements are not high, and distributed is required. For example, development test environments, or applications that do not have much pressure online
<ul>
<li><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner</a></li>
<li><a href="https://github.com/kubernetes-csi/csi-driver-nfs">https://github.com/kubernetes-csi/csi-driver-nfs</a></li>
<li>The reliability of NFS data depends on external NFS servers, and enterprises often use NAS such as Synology as NFS servers.</li>
<li>If the external NFS server goes down, the application will crash.</li>
</ul>
</li>
<li>Direct use of object storage on the cloud is suitable for scenarios where you want to have no data loss and low performance requirements.
<ul>
<li>Use <a href="https://github.com/rclone/rclone">https://github.com/rclone/rclone</a> mount mode directly to save data, or sync folder data directly to the cloud (there may be some data loss).</li>
</ul>
</li>
</ul>

    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/k8s/">k8s</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2022-01/common-distributed-protocols-and-lgorithms/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Common Distributed Protocols and Lgorithms</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/2022-01/kubernetes-best-practices/">
            <span class="next-text nav-default">Kubernetes Microservices Best Practices</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="https://www.sobyte.net/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2021 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
